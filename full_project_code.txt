################################
# config/settings.py
################################
# DEPENDENCIES
from pathlib import Path
from pydantic import Field
from typing import Optional
from pydantic_settings import BaseSettings


class Settings(BaseSettings):
    """
    Application-wide settings: primary configuration source
    """
    # Application Info
    APP_NAME               : str           = "AI Contract Risk Analyzer"
    APP_VERSION            : str           = "1.0.0"
    API_PREFIX             : str           = "/api/v1/"
    
    # Server Configuration
    HOST                   : str           = "0.0.0.0"
    PORT                   : int           = 8000
    RELOAD                 : bool          = True
    WORKERS                : int           = 1
    
    # CORS Settings
    CORS_ORIGINS           : list          = ["http://localhost:3000", "http://localhost:8000", "http://127.0.0.1:8000"]
    CORS_ALLOW_CREDENTIALS : bool          = True
    CORS_ALLOW_METHODS     : list          = ["*"]
    CORS_ALLOW_HEADERS     : list          = ["*"]
    
    # File Upload Settings
    MAX_UPLOAD_SIZE        : int           = 10 * 1024 * 1024  # 10 MB
    ALLOWED_EXTENSIONS     : list          = [".pdf", ".docx", ".txt"]
    UPLOAD_DIR             : Path          = Path("uploads")
    
    # Model Management Settings
    MODEL_CACHE_SIZE       : int           = 3     # Number of models to keep in memory
    MODEL_DOWNLOAD_TIMEOUT : int           = 1800  # 30 minutes
    USE_GPU                : bool          = True  # Automatically detect and use GPU if available
    
    # External API Settings
    OLLAMA_BASE_URL        : str           = "http://localhost:11434"
    OLLAMA_MODEL           : str           = "llama3:8b"
    OLLAMA_TIMEOUT         : int           = 300
    OLLAMA_TEMPERATURE     : float         = 0.1
    
    # External API Keys
    OPENAI_API_KEY         : Optional[str] = None
    ANTHROPIC_API_KEY      : Optional[str] = None
    
    # Analysis Limits
    MIN_CONTRACT_LENGTH    : int           = 300    # Minimum characters for valid contract
    MAX_CONTRACT_LENGTH    : int           = 500000 # Maximum characters (500KB text)
    MAX_CLAUSES_TO_ANALYZE : int           = 15
    
    # Logging Settings
    LOG_LEVEL              : str           = "INFO"
    LOG_FORMAT             : str           = "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    LOG_FILE               : Optional[Path] = Path("logs/app.log")
    
    # Cache Settings
    ENABLE_CACHE           : bool          = True
    CACHE_TTL              : int           = 3600 # 1 hour
    CACHE_DIR              : Path          = Path("cache")
    
    # Rate Limiting Settings
    RATE_LIMIT_ENABLED     : bool          = True
    RATE_LIMIT_REQUESTS    : int           = 10
    RATE_LIMIT_PERIOD      : int           = 60  # seconds
    
    # PDF Report Settings
    PDF_FONT_SIZE          : int           = 10
    PDF_MARGIN             : float         = 0.5 # inches
    PDF_PAGE_SIZE          : str           = "letter"
    

    class Config:
        env_file          = ".env"
        env_file_encoding = "utf-8"
        case_sensitive    = True
    

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        # Ensure directories exist
        self.UPLOAD_DIR.mkdir(parents = True, exist_ok = True)
        self.CACHE_DIR.mkdir(parents = True, exist_ok = True)
        
        if self.LOG_FILE:
            self.LOG_FILE.parent.mkdir(parents = True, exist_ok = True)


# Global settings instance
settings = Settings()


##########################
# config/risk_rules.py
##########################
# DEPENDENCIES
from enum import Enum
from typing import Dict
from typing import List
from typing import Tuple


class ContractType(Enum):
    EMPLOYMENT  = "employment"
    CONSULTING  = "consulting"
    NDA         = "nda"
    SOFTWARE    = "software"
    SERVICE     = "service"
    PARTNERSHIP = "partnership"
    LEASE       = "lease"
    PURCHASE    = "purchase"
    GENERAL     = "general"


class RiskRules:
    """
    Comprehensive risk scoring rules for broad contract coverage
    """
    CATEGORY_WEIGHTS          = {"restrictive_covenants" : 15,
                                 "termination_rights"    : 12,
                                 "penalties_liability"   : 14,
                                 "compensation_benefits" : 13,
                                 "intellectual_property" : 12,
                                 "confidentiality"       : 10,
                                 "liability_indemnity"   : 11,
                                 "governing_law"         : 8,
                                 "payment_terms"         : 10,
                                 "warranties"            : 9,
                                 "dispute_resolution"    : 7,
                                 "assignment_change"     : 6,
                                 "insurance"             : 5,
                                 "force_majeure"         : 4,
                                }
    
    # Contract-specific weight adjustments
    CONTRACT_TYPE_ADJUSTMENTS = {ContractType.EMPLOYMENT  : {"restrictive_covenants": 1.8, "compensation_benefits": 1.6, "termination_rights": 1.4, "confidentiality": 1.3},
                                 ContractType.SOFTWARE    : {"intellectual_property": 1.8, "penalties_liability": 1.5, "warranties": 1.4, "payment_terms": 1.3},
                                 ContractType.NDA         : {"confidentiality": 2.0, "penalties_liability": 1.6, "restrictive_covenants": 1.4},
                                 ContractType.CONSULTING  : {"compensation_benefits": 1.5, "termination_rights": 1.3, "liability_indemnity": 1.4},
                                 ContractType.LEASE       : {"payment_terms": 1.6, "termination_rights": 1.5, "liability_indemnity": 1.4},
                                 ContractType.PURCHASE    : {"warranties": 1.7, "payment_terms": 1.5, "liability_indemnity": 1.3},
                                 ContractType.PARTNERSHIP : {"governing_law": 1.6, "dispute_resolution": 1.5, "assignment_change": 1.4},
                                 ContractType.SERVICE     : {"payment_terms": 1.5, "warranties": 1.4, "termination_rights": 1.3},
                                } 
    
    CRITICAL_KEYWORDS         = {"non-compete"                : 25,
                                 "non-solicit"                : 23,
                                 "non-solicitation"           : 23,
                                 "forfeit"                    : 25,
                                 "liquidated damages"         : 24,
                                 "wage withholding"           : 25,
                                 "unlimited liability"        : 25,
                                 "joint and several"          : 23,
                                 "perpetual"                  : 22,
                                 "irrevocable"                : 20,
                                 "automatic renewal"          : 21,
                                 "assignment without consent" : 22,
                                 "sole discretion"            : 23,
                                }
    
    HIGH_RISK_KEYWORDS        = {"indemnify"             : 18,
                                 "indemnification"       : 18,
                                 "hold harmless"         : 17,
                                 "penalty"               : 18,
                                 "damages"               : 15,
                                 "breach"                : 15,
                                 "default"               : 14,
                                 "immediate termination" : 16,
                                 "without cause"         : 15,
                                 "at-will"               : 14,
                                 "waive"                 : 16,
                                 "release"               : 15,
                                 "confidential"          : 12,
                                 "proprietary"           : 12,
                                 "exclusive"             : 14,
                                 "non-refundable"        : 16,
                                }
    
    MEDIUM_RISK_KEYWORDS      = {"terminate"      : 7, 
                                 "termination"    : 7, 
                                 "assignment"     : 6, 
                                 "warranty"       : 8, 
                                 "representation" : 7, 
                                 "covenant"       : 8, 
                                 "jurisdiction"   : 6, 
                                 "governing law"  : 6, 
                                 "insurance"      : 5, 
                                 "force majeure"  : 4, 
                                 "amendment"      : 5,  
                                 "notice"         : 4,
                                }

    RISKY_PATTERNS            = [(r'\d+\s*(year|yr|month|mo)s?\s*(non-compete|non-solicit)', 20, "Long duration restrictive covenant"),
                                 (r'(entire|all|worldwide|global)\s*(industry|market|territory)', 18, "Overly broad geographic/industry scope"),
                                 (r'notice\s+period.*\d+\s*days.*employee.*\d+\s*days.*employer', 15, "Unequal notice periods"),
                                 (r'(may|can|shall)\s+(withhold|deduct|retain).*compensation', 22, "Wage withholding clause"),
                                 (r'(unlimited|no\s+limit|without\s+limitation).*liability', 25, "Unlimited liability exposure"),
                                 (r'(sole|absolute|unfettered)\s+discretion', 18, "One-sided discretionary power"),
                                 (r'penalty.*(?:equal\s+to|of|amount).*\$?\d+', 16, "Specific penalty amount"),
                                 (r'(automatically|immediately)\s+(renew|extend)', 12, "Auto-renewal clause"),
                                 (r'waive.*right.*arbitration', 20, "Arbitration rights waiver"),
                                 (r'(all|any).*intellectual\s+property.*created', 17, "Broad IP assignment"),
                                 (r'payment.*due.*upon.*signature', 14, "Payment due upon signature"),
                                 (r'no.*warranty.*(?:merchantability|fitness)', 15, "No warranty disclaimer"),
                                 (r'governing\s+law.*\b(?:delaware|nevada)\b', 8, "Specific governing law"),
                                ]

    CLAUSE_RISK_FACTORS       = {"non_compete"           : {"base_risk" : 70,
                                                            "red_flags" : {"same industry": 0, "direct competitor": -5, "entire industry": +20, "all industries": +25, "worldwide": +15, "global": +15, "specific city": -10, "specific state": -5},
                                                           },
                                 "termination"           : {"base_risk" : 50,
                                                            "red_flags" : {"without cause": +15, "immediate": +12, "at will": +10, "sole discretion": +18, "no notice": +20, "no reason": +15},
                                                           },
                                 "indemnification"       : {"base_risk" : 60,
                                                            "red_flags" : {"unlimited": +25, "joint and several": +20, "gross negligence": -10, "willful misconduct": -10, "third party": +8, "all claims": +15, "any claims": +15},
                                                           },
                                 "compensation"          : {"base_risk" : 30,
                                                            "red_flags" : {"to be determined": +20, "tbd": +20, "subject to review": +15, "discretionary": +18, "at employer's discretion": +22, "may withhold": +25, "can deduct": +20},
                                                           },
                                 "intellectual_property" : {"base_risk" : 55,
                                                            "red_flags" : {"all work product": +18, "anything created": +20, "during employment": +10, "after employment": +25, "including personal projects": +30, "whether or not related": +25},
                                                           },
                                 "confidentiality"       : {"base_risk" : 45,
                                                            "red_flags" : {"perpetual": +20, "indefinite": +18, "all information": +15, "including public information": +25},
                                                           },
                                 "payment"               : {"base_risk" : 35,
                                                            "red_flags" : {"net 90": +15, "net 120": +20, "upon completion": +10, "discretionary": +18},
                                                           },
                                 "warranty"              : {"base_risk" : 40,
                                                            "red_flags" : {"as is": +25, "no warranty": +20, "disclaims all": +22},
                                                           },
                                }
    
    INDUSTRY_BENCHMARKS       = {"non_compete_duration"     : {"tech"       : {"reasonable": 6, "standard": 12, "excessive": 24},
                                                               "finance"    : {"reasonable": 12, "standard": 18, "excessive": 36},
                                                               "healthcare" : {"reasonable": 12, "standard": 24, "excessive": 36},
                                                               "general"    : {"reasonable": 6, "standard": 12, "excessive": 24},
                                                              },
                                 "notice_period_days"       : {"executive"    : {"reasonable": 90, "standard": 60, "minimal": 30},
                                                               "senior"       : {"reasonable": 60, "standard": 30, "minimal": 14},
                                                               "professional" : {"reasonable": 30, "standard": 14, "minimal": 7},
                                                               "general"      : {"reasonable": 30, "standard": 14, "minimal": 7},
                                                              },
                                 "liability_cap_multiplier" : {"saas"        : {"generous": 24, "standard": 12, "restrictive": 3},
                                                               "consulting"  : {"generous": 3, "standard": 1, "restrictive": 0.5},
                                                               "general"     : {"generous": 12, "standard": 6, "restrictive": 1},
                                                              },
                                }

    PROTECTION_CHECKLIST      = {"for_cause_definition"     : {"importance": "critical", "risk_if_missing": 25, "categories": ["termination_rights"]},
                                 "severance_provision"      : {"importance": "high", "risk_if_missing": 18, "categories": ["termination_rights", "compensation_benefits"]},
                                 "mutual_indemnification"   : {"importance": "high", "risk_if_missing": 20, "categories": ["liability_indemnity"]},
                                 "liability_cap"            : {"importance": "critical", "risk_if_missing": 25, "categories": ["liability_indemnity", "penalties_liability"]},
                                 "prior_ip_exclusion"       : {"importance": "high", "risk_if_missing": 22, "categories": ["intellectual_property"]},
                                 "confidentiality_duration" : {"importance": "medium", "risk_if_missing": 12, "categories": ["confidentiality"]},
                                 "dispute_resolution"       : {"importance": "medium", "risk_if_missing": 15, "categories": ["dispute_resolution"]},
                                 "change_control_process"   : {"importance": "medium", "risk_if_missing": 10, "categories": ["assignment_change"]},
                                 "insurance_requirements"   : {"importance": "medium", "risk_if_missing": 12, "categories": ["insurance"]},
                                 "force_majeure"            : {"importance": "low", "risk_if_missing": 8, "categories": ["force_majeure"]},
                                }
                        
    RISK_THRESHOLDS           = {"critical" : 80,
                                 "high"     : 60,
                                 "medium"   : 40,
                                 "low"      : 20,
                                }
                            
    CATEGORY_DESCRIPTIONS     = {"restrictive_covenants"  : {"high"   : "Overly restrictive non-compete, non-solicit, or confidentiality terms that may significantly limit future opportunities",
                                                             "medium" : "Some restrictive terms present; review duration, geographic scope, and industry limitations",
                                                             "low"    : "Reasonable restrictive covenants appropriate for this role and industry standards",
                                                            },
                                 "termination_rights"     : {"high"   : "Unbalanced termination rights with immediate termination, 'at-will' clauses, or unequal notice periods favoring one party",
                                                             "medium" : "Moderately balanced termination provisions; review notice period requirements and severance terms",
                                                             "low"    : "Fair termination rights with reasonable notice periods and balanced severance provisions",
                                                            },
                                 "penalties_liability"    : {"high"   : "Excessive penalty clauses, unlimited liability exposure, or one-sided indemnification terms",
                                                             "medium" : "Some concerning liability terms; review indemnification scope, damage limitations, and warranty provisions",
                                                             "low"    : "Standard liability limitations, reasonable penalty provisions, and balanced indemnification terms",
                                                            },
                                 "compensation_benefits"  : {"high"   : "Compensation structure lacks clarity, contains vague terms, or has unfavorable payment conditions",
                                                             "medium" : "Compensation terms are generally clear but could benefit from more specific bonus structure and payment terms",
                                                             "low"    : "Clear and competitive compensation package with well-defined payment terms and bonus structure",
                                                            },
                                 "intellectual_property"  : {"high"   : "Overly broad IP assignment that may cover personal projects or lacks proper prior IP exclusion",
                                                             "medium" : "IP terms mostly clear but could benefit from stronger prior IP protection and clearer ownership terms",
                                                             "low"    : "Well-defined intellectual property ownership, clear usage rights, and proper prior IP exclusion",
                                                            },
                                 "confidentiality"        : {"high"   : "Overly broad confidentiality scope, perpetual duration, or insufficient protection exceptions",
                                                             "medium" : "Standard confidentiality terms with some areas that could be more precisely defined",
                                                             "low"    : "Reasonable confidentiality provisions with appropriate scope and duration",
                                                            },
                                 "liability_indemnity"    : {"high"   : "Unbalanced indemnification, unlimited liability exposure, or insufficient liability caps",
                                                             "medium" : "Moderate liability terms; review indemnification mutuality and liability limitations",
                                                             "low"    : "Balanced indemnification provisions with reasonable liability limitations",
                                                            },
                                 "governing_law"          : {"high"   : "Unfavorable jurisdiction selection, one-sided dispute resolution, or restrictive venue requirements",
                                                             "medium" : "Standard governing law terms with generally acceptable jurisdiction and dispute resolution",
                                                             "low"    : "Reasonable governing law and jurisdiction provisions favorable to both parties",
                                                            },
                                 "payment_terms"          : {"high"   : "Unfavorable payment terms, extended payment periods, or unclear payment conditions",
                                                             "medium" : "Standard payment terms with some areas that could be improved for cash flow",
                                                             "low"    : "Favorable payment terms with reasonable payment periods and clear conditions",
                                                            },
                                 "warranties"             : {"high"   : "Overly broad warranty disclaimers, insufficient product guarantees, or one-sided warranty terms",
                                                             "medium" : "Standard warranty provisions with typical product/service guarantees",
                                                             "low"    : "Comprehensive warranty coverage with reasonable limitations and clear guarantees",
                                                            },
                                 "dispute_resolution"     : {"high"   : "Unfavorable dispute resolution process, restrictive arbitration clauses, or one-sided legal fee allocation",
                                                             "medium" : "Standard dispute resolution terms with generally fair arbitration or litigation process",
                                                             "low"    : "Reasonable dispute resolution process with fair arbitration and cost allocation",
                                                            },
                                 "assignment_change"      : {"high"   : "Restrictive assignment clauses, one-sided change control, or unfavorable amendment procedures",
                                                             "medium" : "Standard assignment and change control terms with reasonable flexibility",
                                                             "low"    : "Reasonable assignment rights and change control processes favorable to both parties",
                                                            },
                                 "insurance"              : {"high"   : "Insufficient insurance requirements, unclear coverage terms, or inadequate policy specifications",
                                                             "medium" : "Standard insurance requirements with typical coverage expectations",
                                                             "low"    : "Comprehensive insurance requirements with clear coverage specifications",
                                                            },
                                 "force_majeure"          : {"high"   : "Overly narrow force majeure definition, insufficient relief provisions, or one-sided termination rights",
                                                             "medium" : "Standard force majeure clause with typical relief provisions",
                                                             "low"    : "Comprehensive force majeure protection with reasonable relief and termination rights",
                                                            },
                                }
        
    PROTECTION_NAME_MAP       = {"for_cause_definition"     : "For Cause Definition",
                                 "severance_proportion"     : "Severance Provision",
                                 "mutual_indemnification"   : "Mutual Indemnification",
                                 "liability_cap"            : "Liability Cap",
                                 "prior_ip_exclusion"       : "Prior IP Exclusion",
                                 "confidentiality_duration" : "Confidentiality Duration Limit",
                                 "dispute_resolution"       : "Dispute Resolution Process",
                                 "change_control_process"   : "Change Control Process",
                                 "insurance_requirements"   : "Insurance Requirements",
                                 "force_majeure"            : "Force Majeure Protection",
                                }

    @classmethod
    def get_adjusted_weights(cls, contract_type: ContractType) -> Dict[str, float]:
        """
        Get category weights adjusted for contract type
        """
        base_weights = cls.CATEGORY_WEIGHTS.copy()
        adjustments  = cls.CONTRACT_TYPE_ADJUSTMENTS.get(contract_type, {})
        
        adjusted    = dict()

        for category, weight in base_weights.items():
            multiplier         = adjustments.get(category, 1.0)
            adjusted[category] = weight * multiplier
        
        # Normalize to sum to 100
        total            = sum(adjusted.values())

        adjusted_weights = {k: (v / total) * 100 for k, v in adjusted.items()}
        
        return adjusted_weights
    

    @classmethod
    def get_category_description(cls, category: str, score: int) -> str:
        """
        Get meaningful description for a category based on score
        """
        if category not in cls.CATEGORY_DESCRIPTIONS:
            return "Review recommended based on risk score"
        
        if (score >= 70):
            risk_level = "high"

        elif (score >= 40):
            risk_level = "medium"

        else:
            risk_level = "low"
        
        category_description = cls.CATEGORY_DESCRIPTIONS[category][risk_level]
        
        return category_description


    @classmethod
    def get_protection_display_name(cls, protection_id: str) -> str:
        """
        Get the display name for a protection ID: Uses PROTECTION_NAME_MAP for known IDs, otherwise formats the ID
        """
        return cls.PROTECTION_NAME_MAP.get(protection_id, protection_id.replace("_", " ").title())


#########################
# config/model_config.py
##########################
# DEPENDENCIES
from pathlib import Path


class ModelConfig:
    """
    Model-specific configurations - FOR AI MODEL SETTINGS ONLY
    """
    # Directory Settings
    MODEL_DIR = Path("models")
    CACHE_DIR = Path("cache/models")
    
    # Model Architecture Settings
    LEGAL_BERT        = {"model_name"      : "nlpaueb/legal-bert-base-uncased",
                         "local_path"      : MODEL_DIR / "nlpaueb" / "legal-bert-base-uncased",
                         "task"            : "clause-extraction",
                         "max_length"      : 512,
                         "batch_size"      : 16,
                         "hidden_dim"      : 768,
                         "num_layers"      : 12,
                         "attention_heads" : 12,
                         "force_download"  : False, 
                        }
    
    # Embedding Model Settings  
    EMBEDDING_MODEL   = {"model_name"           : "sentence-transformers/all-MiniLM-L6-v2",
                         "local_path"           : MODEL_DIR / "sentence-transformers" / "all-MiniLM-L6-v2",
                         "dimension"            : 384,
                         "pooling"              : "mean",
                         "normalize"            : True,
                         "similarity_threshold" : 0.7,
                         "force_download"       : True,  
                        }

    
    # Classification Model Settings
    CLASSIFIER_MODEL  = {"embedding_dim"    : 384,
                         "hidden_dim"       : 256,
                         "num_categories"   : 12,
                         "dropout_rate"     : 0.1,
                         "learning_rate"    : 2e-5,
                         "max_seq_length"   : 512,
                        }
    
    # Clause Extraction Settings
    CLAUSE_EXTRACTION = {"min_clause_length"    : 50,
                         "max_clause_length"    : 2000,
                         "confidence_threshold" : 0.7,
                         "overlap_threshold"    : 0.3,
                         "max_clauses_per_doc"  : 50,
                        }
    
    # Risk Analysis Settings
    RISK_ANALYSIS     = {"score_ranges"     : {"low"      : (0, 40),
                                               "medium"   : (40, 60),
                                               "high"     : (60, 80),
                                               "critical" : (80, 100),
                                              },
                         "weight_decay"     : 0.1,
                         "smoothing_factor" : 0.5,
                        }
    
    # Market Comparison Settings
    MARKET_COMPARISON = {"similarity_threshold" : 0.75,
                         "min_matches_required" : 3,
                         "max_comparisons"      : 20,
                         "embedding_cache_size" : 1000,
                        }
    
    # LLM Generation Settings
    LLM_GENERATION    = {"max_tokens"        : 5000,
                         "temperature"       : 0.1,
                         "top_p"             : 0.9,
                         "frequency_penalty" : 0.1,
                         "presence_penalty"  : 0.1,
                         "stop_sequences"    : ["\n\n", "###", "---"],
                        }
    
    # Text Processing Settings
    TEXT_PROCESSING   = {"chunk_size"          : 512,
                         "chunk_overlap"       : 50,
                         "min_sentence_length" : 10,
                         "max_sentence_length" : 200,
                         "entity_confidence"   : 0.8,
                        }

    @classmethod
    def ensure_directories(cls):
        """
        Ensure all required directories exist
        """
        directories = [cls.MODEL_DIR,
                       cls.CACHE_DIR,
                       cls.MODEL_DIR / "nlpaueb" / "legal-bert-base-uncased",
                       cls.MODEL_DIR / "sentence-transformers" / "all-MiniLM-L6-v2",
                      ]
                    
        for directory in directories:
            directory.mkdir(parents = True, exist_ok = True)


    @classmethod
    def get_model_config(cls, model_type: str) -> dict:
        """
        Get configuration for specific model type
        """
        config_map = {"legal_bert"        : cls.LEGAL_BERT,
                      "embedding"         : cls.EMBEDDING_MODEL,
                      "classifier"        : cls.CLASSIFIER_MODEL,
                      "clause_extraction" : cls.CLAUSE_EXTRACTION,
                      "risk_analysis"     : cls.RISK_ANALYSIS,
                      "market_comparison" : cls.MARKET_COMPARISON,
                      "llm_generation"    : cls.LLM_GENERATION,
                      "text_processing"   : cls.TEXT_PROCESSING,
                     }
        
        return config_map.get(model_type, {})


################################
# utils/logger.py
#################################
# DEPENDENCIES
import sys
import time
import json
import logging
import traceback
from typing import Any
from typing import Dict
from pathlib import Path
from typing import Optional
from functools import wraps
from datetime import datetime



class ContractAnalyzerLogger:
    """
    Production-grade logging for contract analysis
    Features:
    - Structured JSON logging
    - Separate files for errors/warnings
    - Request ID tracking
    - Performance metrics
    - Log rotation
    """
    _loggers : Dict[str, logging.Logger] = dict()
    _log_dir : Optional[Path]            = None
    
    # Log levels
    DEBUG                                = logging.DEBUG
    INFO                                 = logging.INFO
    WARNING                              = logging.WARNING
    ERROR                                = logging.ERROR
    CRITICAL                             = logging.CRITICAL
    

    @classmethod
    def setup(cls, log_dir: str = "logs", app_name: str = "contract_analyzer"):
        """
        Setup logging system
        
        Arguments:
        ----------
            log_dir  { str } : Directory for log files

            app_name { str } : Application name for log files
        """
        cls._log_dir = Path(log_dir)
        cls._log_dir.mkdir(parents = True, exist_ok = True)
        
        # Create main logger
        cls._create_logger(name     = app_name,
                           log_file = cls._log_dir / f"{app_name}.log",
                           level    = logging.INFO,
                          )
        
        # Create error logger
        cls._create_logger(name     = f"{app_name}.error",
                           log_file = cls._log_dir / f"{app_name}_error.log",
                           level    = logging.ERROR,
                          )
        
        # Create performance logger
        cls._create_logger(name     = f"{app_name}.performance",
                           log_file = cls._log_dir / f"{app_name}_performance.log",
                           level    = logging.INFO,
                          )
        
        print(f"[Logger] Logging initialized. Logs: {cls._log_dir}")
    

    @classmethod
    def _create_logger(cls, name: str, log_file: Path, level: int) -> logging.Logger:
        """
        Create and configure a logger
        """
        logger             = logging.getLogger(name)
        logger.setLevel(level)

        # Clear existing handlers
        logger.handlers.clear()  
        
        # File handler
        file_handler       = logging.FileHandler(log_file)
        file_handler.setLevel(level)
        
        # Console handler (for errors and above)
        console_handler    = logging.StreamHandler(sys.stdout)
        console_handler.setLevel(logging.WARNING)
        
        # Formatter
        formatter          = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s', datefmt = '%Y-%m-%d %H:%M:%S')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        cls._loggers[name] = logger

        return logger
    

    @classmethod
    def get_logger(cls, name: str = "contract_analyzer") -> logging.Logger:
        """
        Get logger by name
        """
        if name not in cls._loggers:
            # Lazy initialization
            cls.setup()

        return cls._loggers.get(name, logging.getLogger(name))
    

    @classmethod
    def log_structured(cls, level: int, message: str, **kwargs):
        """
        Log structured data as JSON
        
        Arguments:
        ----------
            level      { int } : Log level

            message    { str } : Log message
            
            **kwargs           : Additional structured data
        """
        logger   = cls.get_logger()
        
        log_data = {"timestamp"  : datetime.now().isoformat(),
                    "message"    : message,
                    **kwargs
                   }
        
        logger.log(level, json.dumps(log_data))
    

    @classmethod
    def log_error(cls, error: Exception, context: Dict[str, Any] = None):
        """
        Log error with full traceback and context
        
        Arguments:
        ----------
            error      { Exception } : Exception object

            context      { dict }    : Additional context dictionary
        """
        error_logger = cls._loggers.get("contract_analyzer.error")
        
        if not error_logger:
            error_logger = cls.get_logger()
        
        error_data = {"timestamp"     : datetime.now().isoformat(),
                      "error_type"    : type(error).__name__,
                      "error_message" : str(error),
                      "traceback"     : traceback.format_exc(),
                      "context"       : context or {},
                     }
        
        error_logger.error(json.dumps(error_data, indent = 2))
    

    @classmethod
    def log_performance(cls, operation: str, duration: float, **metrics):
        """
        Log performance metrics
        
        Arguments:
        ----------
            operation  { str }  : Operation name

            duration  { float } : Duration in seconds
            
            **metrics           : Additional metrics
        """
        perf_logger = cls._loggers.get("contract_analyzer.performance")
        if not perf_logger:
            perf_logger = cls.get_logger()
        
        perf_data = {"timestamp"        : datetime.now().isoformat(),
                     "operation"        : operation,
                     "duration_seconds" : round(duration, 3),
                     **metrics
                    }
        
        perf_logger.info(json.dumps(perf_data))
    

    @staticmethod
    def log_execution_time(operation_name: str = None):
        """
        Decorator to log execution time of functions
        """
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                op_name    = operation_name or func.__name__
                start_time = time.time()
                
                try:
                    result   = func(*args, **kwargs)
                    duration = time.time() - start_time
                    
                    ContractAnalyzerLogger.log_performance(operation = op_name,
                                                           duration  = duration,
                                                           status    = "success",
                                                          )
                    
                    return result
                
                except Exception as e:
                    duration = time.time() - start_time
                    
                    ContractAnalyzerLogger.log_performance(operation = op_name,
                                                           duration  = duration,
                                                           status    = "error",
                                                           error     = str(e),
                                                          )
                    
                    ContractAnalyzerLogger.log_error(e, context = {"operation" : op_name})
                    raise
            
            return wrapper

        return decorator



# Convenience functions
def get_logger(name: str = "contract_analyzer") -> logging.Logger:
    """
    Get logger instance
    """
    return ContractAnalyzerLogger.get_logger(name)


def log_info(message: str, **kwargs):
    """
    Log info message
    """
    ContractAnalyzerLogger.log_structured(logging.INFO, message, **kwargs)


def log_warning(message: str, **kwargs):
    """
    Log warning message
    """
    ContractAnalyzerLogger.log_structured(logging.WARNING, message, **kwargs)


def log_error(error: Exception, context: Dict[str, Any] = None):
    """
    Log error with context
    """
    ContractAnalyzerLogger.log_error(error, context)


def log_debug(message: str, **kwargs):
    """
    Log debug message
    """
    ContractAnalyzerLogger.log_structured(logging.DEBUG, message, **kwargs)


#############################
# utils/document_reader.py
#############################
# DEPENDENCIES
import io
import re
import sys
import PyPDF2
from typing import Any
from typing import Dict
from typing import Union
from pathlib import Path
from docx import Document
from typing import Optional

# Add parent directory to path for imports
#sys.path.append(str(Path(__file__).parent.parent))

from config.settings import settings


try:
    # For PyMuPDF
    import fitz  
    PYMUPDF_AVAILABLE = True

except ImportError:
    PYMUPDF_AVAILABLE = False
    print("[DocumentReader] PyMuPDF not available. Install with: pip install PyMuPDF")

# Encoding detection
try:
    import chardet
    CHARDET_AVAILABLE = True

except ImportError:
    CHARDET_AVAILABLE = False



class DocumentReader:
    """
    Document reader supporting PDF and DOCX : Uses PyMuPDF for better PDF extraction when available
    """
    # File Size Constraint 
    MAX_FILE_SIZE   = settings.MAX_UPLOAD_SIZE

    # File Type Constraint
    ALLOWED_TYPES   = settings.ALLOWED_EXTENSIONS

    # Minimum extracted text length
    MIN_TEXT_LENGTH = settings.MIN_CONTRACT_LENGTH  
    

    @staticmethod
    def read_file(file_path_or_bytes: Union[str, Path, io.BytesIO], file_type: str = "pdf") -> str:
        """
        Read document and extract text with validation
        
        Arguments:
        ----------
            file_path_or_bytes { str / Path / BytesIO } : File path (str/Path) or bytes object (io.BytesIO)
            
            file_type                   { str }         : "pdf" or "docx"
        
        Returns:
        --------
                            { str }                     : Extracted and cleaned text
            
        Raises:
        -------
            ValueError                                  : If file type unsupported or validation fails

            Exception                                   : If extraction fails
        """
        # Normalize file_type by removing any dots and converting to lowercase
        normalized_file_type = file_type.lower().replace('.', '')

        # Validate file size
        DocumentReader._validate_file_size(file_path_or_bytes = file_path_or_bytes)

        # Use normalized file type for routing
        if (normalized_file_type == "pdf"):
            text = DocumentReader._read_pdf(file_or_bytes = file_path_or_bytes)

        elif (normalized_file_type in ["docx", "doc"]):
            text = DocumentReader._read_docx(file_or_bytes = file_path_or_bytes) 
        
        elif (normalized_file_type == "txt"):
            text = DocumentReader._read_txt(file_or_bytes = file_path_or_bytes)

        else:
            raise ValueError(f"Unsupported file type: {file_type}")
        
        # Validate extracted text
        if (len(text.strip()) < DocumentReader.MIN_TEXT_LENGTH):
            raise ValueError(f"Extracted text too short ({len(text)} chars). Minimum: {DocumentReader.MIN_TEXT_LENGTH} chars. File may be corrupted or empty.")
        
        return text

    
    @staticmethod
    def _validate_file_size(file_path_or_bytes: Union[str, Path, io.BytesIO]) -> None:
        """
        Validate file size is within limits
        """
        if isinstance(file_path_or_bytes, (str, Path)):
            size = Path(file_path_or_bytes).stat().st_size
        
        else:
            # It's a file-like object
            current_pos = file_path_or_bytes.tell()
            # Seek to end
            file_path_or_bytes.seek(0, 2)  
            
            size        = file_path_or_bytes.tell()

            # Reset to original position
            file_path_or_bytes.seek(current_pos)  
        
        if (size > DocumentReader.MAX_FILE_SIZE):
            raise ValueError(f"File too large: {size / (1024 * 1024):.1f}MB. Maximum allowed: {DocumentReader.MAX_FILE_SIZE / (1024 * 1024):.1f}MB")
        
        if (size == 0):
            raise ValueError("File is empty (0 bytes)")
    

    @staticmethod
    def _read_pdf(file_or_bytes: Union[str, Path, io.BytesIO]) -> str:
        """
        Read PDF with PyMuPDF (preferred) or PyPDF2 (fallback)
        """
        if PYMUPDF_AVAILABLE:
            try:
                return DocumentReader._read_pdf_pymupdf(file_or_bytes = file_or_bytes)
            
            except Exception as e:
                print(f"[DocumentReader] PyMuPDF failed: {e}, falling back to PyPDF2")
                return DocumentReader._read_pdf_pypdf2(file_or_bytes)
        
        else:
            return DocumentReader._read_pdf_pypdf2(file_or_bytes = file_or_bytes)
    

    @staticmethod
    def _read_pdf_pymupdf(file_or_bytes: Union[str, Path, io.BytesIO]) -> str:
        """
        Read PDF using PyMuPDF (superior text extraction)
        """
        # Handle both file paths and bytes
        if isinstance(file_or_bytes, (str, Path)):
            doc = fitz.open(file_or_bytes)
        
        else:
            # It's a file-like object
            file_or_bytes.seek(0)
            file_content = file_or_bytes.read()
            doc          = fitz.open(stream   = file_content, 
                                     filetype = "pdf",
                                    )
        
        text = ""

        for page_num in range(doc.page_count):
            page      = doc[page_num]
            
            # Extract text with layout preservation
            page_text = page.get_text("text", sort = True)
            
            # Clean the text
            page_text = DocumentReader._clean_extracted_text(text = page_text)
            text     += page_text + "\n\n"
        
        doc.close()
        
        # Post-process entire document
        text = DocumentReader._post_process_text(text = text)
        
        return text
    

    @staticmethod
    def _read_pdf_pypdf2(file_or_bytes: Union[str, Path, io.BytesIO]) -> str:
        """
        Read PDF using PyPDF2 (fallback)
        """
        try:
            # Handle both file paths and bytes
            if isinstance(file_or_bytes, (str, Path)):
                with open(file_or_bytes, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    text       = DocumentReader._extract_from_pypdf2(pdf_reader = pdf_reader)
            
            else:
                file_or_bytes.seek(0)
                pdf_reader = PyPDF2.PdfReader(stream = file_or_bytes)
                text       = DocumentReader._extract_from_pypdf2(pdf_reader = pdf_reader)
            
            text = DocumentReader._post_process_text(text = text)
            
            return text
            
        except Exception as e:
            raise Exception(f"PDF extraction failed: {repr(e)}")
    

    @staticmethod
    def _extract_from_pypdf2(pdf_reader: PyPDF2.PdfReader) -> str:
        """
        Extract text from PyPDF2 reader
        """
        text = ""

        for page in pdf_reader.pages:
            page_text = page.extract_text()
            
            if page_text:
                page_text = DocumentReader._clean_extracted_text(text = page_text)
                text     += page_text + "\n\n"

        return text
    

    @staticmethod
    def _read_docx(file_or_bytes: Union[str, Path, io.BytesIO]) -> str:
        """
        Read DOCX file
        """
        try:
            # Handle both file paths and bytes
            if (isinstance(file_or_bytes, (str, Path))):
                doc = Document(file_or_bytes)
            
            else:
                file_or_bytes.seek(0)
                doc = Document(file_or_bytes)
            
            text = ""
            
            # Extract paragraphs
            for paragraph in doc.paragraphs:
                if (paragraph.text.strip()):
                    clean_text = DocumentReader._clean_extracted_text(text = paragraph.text)
                    text      += clean_text + "\n"
            
            # Extract tables
            for table in doc.tables:
                for row in table.rows:
                    for cell in row.cells:
                        if cell.text.strip():
                            clean_text = DocumentReader._clean_extracted_text(text = cell.text)
                            text      += clean_text + " "

                    text += "\n"
            
            text = DocumentReader._post_process_text(text = text)
            
            return text
            
        except Exception as e:
            raise Exception(f"DOCX extraction failed: {repr(e)}")
    
    
    @staticmethod
    def _read_txt(file_or_bytes: Union[str, Path, io.BytesIO]) -> str:
        """
        Read text file with encoding detection
        """
        try:
            if isinstance(file_or_bytes, (str, Path)):
                with open(file_or_bytes, 'rb') as f:
                    content = f.read()
            
            else:
                file_or_bytes.seek(0)
                content = file_or_bytes.read()
            
            # Detect encoding
            encoding = DocumentReader.detect_encoding(content)
            
            # Decode with detected encoding, fallback to utf-8
            try:
                text = content.decode(encoding)
            
            except UnicodeDecodeError:
                text = content.decode('utf-8', errors = 'replace')
            
            return DocumentReader._post_process_text(text=text)
        
        except Exception as e:
            raise Exception(f"TXT extraction failed: {repr(e)}")


    @staticmethod
    def _clean_extracted_text(text: str) -> str:
        """
        Clean and normalize extracted text
        """
        if not text:
            return ""
        
        # Replace multiple newlines with single newline
        text = re.sub(r'\n\s*\n', '\n\n', text)
        
        # Fix hyphenated words split across lines
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        
        # Normalize whitespace (but preserve single newlines)
        text = re.sub(r'[ \t]+', ' ', text)
        
        # Remove page numbers on separate lines
        text = re.sub(r'\n\s*\d+\s*\n', '\n', text)
        
        # Remove lines with just numbers
        text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
        
        return text.strip()
    

    @staticmethod
    def _post_process_text(text: str) -> str:
        """
        Post-process entire extracted text
        """
        if not text:
            return ""
        
        # Remove excessive empty lines
        text  = re.sub(r'\n{3,}', '\n\n', text)
        
        # Fix mid-sentence line breaks (lowercase to lowercase)
        text  = re.sub(r'(?<=[a-z,])\n(?=[a-z])', ' ', text)
        
        # Ensure proper spacing around section numbers
        text  = re.sub(r'(\d+\.\d+)([A-Za-z])', r'\1 \2', text)
        
        # Remove excessive spaces
        text  = re.sub(r' {2,}', ' ', text)
        
        # Remove whitespace at line starts/ends
        lines = [line.strip() for line in text.split('\n')]
        text  = '\n'.join(lines)
        
        return text.strip()
    

    @staticmethod
    def extract_metadata(file_path_or_bytes: Union[str, Path, io.BytesIO], file_type: str = "pdf") -> Dict[str, Any]:
        """
        Extract document metadata (pages, author, creation date, etc.)
        
        Arguments:
        ----------
            file_path_or_bytes { str / Path / BytesIO } : File path or bytes object
            
            file_type                  { str }          : "pdf" or "docx"
        
        Returns:
        --------
                        { dict }                        : Dictionary containing metadata
        """
        metadata = {"pages"     : 0,
                    "title"     : "",
                    "author"    : "",
                    "creator"   : "",
                    "created"   : "",
                    "modified"  : "",
                    "file_type" : file_type,
                   }
        
        try:
            if ((file_type == "pdf") and PYMUPDF_AVAILABLE):
                if isinstance(file_path_or_bytes, (str, Path)):
                    doc = fitz.open(file_path_or_bytes)
                
                else:
                    file_path_or_bytes.seek(0)
                    file_content = file_path_or_bytes.read()
                    doc          = fitz.open(stream   = file_content, 
                                             filetype = "pdf",
                                            )
                
                metadata.update({"pages"    : doc.page_count,
                                 "title"    : doc.metadata.get("title", ""),
                                 "author"   : doc.metadata.get("author", ""),
                                 "creator"  : doc.metadata.get("creator", ""),
                                 "created"  : doc.metadata.get("creationDate", ""),
                                 "modified" : doc.metadata.get("modDate", ""),
                               })

                doc.close()
            
            elif (file_type in ["docx", "doc"]):
                if (isinstance(file_path_or_bytes, (str, Path))):
                    doc = Document(file_path_or_bytes)
                
                else:
                    file_path_or_bytes.seek(0)
                    doc = Document(file_path_or_bytes)
                
                core_props = doc.core_properties
                metadata.update({"pages"    : len(doc.sections),
                                 "title"    : core_props.title or "",
                                 "author"   : core_props.author or "",
                                 "creator"  : core_props.author or "",
                                 "created"  : str(core_props.created) if core_props.created else "",
                                 "modified" : str(core_props.modified) if core_props.modified else "",
                               })
        
        except Exception as e:
            print(f"[DocumentReader] Metadata extraction failed: {repr(e)}")
        
        return metadata

    
    @staticmethod
    def detect_encoding(file_bytes: bytes) -> str:
        """
        Detect text encoding for better extraction
        
        Arguments:
        ----------
            file_bytes { bytes } : Raw file bytes
        
        Returns:
        --------
                { str }          : Detected encoding (e.g., 'utf-8', 'latin-1')
        """
        if not CHARDET_AVAILABLE:
            return 'utf-8'
        
        try:
            # Check first 10KB
            result = chardet.detect(file_bytes[:10000])  
            return result['encoding'] or 'utf-8'
        
        except Exception:
            return 'utf-8'
    

    @staticmethod
    def validate_file_integrity(file_path: Union[str, Path]) -> tuple[bool, str]:
        """
        Validate file isn't corrupted and is readable
        
        Arguments:
        ----------
            file_path { str } : Path to file
        
        Returns:
        --------
               { tuple }      : (is_valid, message) tuple
        """
        try:
            file_path = Path(file_path)
            
            # Check file exists
            if not file_path.exists():
                return False, "File does not exist"
            
            # Check file size
            file_size = file_path.stat().st_size
            
            if (file_size == 0):
                return (False, "File is empty (0 bytes)")
            
            # Less than 1KB
            if (file_size < 1024):  
                return (False, f"File suspiciously small ({file_size} bytes)")
            
            # Check file is readable
            with open(file_path, 'rb') as f:
                # Try reading first KB
                f.read(1024)  
            
            return (True, "File integrity OK")
            
        except PermissionError:
            return (False, "Permission denied - cannot read file")
        
        except Exception as e:
            return (False, f"File integrity check failed: {repr(e)}")


############################
# utils/text_processor.py
############################
# DEPENDENCIES
import re
from typing import Any
from typing import List
from typing import Dict
from typing import Optional
from difflib import SequenceMatcher

# Advanced NLP (optional but recommended)
try:
    import spacy
    SPACY_AVAILABLE = True

except ImportError:
    SPACY_AVAILABLE = False
    print("[TextProcessor] spaCy not available. Install with: pip install spacy && python -m spacy download en_core_web_sm")

# Language detection
try:
    from langdetect import detect, LangDetectException
    LANGDETECT_AVAILABLE = True
    
except ImportError:
    LANGDETECT_AVAILABLE = False


class TextProcessor:
    """
    Text processing and normalization utilities
    """
    def __init__(self, use_spacy: bool = True):
        """
        Initialize text processor
        
        Arguments:
        ----------
            use_spacy { bool } : Whether to use spaCy for advanced NLP (if available)
        """
        self.nlp = None
        
        if use_spacy and SPACY_AVAILABLE:
            try:
                self.nlp = spacy.load("en_core_web_sm")

                print("[TextProcessor] spaCy model loaded successfully")
            
            except OSError:
                print("[TextProcessor] spaCy model not found. Run: python -m spacy download en_core_web_sm")
                self.nlp = None
    

    @staticmethod
    def normalize_text(text: str, lowercase: bool = True, remove_special_chars: bool = False) -> str:
        """
        Normalize text for analysis
        
        Arguments:
        ----------
            text                 { str }  : Input text

            lowercase            { bool } : Convert to lowercase

            remove_special_chars { bool } : Remove special characters
        
        Returns:
        --------
                      { str }             : Normalized text
        """
        if lowercase:
            text = text.lower()
        
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        
        if remove_special_chars:
            # Keep alphanumeric and basic punctuation
            text = re.sub(r'[^\w\s.,;:!?()\-\'\"&@#$%]', '', text)
        
        return text.strip()
    

    @staticmethod
    def split_into_paragraphs(text: str, min_length: int = 20) -> List[str]:
        """
        Split text into paragraphs
        
        Arguments:
        ----------
            text       { str } : Input text

            min_length { int } : Minimum paragraph length in characters
        
        Returns:
        --------
                { list }       : List of paragraphs
        """
        # Split on double newlines
        paragraphs = re.split(r'\n\s*\n', text)
        
        # Filter short and empty paragraphs
        return [p.strip() for p in paragraphs if len(p.strip()) >= min_length]
    

    @staticmethod
    def extract_sentences(text: str, min_length: int = 10) -> List[str]:
        """
        Extract sentences from text (basic method)
        
        Arguments:
        ----------
            text       { str } : Input text

            min_length { int  } : Minimum sentence length in characters
        
        Returns:
        --------
                { list }        : List of sentences
        """
        # Simple sentence splitting on .!?
        sentences = re.split(r'[.!?]+', text)
        
        # Clean and filter
        sentences = [s.strip() for s in sentences if len(s.strip()) >= min_length]
        
        return sentences
    

    def extract_sentences_advanced(self, text: str) -> List[Dict[str, Any]]:
        """
        Extract sentences with NER and metadata using spaCy
        
        Args:
            text: Input text
        
        Returns:
            List of sentence dictionaries with entities and metadata
        """
        if not self.nlp:
            # Fallback to basic extraction
            basic_sentences = self.extract_sentences(text)

            return [{"text" : s, "entities" : [], "start_char" : 0, "end_char" : 0} for s in basic_sentences]
        
        # Limit to 100K chars for performance
        doc       = self.nlp(text[:100000])  
        sentences = list()
        
        for sent in doc.sents:
            sentences.append({"text"       : sent.text.strip(),
                              "entities"   : [(ent.text, ent.label_) for ent in sent.ents],
                              "start_char" : sent.start_char,
                              "end_char"   : sent.end_char,
                              "tokens"     : [token.text for token in sent],
                            })
        
        return sentences
    
    
    @staticmethod
    def extract_legal_entities(text: str) -> Dict[str, List[str]]:
        """
        Extract legal-specific entities (parties, dates, amounts, references)
        
        Arguments:
        ----------
            text { str } : Input text
        
        Returns:
        --------
            { dict }     : Dictionary of extracted entities by type
        """
        entities       = {"parties"       : [],
                          "dates"         : [],
                          "amounts"       : [],
                          "addresses"     : [],
                          "references"    : [],
                          "emails"        : [],
                          "phone_numbers" : [],
                         }
        
        # Party names (PARTY A, "the Employee", Company Name Inc.)
        party_patterns = [r'(?:PARTY|Party)\s+[A-Z]',
                          r'"the\s+\w+"',
                          r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\s+(?:Inc|LLC|Corp|Ltd|Limited|Company)\.?',
                          r'(?:the\s+)?(Employer|Employee|Consultant|Contractor|Client|Vendor|Supplier|Landlord|Tenant|Buyer|Seller)',
                         ]

        for pattern in party_patterns:
            matches = re.findall(pattern, text)

            entities["parties"].extend(matches)
        
        # Dates (various formats)
        date_patterns = [r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b',
                         r'\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{1,2},?\s+\d{4}\b',
                         r'\b\d{1,2}\s+(?:January|February|March|April|May|June|July|August|September|October|November|December)\s+\d{4}\b'
                        ]

        for pattern in date_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)

            entities["dates"].extend(matches)
        
        # Legal references (Section 5.2, Clause 11.1, Article III)
        ref_patterns = [r'(?:Section|Clause|Article|Paragraph|Exhibit|Schedule|Appendix)\s+(?:\d+(?:\.\d+)*|[IVXLCDM]+)']

        for pattern in ref_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)

            entities["references"].extend(matches)
        
        # Monetary amounts
        entities["amounts"]       = TextProcessor.extract_monetary_amounts(text)
        
        # Email addresses
        email_pattern             = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
        entities["emails"]        = re.findall(email_pattern, text)
        
        # Phone numbers (US format)
        phone_pattern             = r'\b(?:\+?1[-.\s]?)?\(?([0-9]{3})\)?[-.\s]?([0-9]{3})[-.\s]?([0-9]{4})\b'
        phone_matches             = re.findall(phone_pattern, text)
        entities["phone_numbers"] = ['-'.join(match) for match in phone_matches]
        
        # Deduplicate
        for key in entities:
            entities[key] = list(set(entities[key]))
        
        return entities
    

    @staticmethod
    def count_words(text: str) -> int:
        """
        Count words in text
        """
        return len(text.split())
    

    @staticmethod
    def extract_numbers(text: str) -> List[str]:
        """
        Extract all numbers from text
        """
        return re.findall(r'\d+', text)
    

    @staticmethod
    def extract_monetary_amounts(text: str) -> List[str]:
        """
        Extract monetary amounts from text
        
        Returns:
        --------
            { list }    : List of monetary amounts (e.g., ['$1,000', '$2,500.00'])
        """
        # Match patterns like $1,000 or $1000.00 or USD 1,000
        patterns = [r'\$[\d,]+(?:\.\d{2})?',
                    r'USD\s*[\d,]+(?:\.\d{2})?',
                    r'EUR\s*[\d,]+(?:\.\d{2})?',
                    r'GBP\s*[\d,]+(?:\.\d{2})?'
                   ]
        
        amounts  = list()

        for pattern in patterns:
            amounts.extend(re.findall(pattern, text, re.IGNORECASE))
        
        return amounts

    
    @staticmethod
    def extract_durations(text: str) -> List[Dict[str, str]]:
        """
        Extract time durations (e.g., "6 months", "2 years")
        
        Returns:
        --------
            { list }    : List of duration dictionaries with 'amount' and 'unit'
        """
        pattern = r'(\d+)\s*(day|week|month|year)s?'
        matches = re.findall(pattern, text, re.IGNORECASE)
        
        return [{"amount": m[0], "unit": m[1].lower()} for m in matches]


    @staticmethod
    def extract_percentages(text: str) -> List[str]:
        """
        Extract percentages from text
        """
        return re.findall(r'\d+(?:\.\d+)?%', text)
    

    @staticmethod
    def chunk_text_for_embedding(text: str, chunk_size: int = 512, overlap: int = 50) -> List[Dict[str, Any]]:
        """
        Chunk text with overlap for embedding models (preserves sentence boundaries)
        
        Arguments:
        ----------
            text       { str } : Input text

            chunk_size { int } : Maximum chunk size in words
            
            overlap    { int } : Number of words to overlap between chunks
        
        Returns:
        --------
                { list }       : List of chunk dictionaries with metadata
        """
        sentences          = TextProcessor.extract_sentences(text)
        chunks             = list()
        current_chunk      = list()
        current_length     = 0
        start_sentence_idx = 0
        
        for i, sentence in enumerate(sentences):
            sentence_words  = sentence.split()
            sentence_length = len(sentence_words)
            
            if (((current_length + sentence_length) > chunk_size) and current_chunk):
                # Save current chunk
                chunks.append({"text"           : " ".join(current_chunk),
                               "start_sentence" : start_sentence_idx,
                               "end_sentence"   : i - 1,
                               "word_count"     : current_length,
                               "chunk_id"       : len(chunks),
                             })
                
                # Start new chunk with overlap
                overlap_sentences  = current_chunk[-2:] if (len(current_chunk) > 2) else current_chunk
                current_chunk      = overlap_sentences + [sentence]
                current_length     = sum(len(s.split()) for s in current_chunk)
                start_sentence_idx = max(0, i - len(overlap_sentences))

            else:
                current_chunk.append(sentence)
                current_length += sentence_length
        
        # Add final chunk
        if current_chunk:
            chunks.append({"text"           : " ".join(current_chunk),
                           "start_sentence" : start_sentence_idx,
                           "end_sentence"   : len(sentences) - 1,
                           "word_count"     : current_length,
                           "chunk_id"       : len(chunks),
                         })
        
        return chunks
    

    @staticmethod
    def text_similarity(text1: str, text2: str) -> float:
        """
        Calculate similarity between two texts (0-1 scale)
        
        Arguments:
        ----------
            text1 { str } : First text

            text2 { str } : Second text
        
        Returns:
        --------
            { float }     : Similarity score (0.0 = completely different, 1.0 = identical)
        """
        return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()
    

    @staticmethod
    def deduplicate_clauses(clauses: List[str], threshold: float = 0.85) -> List[str]:
        """
        Remove near-duplicate clauses
        
        Arguments:
        ----------
            clauses   { list }  : List of clause texts

            threshold { float } : Similarity threshold for deduplication (0.0-1.0)
        
        Returns:
        --------
                { list }        : List of unique clauses
        """
        unique = list()
        
        for clause in clauses:
            is_duplicate = any(TextProcessor.text_similarity(clause, existing) > threshold for existing in unique)

            if not is_duplicate:
                unique.append(clause)
        
        return unique
    

    @staticmethod
    def detect_language(text: str) -> str:
        """
        Detect text language
        
        Arguments:
        ----------
            text { str } : Input text
        
        Returns:
        --------
             { str }     : ISO 639-1 language code (e.g., 'en', 'es', 'fr')
        """
        if not LANGDETECT_AVAILABLE:
            # Default to English
            return "en"  
        
        try:
            # Use first 1000 chars for detection
            return detect(text[:1000])

        except LangDetectException:
            return "en"
    
    
    @staticmethod
    def get_text_statistics(text: str) -> Dict[str, Any]:
        """
        Get comprehensive text statistics
        
        Returns:
        --------
            { dict }    : Dictionary with character count, word count, sentence count, etc.
        """
        sentences  = TextProcessor.extract_sentences(text)
        paragraphs = TextProcessor.split_into_paragraphs(text)
        words      = text.split()
        
        return {"character_count"        : len(text),
                "word_count"             : len(words),
                "sentence_count"         : len(sentences),
                "paragraph_count"        : len(paragraphs),
                "avg_words_per_sentence" : len(words) / len(sentences) if sentences else 0,
                "avg_chars_per_word"     : len(text) / len(words) if words else 0,
                "language"               : TextProcessor.detect_language(text),
               }
    

    @staticmethod
    def highlight_keywords(text: str, keywords: List[str], highlight_format: str = "**{}**") -> str:
        """
        Highlight keywords in text (for display purposes)
        
        Arguments:
        ----------
            text             { str }  : Input text

            keywords         { list } : List of keywords to highlight
            
            highlight_format { str  } : Format string with {} placeholder (default: Markdown bold)
        
        Returns:
        --------
                    { str }           : Text with highlighted keywords
        """
        for keyword in keywords:
            pattern = re.compile(re.escape(keyword), re.IGNORECASE)
            text    = pattern.sub(lambda m: highlight_format.format(m.group(0)), text)
        
        return text
    
    
    @staticmethod
    def extract_numbered_sections(text: str) -> List[Dict[str, Any]]:
        """
        Extract numbered sections/clauses (1.1, 1.2, Article 5, etc.)
        
        Returns:
        --------
            { list }    : List of section dictionaries with number and text
        """
        patterns = [(r'(\d+\.\d+(?:\.\d+)*)\.\s*([^\n]{20,}?)(?=\n\s*\d+\.\d+|\n\n|$)', 'numbered'),
                    (r'(Article\s+(?:\d+|[IVXLCDM]+))\.\s*([^\n]{20,}?)(?=\nArticle|\n\n|$)', 'article'),
                    (r'(Section\s+(?:\d+|[IVXLCDM]+))\.\s*([^\n]{20,}?)(?=\nSection|\n\n|$)', 'section'),
                    (r'(Clause\s+\d+(?:\.\d+)*)\.\s*([^\n]{20,}?)(?=\nClause|\n\n|$)', 'clause'),
                   ]
        
        sections = list()

        for pattern, section_type in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)
            
            for match in matches:
                sections.append({"reference" : match.group(1).strip(),
                                 "text"      : match.group(2).strip(),
                                 "type"      : section_type,
                                 "start_pos" : match.start(),
                                 "end_pos"   : match.end(),
                               })
        
        # Sort by position
        sections.sort(key = lambda x: x['start_pos'])
        
        return sections
    

    @staticmethod
    def clean_legal_text(text: str) -> str:
        """
        Clean legal text by removing boilerplate artifacts
        
        Arguments:
        ----------
            text { str } : Input legal text
        
        Returns:
        --------
             { str }     : Cleaned text
        """
        # Remove "Page X of Y" markers
        text = re.sub(r'Page\s+\d+\s+of\s+\d+', '', text, flags = re.IGNORECASE)
        
        # Remove "[Signature Page Follows]" type markers
        text = re.sub(r'\[.*?(?:Signature|Initial|Page).*?\]', '', text, flags = re.IGNORECASE)
        
        # Remove excessive underscores (signature lines)
        text = re.sub(r'_{3,}', '', text)
        
        # Remove "CONFIDENTIAL" watermarks
        text = re.sub(r'\b(CONFIDENTIAL|DRAFT|INTERNAL USE ONLY)\b', '', text, flags = re.IGNORECASE)
        
        # Clean up resulting whitespace
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r' {2,}', ' ', text)
        
        return text.strip()


#####################
# utils/validators.py
#####################
# DEPENDENCIES
import re
import os
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path


class ContractValidator:
    """
    Validate if document is a legal contract
    """
    # File constraints
    MIN_CONTRACT_LENGTH = 500    
    MAX_CONTRACT_LENGTH = 500000  # 500KB text
    
    # Strong indicators of legal contracts (keyword: weight)
    STRONG_INDICATORS   = {'agreement'             : 3, 
                           'contract'              : 3, 
                           'party'                 : 2, 
                           'parties'               : 2, 
                           'whereas'               : 5, 
                           'hereinafter'           : 5, 
                           'witnesseth'            : 5, 
                           'indemnification'       : 4, 
                           'liability'             : 3, 
                           'confidentiality'       : 3, 
                           'termination'           : 3, 
                           'governing law'         : 4, 
                           'jurisdiction'          : 3, 
                           'warranty'              : 3, 
                           'representation'        : 3, 
                           'covenant'              : 4, 
                           'clause'                : 3, 
                           'section'               : 2,
                           'article'               : 2, 
                           'hereby'                : 3, 
                           'undersigned'           : 4, 
                           'executed'              : 3,
                           'consideration'         : 4, 
                           'effective date'        : 3, 
                           'in witness whereof'    : 5, 
                           'binding'               : 3, 
                           'enforceable'           : 3, 
                           'obligations'           : 2,
                           'employment'            : 3, 
                           'employee'              : 2, 
                           'employer'              : 2,
                           'probation'             : 3, 
                           'salary'                : 2, 
                           'compensation'          : 3,
                           'non-compete'           : 4, 
                           'non-solicit'           : 4,
                           'remuneration'          : 3, 
                           'indemnity'             : 3, 
                           'intellectual property' : 4,
                           'confidential'          : 2, 
                           'proprietary'           : 2, 
                           'post-termination'      : 3,
                           'agrees to'             : 2, 
                           'shall not'             : 2, 
                           'agrees and accepts'    : 3,
                           'subject to'            : 1, 
                           'in accordance with'    : 2,
                          }
    
    # Anti-patterns (things that indicate NOT a contract)
    ANTI_PATTERNS       = {'case law'           : 5, 
                           'plaintiff'          : 5, 
                           'defendant'          : 5, 
                           'supreme court'      : 5, 
                           'appellate court'    : 5, 
                           'court held'         : 5, 
                           'legal opinion'      : 4, 
                           'court of appeals'   : 5, 
                           'trial court'        : 5, 
                           'article written by' : 4, 
                           'blog post'          : 5, 
                           'this article'       : 3,
                           'author:'            : 3, 
                           'published in'       : 3, 
                           'journal of'         : 3, 
                           'abstract:'          : 4, 
                           'introduction:'      : 3, 
                           'conclusion:'        : 3, 
                           'table of contents'  : 4, 
                           'bibliography'       : 4,
                           'references:'        : 3, 
                           'chapter'            : 2, 
                           'section i.'         : 2, 
                           'section ii.'        : 2,
                          }
    

    @staticmethod
    def is_valid_contract(text: str, min_length: int = None) -> Tuple[bool, str, str]:
        """
        Comprehensive contract validation with relaxed thresholds
        
        Arguments:
        ----------
            text       { str } : Document text to validate
            
            min_length { int } : Minimum length override (optional)
        
        Returns:
        --------
               { tuple }       : (is_valid, validation_type, message) tuple
        """
        min_length = min_length or ContractValidator.MIN_CONTRACT_LENGTH
        text_lower = text.lower().strip()
        
        # Length Validation
        if (len(text_lower) < min_length):
            return (False, "too_short", f"Text too short ({len(text_lower)} chars, minimum {min_length}). This is likely a snippet, not a full contract.")
        
        if (len(text_lower) > ContractValidator.MAX_CONTRACT_LENGTH):
            return (False, "too_long", f"Text too long ({len(text_lower)} chars, maximum {ContractValidator.MAX_CONTRACT_LENGTH}). This may be a contract bundle or combined document.")
        
        # Anti-pattern Check (Prevent False Positives)
        anti_score          = 0
        found_anti_patterns = list()
        
        for pattern, weight in ContractValidator.ANTI_PATTERNS.items():
            if pattern in text_lower:
                anti_score += weight
                found_anti_patterns.append(pattern)
        
        # More strict anti-pattern check
        if (anti_score >= 10):  # Reduced from 15
            return (False, "not_contract", f"The provided document does not appear to be a legal contract. Please upload a valid contract for analysis.")
        
        # Positive Indicator Scoring
        score            = 0
        found_indicators = list()
        
        for indicator, weight in ContractValidator.STRONG_INDICATORS.items():
            if indicator in text_lower:
                score += weight
                found_indicators.append(indicator)
        
        # Structural Pattern Analysis
        structural_score = ContractValidator._check_structural_patterns(text = text_lower)
        score           += structural_score
        
        # Signature Block Check
        has_signature_block = ContractValidator._has_signature_block(text = text_lower)
        if has_signature_block:
            score += 5
            found_indicators.append("signature block")
        
        # Effective Date Check
        has_effective_date = ContractValidator._has_effective_date(text = text)
        if has_effective_date:
            score += 3
            found_indicators.append("effective date")
        
        # Party Identification Check
        has_parties = ContractValidator._has_party_identification(text = text)
        if has_parties:
            score += 4
            found_indicators.append("party identification")
        
        # Validation Thresholds 
        if (score >= 50): 
            return (True, "high_confidence", f"Strong contract indicators detected (score: {score}). This is highly likely a legal contract.")
        
        elif (score >= 40):  # Reduced from 15 (now accepts lower confidence)
            return (True, "medium_confidence", f"Contract indicators present (score: {score}). This appears to be a contract.")
        
        elif (score >= 25):  
            return (True, "low_confidence", f"Some contract indicators present (score: {score}). Proceeding with analysis.")
        
        else:
            return (False, "not_contract", f"The provided document does not appear to be a legal contract. Please upload a valid contract for analysis.")
    

    @staticmethod
    def _check_structural_patterns(text: str) -> int:
        """
        Check for structural patterns unique to contracts
        """
        score    = 0
        patterns = [(r'in\s+consideration\s+of', 3),
                    (r'now,?\s+therefore', 3),
                    (r'agree\s+as\s+follows', 3),
                    (r'in\s+witness\s+whereof', 4),
                    (r'this\s+agreement.*(?:made|entered)', 3),
                    (r'between.*and.*(?:collectively|hereinafter)', 3),
                    (r'effective\s+as\s+of', 2),
                    (r'signed.*presence\s+of', 2),
                    (r'intending\s+to\s+be\s+legally\s+bound', 4),
                    (r'mutually\s+agree', 2),
                    (r'terms\s+and\s+conditions', 2),
                   ]
        
        for pattern, weight in patterns:
            if re.search(pattern, text, re.IGNORECASE):
                score += weight
        
        return score
    

    @staticmethod
    def _has_signature_block(text: str) -> bool:
        """
        Check for signature block patterns
        """
        signature_patterns = [r'signature:?\s*_+',
                              r'signed:?\s*_+',
                              r'by:?\s*_+',
                              r'name:?\s*_+.*title:?\s*_+',
                              r'\[signature\]',
                              r'\[seal\]',
                              r'authorized\s+signatory',
                              r'in\s+witness\s+whereof.*executed',
                             ]
        
        return any(re.search(p, text, re.IGNORECASE) for p in signature_patterns)
    

    @staticmethod
    def _has_effective_date(text: str) -> bool:
        """
        Check for effective date patterns
        """
        date_patterns = [r'effective\s+(?:date|as\s+of)',
                         r'dated\s+as\s+of',
                         r'this\s+\d+(?:st|nd|rd|th)?\s+day\s+of',
                         r'(?:january|february|march|april|may|june|july|august|september|october|november|december)\s+\d{1,2},?\s+\d{4}',
                         r'commencement\s+date',
                         r'execution\s+date',
                        ]
        
        return any(re.search(p, text, re.IGNORECASE) for p in date_patterns)
    

    @staticmethod
    def _has_party_identification(text: str) -> bool:
        """
        Check if parties are clearly identified
        """
        party_patterns = [r'between.*and.*\(.*".*"\)',
                          r'party\s+[a-z]\s*[:\-]',
                          r'(?:the\s+)?(?:employer|employee|consultant|contractor|client|vendor|landlord|tenant|buyer|seller)',
                          r'hereinafter\s+referred\s+to\s+as',
                          r'\("(?:the\s+)?(?:company|employee|consultant)"\)',
                          r'first\s+party.*second\s+party',
                         ]
        
        return any(re.search(p, text, re.IGNORECASE) for p in party_patterns)
    

    @staticmethod
    def validate_file_integrity(file_path: str) -> Tuple[bool, str]:
        """
        Validate file isn't corrupted and is readable
        """
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                return False, "File does not exist"
            
            file_size = file_path.stat().st_size
            
            if (file_size == 0):
                return False, "File is empty (0 bytes)"
            
            if (file_size < 1024):  
                return (False, f"File suspiciously small ({file_size} bytes)")
            
            with open(file_path, 'rb') as f:
                first_kb = f.read(1024)
                if (b'\x00' * 10 in first_kb):
                    return (False, "File appears corrupted (contains null bytes)")
            
            return (True, "File integrity OK")
            
        except PermissionError:
            return (False, "Permission denied - cannot read file")
        except Exception as e:
            return (False, f"File integrity check failed: {repr(e)}")
    

    @staticmethod
    def get_validation_report(text: str) -> Dict[str, any]:
        """
        Get detailed validation report with scores and findings
        """
        is_valid, validation_type, message = ContractValidator.is_valid_contract(text = text)
        
        text_lower                         = text.lower()
        
        # Calculate individual scores
        indicator_score                    = sum(weight for indicator, weight in ContractValidator.STRONG_INDICATORS.items() if indicator in text_lower)
        anti_score                         = sum(weight for pattern, weight in ContractValidator.ANTI_PATTERNS.items() if pattern in text_lower)
        structural_score                   = ContractValidator._check_structural_patterns(text = text_lower)
        
        # Collect found indicators
        found_indicators                   = [indicator for indicator in ContractValidator.STRONG_INDICATORS.keys() if indicator in text_lower]
        found_anti_patterns                = [pattern for pattern in ContractValidator.ANTI_PATTERNS.keys() if pattern in text_lower]
        
        return {"is_valid"            : is_valid,
                "validation_type"     : validation_type,
                "message"             : message,
                "scores"              : {"total"         : indicator_score + structural_score,
                                         "indicators"    : indicator_score,
                                         "structural"    : structural_score,
                                         "anti_patterns" : anti_score,
                                        },
                "features"            : {"has_signature_block"      : ContractValidator._has_signature_block(text = text_lower),
                                         "has_effective_date"       : ContractValidator._has_effective_date(text = text),
                                         "has_party_identification" : ContractValidator._has_party_identification(text = text),
                                        },
                "found_indicators"    : found_indicators,
                "found_anti_patterns" : found_anti_patterns,
                "text_statistics"     : {"length"     : len(text),
                                         "word_count" : len(text.split()),
                                         "line_count" : len(text.split('\n')),
                                        }
               }

               
################################
# model_manager/model_cache.py
################################
# DEPENDENCIES
import os
import sys
import pickle
import hashlib
from typing import Any
from pathlib import Path
from typing import Callable
from typing import Optional
from functools import wraps
from datetime import datetime
from datetime import timedelta

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from utils.logger import ContractAnalyzerLogger



class ModelCache:
    """
    Smart caching for model outputs and embeddings : uses disk-based caching with TTL
    """
    def __init__(self, cache_dir: Path, ttl_seconds: int = 3600):
        self.cache_dir   = Path(cache_dir)
        self.cache_dir.mkdir(parents = True, exist_ok = True)
        
        self.ttl_seconds = ttl_seconds
        self.logger      = ContractAnalyzerLogger.get_logger()
        
        log_info("ModelCache initialized",
                 cache_dir   = str(self.cache_dir),
                 ttl_seconds = ttl_seconds,
                )
    

    def _get_cache_key(self, prefix: str, *args, **kwargs) -> str:
        """
        Generate cache key from function arguments
        """
        # Create a unique key from arguments
        key_data  = f"{prefix}_{args}_{sorted(kwargs.items())}"
        cache_key = hashlib.md5(key_data.encode()).hexdigest()

        return cache_key
    

    def _get_cache_path(self, cache_key: str) -> Path:
        """
        Get file path for cache key
        """
        return self.cache_dir / f"{cache_key}.pkl"
    

    def _is_expired(self, cache_path: Path) -> bool:
        """
        Check if cache file is expired
        """
        if not cache_path.exists():
            return True
        
        file_time  = datetime.fromtimestamp(cache_path.stat().st_mtime)
        age        = datetime.now() - file_time
        is_expired = age > timedelta(seconds = self.ttl_seconds)
        
        return is_expired
    

    def get(self, prefix: str, *args, **kwargs) -> Optional[Any]:
        """
        Get cached value
        """
        cache_key  = self._get_cache_key(prefix, *args, **kwargs)
        cache_path = self._get_cache_path(cache_key)
        
        if self._is_expired(cache_path):
            log_info(f"Cache miss (expired): {prefix}",
                     cache_key = cache_key,
                    )

            return None
        
        try:
            with open(cache_path, 'rb') as f:
                result = pickle.load(f)
            
            log_info(f"Cache hit: {prefix}",
                     cache_key    = cache_key,
                     file_size_kb = cache_path.stat().st_size / 1024,
                    )
            
            return result
            
        except Exception as e:
            log_error(e, context = {"component" : "ModelCache", "operation" : "get", "prefix" : prefix, "cache_key" : cache_key})
            
            return None
    

    def set(self, prefix: str, value: Any, *args, **kwargs):
        """
        Set cached value
        """
        cache_key  = self._get_cache_key(prefix, *args, **kwargs)
        cache_path = self._get_cache_path(cache_key)
        
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(value, f)
            
            file_size = cache_path.stat().st_size
            
            log_info(f"Cache set: {prefix}",
                     cache_key    = cache_key,
                     file_size_kb = file_size / 1024,
                     ttl_seconds  = self.ttl_seconds,
                    )
            
        except Exception as e:
            log_error(e, context = {"component" : "ModelCache", "operation" : "set", "prefix" : prefix, "cache_key" : cache_key})
    

    def clear_expired(self):
        """
        Clear all expired cache files
        """
        expired_count = 0
        freed_bytes   = 0
        
        log_info("Starting cache cleanup (expired files)")
        
        for cache_file in self.cache_dir.glob("*.pkl"):
            if self._is_expired(cache_file):
                file_size      = cache_file.stat().st_size
                cache_file.unlink()

                expired_count += 1
                freed_bytes   += file_size
        
        log_info("Cache cleanup completed",
                 expired_files = expired_count,
                 freed_mb      = freed_bytes / (1024 * 1024),
                )

    
    def clear_all(self):
        """
        Clear all cache files
        """
        file_count  = 0
        freed_bytes = 0
        
        log_info("Clearing all cache files")
        
        for cache_file in self.cache_dir.glob("*.pkl"):
            file_size    = cache_file.stat().st_size
            cache_file.unlink()

            file_count  += 1
            freed_bytes += file_size
        
        log_info("All cache cleared",
                 files_deleted = file_count,
                 freed_mb      = freed_bytes / (1024 * 1024),
                )

    
    def get_size_mb(self) -> float:
        """
        Get total cache size in MB
        """
        total_bytes = sum(f.stat().st_size for f in self.cache_dir.glob("*.pkl"))
        size_mb     = total_bytes / (1024 * 1024)
        
        log_info("Cache size calculated",
                 size_mb    = round(size_mb, 2),
                 file_count = len(list(self.cache_dir.glob("*.pkl"))),
                )
        
        return size_mb
    

    def get_stats(self) -> dict:
        """
        Get cache statistics
        """
        cache_files   = list(self.cache_dir.glob("*.pkl"))
        total_size    = sum(f.stat().st_size for f in cache_files)
        
        expired_files = [f for f in cache_files if self._is_expired(f)]
        
        stats         = {"total_files"   : len(cache_files),
                         "expired_files" : len(expired_files),
                         "total_size_mb" : total_size / (1024 * 1024),
                         "cache_dir"     : str(self.cache_dir),
                         "ttl_seconds"   : self.ttl_seconds,
                        }
        
        log_info("Cache statistics retrieved", **stats)
        
        return stats


def cached(prefix: str, cache: ModelCache):
    """
    Decorator for caching function results
    """
    def decorator(func: Callable):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Try to get from cache
            cached_result = cache.get(prefix, *args, **kwargs)
            
            if cached_result is not None:
                return cached_result
            
            # Compute and cache
            log_info(f"Computing (cache miss): {func.__name__}", prefix=prefix)
            result = func(*args, **kwargs)

            cache.set(prefix, result, *args, **kwargs)
            
            return result
        
        return wrapper

    return decorator

##################################
# model_manager/model_registry.py
##################################
# DEPENDENCIES
import sys
import threading
from enum import Enum
from typing import Any
from typing import Dict
from pathlib import Path
from typing import Optional
from dataclasses import field
from datetime import datetime
from dataclasses import dataclass

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from utils.logger import ContractAnalyzerLogger 


class ModelType(Enum):
    """
    Enum for model types
    """
    LEGAL_BERT = "legal-bert"
    EMBEDDING  = "embedding"
    TOKENIZER  = "tokenizer"
    CLASSIFIER = "classifier"


class ModelStatus(Enum):
    """
    Model loading status
    """
    NOT_LOADED = "not_loaded"
    LOADING    = "loading"
    LOADED     = "loaded"
    ERROR      = "error"


@dataclass
class ModelInfo:
    """
    Model metadata and state
    """
    name           : str
    type           : ModelType
    status         : ModelStatus        = ModelStatus.NOT_LOADED
    model          : Optional[Any]      = None
    tokenizer      : Optional[Any]      = None
    loaded_at      : Optional[datetime] = None
    error_message  : Optional[str]      = None
    memory_size_mb : float              = 0.0
    access_count   : int                = 0
    last_accessed  : Optional[datetime] = None
    metadata       : Dict[str, Any]     = field(default_factory = dict)
    

    def mark_accessed(self):
        """
        Update access statistics
        """
        self.access_count += 1
        self.last_accessed = datetime.now()
    

    def get_age_seconds(self) -> float:
        """
        Get seconds since last access
        """
        if self.last_accessed:
            return (datetime.now() - self.last_accessed).total_seconds()
        
        return float('inf')


class ModelRegistry:
    """
    Thread-safe singleton model registry : manages all loaded models with LRU eviction
    """
    _instance = None
    _lock     = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance              = super().__new__(cls)
                    cls._instance._initialized = False
        
        return cls._instance
    

    def __init__(self):
        if self._initialized:
            return
        
        self._registry : Dict[ModelType, ModelInfo] = dict()
        self._model_lock                            = threading.Lock()
        self._max_models                            = 3  # LRU cache size
        self._initialized                           = True
        self.logger                                 = ContractAnalyzerLogger.get_logger()
        
        log_info("ModelRegistry initialized", max_models = self._max_models)
    

    def register(self, model_type: ModelType, model_info: ModelInfo):
        """
        Register a model (thread-safe)
        """
        with self._model_lock:
            self._registry[model_type] = model_info
            self._enforce_cache_limit()
            
            log_info(f"Model registered: {model_type.value}",
                     model_name = model_info.name,
                     status     = model_info.status.value,
                     memory_mb  = model_info.memory_size_mb,
                    )
    

    def get(self, model_type: ModelType) -> Optional[ModelInfo]:
        """
        Get model info (thread-safe)
        """
        with self._model_lock:
            info = self._registry.get(model_type)
            
            if info:
                info.mark_accessed()
                log_info(f"Model accessed: {model_type.value}",
                         access_count = info.access_count,
                         status       = info.status.value,
                        )

            return info
    

    def is_loaded(self, model_type: ModelType) -> bool:
        """
        Check if model is loaded
        """
        info = self.get(model_type)
        return info is not None and (info.status == ModelStatus.LOADED)
    

    def unload(self, model_type: ModelType):
        """
        Unload a model from memory
        """
        with self._model_lock:
            if model_type in self._registry:
                info = self._registry[model_type]
                
                log_info(f"Unloading model: {model_type.value}",
                         memory_freed_mb = info.memory_size_mb,
                         access_count    = info.access_count,
                        )
                
                # Clear references to allow garbage collection
                info.model     = None
                info.tokenizer = None
                info.status    = ModelStatus.NOT_LOADED

                del self._registry[model_type]
    

    def get_all_loaded(self) -> list[ModelInfo]:
        """
        Get all loaded models
        """
        with self._model_lock:
            loaded = [info for info in self._registry.values() if (info.status == ModelStatus.LOADED)]
            
            if loaded:
                log_info(f"Retrieved {len(loaded)} loaded models", models = [info.name for info in loaded])
            
            return loaded
    

    def get_memory_usage(self) -> float:
        """
        Get total memory usage in MB
        """
        with self._model_lock:
            total = sum(info.memory_size_mb for info in self._registry.values() if (info.status == ModelStatus.LOADED))

            return total
    

    def _enforce_cache_limit(self):
        """
        Enforce LRU cache limit
        """
        loaded_models = [(model_type, info) for model_type, info in self._registry.items() if (info.status == ModelStatus.LOADED)]
        
        if (len(loaded_models) > self._max_models):
            # Sort by last access time (oldest first)
            loaded_models.sort(key = lambda x: x[1].get_age_seconds(), reverse = True)
            
            # Unload oldest models
            for model_type, info in loaded_models[self._max_models:]:
                log_info(f"LRU eviction: {model_type.value}",
                         reason      = "cache_limit_exceeded",
                         age_seconds = info.get_age_seconds(),
                         max_models  = self._max_models,
                        )

                self.unload(model_type)
    

    def clear_all(self):
        """
        Clear all models from registry
        """
        with self._model_lock:
            model_count  = len(self._registry)
            total_memory = self.get_memory_usage()
            
            log_info("Clearing all models from registry",
                     models_cleared  = model_count,
                     memory_freed_mb = total_memory,
                    )
            
            for model_type in list(self._registry.keys()):
                self.unload(model_type)
    

    def get_stats(self) -> Dict[str, Any]:
        """
        Get registry statistics
        """
        with self._model_lock:
            stats = {"total_models"    : len(self._registry),
                     "loaded_models"   : sum(1 for info in self._registry.values() if info.status == ModelStatus.LOADED),
                     "total_memory_mb" : self.get_memory_usage(),
                     "models"          : {model_type.value: {"status"        : info.status.value,
                                                             "access_count"  : info.access_count,
                                                             "memory_mb"     : info.memory_size_mb,
                                                             "last_accessed" : info.last_accessed.isoformat() if info.last_accessed else None,
                                                            }
                                          for model_type, info in self._registry.items()
                                         },
                    }
            
            log_info("Registry stats retrieved", **stats)
            
            return stats


#################################
# model_manager/model_loader.py
#################################
# DEPENDENCIES
import sys
import torch
from pathlib import Path
from transformers import AutoModel
from transformers import AutoTokenizer
from sentence_transformers import SentenceTransformer

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.model_config import ModelConfig
from utils.logger import ContractAnalyzerLogger
from model_manager.model_registry import ModelInfo
from model_manager.model_registry import ModelType
from model_manager.model_registry import ModelStatus
from model_manager.model_registry import ModelRegistry


class ModelLoader:
    """
    Smart model loader with automatic download, caching, and GPU support
    """
    def __init__(self):
        self.registry = ModelRegistry()
        self.config   = ModelConfig()
        self.logger   = ContractAnalyzerLogger.get_logger()
        
        # Detect device
        self.device   = "cuda" if torch.cuda.is_available() else "cpu"

        log_info(f"ModelLoader initialized", device = self.device, gpu_available = torch.cuda.is_available())
        
        # Ensure directories exist
        ModelConfig.ensure_directories()
        log_info("Model directories ensured", 
                 model_dir = str(self.config.MODEL_DIR),
                 cache_dir = str(self.config.CACHE_DIR),
                )

    
    def _check_model_files_exist(self, local_path: Path) -> bool:
        """
        Check if all required model files exist in local path
        """
        if not local_path.exists():
            return False
            
        # Check for essential files that indicate a complete model
        essential_files = ["config.json",
                           "pytorch_model.bin",
                           "model.safetensors", 
                           "vocab.txt",
                           "tokenizer_config.json"
                          ]
        
        # At least config.json and one model file should exist
        has_config     = (local_path / "config.json").exists()
        has_model_file = any((local_path / file).exists() for file in ["pytorch_model.bin", "model.safetensors"])
        
        return has_config and has_model_file

    
    def load_legal_bert(self) -> tuple:
        """
        Load Legal-BERT model and tokenizer (nlpaueb/legal-bert-base-uncased)
        """
        # Check if already loaded
        if self.registry.is_loaded(ModelType.LEGAL_BERT):
            info = self.registry.get(ModelType.LEGAL_BERT)

            log_info("Legal-BERT already loaded from cache",
                     memory_mb    = info.memory_size_mb,
                     access_count = info.access_count,
                    )

            return info.model, info.tokenizer
        
        # Mark as loading
        self.registry.register(ModelType.LEGAL_BERT, 
                               ModelInfo(name   = "legal-bert", 
                                         type   = ModelType.LEGAL_BERT, 
                                         status = ModelStatus.LOADING,
                                        )
                              )
        
        try:
            config = self.config.LEGAL_BERT
            local_path     = config["local_path"]
            force_download = config.get("force_download", False)
            
            # Check if we should use local cache
            if self._check_model_files_exist(local_path) and not force_download:
                log_info(f"Loading Legal-BERT from local cache", path=str(local_path))
                
                model     = AutoModel.from_pretrained(pretrained_model_name_or_path = str(local_path))
                tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = str(local_path))

            else:
                log_info(f"Downloading Legal-BERT from HuggingFace", model_name = config["model_name"])
                
                model     = AutoModel.from_pretrained(pretrained_model_name_or_path = config["model_name"])
                tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = config["model_name"])
                
                # Save to local cache
                log_info(f"Saving Legal-BERT to local cache", path = str(local_path))
                local_path.mkdir(parents = True, exist_ok = True)

                model.save_pretrained(save_directory = str(local_path))
                tokenizer.save_pretrained(save_directory = str(local_path))
            
            # Move to device
            model.to(self.device)
            model.eval()
            
            # Calculate memory size
            memory_mb = sum(p.nelement() * p.element_size() for p in model.parameters()) / (1024 * 1024)
            
            # Register as loaded
            self.registry.register(ModelType.LEGAL_BERT,
                                   ModelInfo(name           = "legal-bert",
                                             type           = ModelType.LEGAL_BERT,
                                             status         = ModelStatus.LOADED,
                                             model          = model,
                                             tokenizer      = tokenizer,
                                             memory_size_mb = memory_mb,
                                             metadata       = {"device" : self.device, "model_name" : config["model_name"]}
                                            )
                                  )
            
            log_info("Legal-BERT loaded successfully",
                     memory_mb  = round(memory_mb, 2),
                     device     = self.device,
                     parameters = sum(p.numel() for p in model.parameters()),
                    )
            
            return model, tokenizer
            
        except Exception as e:
            log_error(e, context = {"component": "ModelLoader", "operation": "load_legal_bert", "model_name": self.config.LEGAL_BERT["model_name"]})
            
            self.registry.register(ModelType.LEGAL_BERT,
                                   ModelInfo(name          = "legal-bert",
                                             type          = ModelType.LEGAL_BERT,
                                             status        = ModelStatus.ERROR,
                                             error_message = str(e),
                                            )
                                  )
            raise


    def load_classifier_model(self) -> tuple:
        """
        Load contract classification model using Legal-BERT with classification head
        """
        # Check if already loaded
        if self.registry.is_loaded(ModelType.CLASSIFIER):
            info = self.registry.get(ModelType.CLASSIFIER)

            log_info("Classifier model already loaded from cache",
                     memory_mb    = info.memory_size_mb,
                     access_count = info.access_count,
                    )

            return info.model, info.tokenizer
        
        # Mark as loading
        self.registry.register(ModelType.CLASSIFIER,
                               ModelInfo(name   = "classifier", 
                                         type   = ModelType.CLASSIFIER, 
                                         status = ModelStatus.LOADING,
                                        )
                              )
        
        try:
            config = self.config.CLASSIFIER_MODEL
            
            log_info("Loading classifier model (Legal-BERT based)", 
                     embedding_dim  = config["embedding_dim"],
                     hidden_dim     = config["hidden_dim"],
                     num_categories = config["num_categories"],
                    )
            
            # Use the Legal-BERT model but prepare it for classification
            base_model, tokenizer = self.load_legal_bert()
            
            # Register as loaded (sharing the same Legal-BERT instance)
            self.registry.register(ModelType.CLASSIFIER,
                                   ModelInfo(name           = "classifier",
                                             type           = ModelType.CLASSIFIER,
                                             status         = ModelStatus.LOADED,
                                             model          = base_model,  
                                             tokenizer      = tokenizer,   
                                             memory_size_mb = 0.0,  
                                             metadata       = {"device"        : self.device, 
                                                               "base_model"    : "legal-bert",
                                                               "embedding_dim" : config["embedding_dim"],
                                                               "num_classes"   : config["num_categories"],
                                                               "purpose"       : "contract_type_classification",
                                                              }
                                            )
                                  )
            
            log_info("Classifier model loaded successfully",
                     base_model     = "legal-bert",
                     num_categories = config["num_categories"],
                     note           = "Using Legal-BERT for both clause extraction and classification",
                    )
            
            return base_model, tokenizer
            
        except Exception as e:
            log_error(e, context = {"component": "ModelLoader", "operation": "load_classifier_model"})
            
            self.registry.register(ModelType.CLASSIFIER,
                                   ModelInfo(name          = "classifier",
                                             type          = ModelType.CLASSIFIER,
                                             status        = ModelStatus.ERROR,
                                             error_message = str(e),
                                            )
                                  )
            raise

    
    def load_embedding_model(self) -> SentenceTransformer:
        """
        Load sentence transformer for embeddings
        """
        # Check if already loaded
        if self.registry.is_loaded(ModelType.EMBEDDING):
            info = self.registry.get(ModelType.EMBEDDING)

            log_info("Embedding model already loaded from cache",
                     memory_mb    = info.memory_size_mb,
                     access_count = info.access_count,
                    )
            return info.model
        
        # Mark as loading
        self.registry.register(ModelType.EMBEDDING,
                               ModelInfo(name   = "embedding", 
                                         type   = ModelType.EMBEDDING, 
                                         status = ModelStatus.LOADING,
                                        )
                              )
        
        try:
            config         = self.config.EMBEDDING_MODEL
            local_path     = config["local_path"]
            force_download = config.get("force_download", False)
            
            # Check if we should use local cache
            if local_path.exists() and not force_download:
                log_info("Loading embedding model from local cache", path = str(local_path))

                model = SentenceTransformer(model_name_or_path = str(local_path))

            else:
                log_info("Downloading embedding model from HuggingFace", model_name = config["model_name"])
                
                model = SentenceTransformer(model_name_or_path = config["model_name"]) 
                
                # Save to local cache
                log_info("Saving embedding model to local cache", path = str(local_path))
                local_path.mkdir(parents = True, exist_ok = True)
                model.save(str(local_path))
            
            # Move to device
            if self.device == "cuda":
                model = model.to(self.device)
            
            # Estimate memory size
            memory_mb = 100  
            
            # Register as loaded
            self.registry.register(ModelType.EMBEDDING,
                                   ModelInfo(name           = "embedding",
                                             type           = ModelType.EMBEDDING,
                                             status         = ModelStatus.LOADED,
                                             model          = model,
                                             memory_size_mb = memory_mb,
                                             metadata       = {"device": self.device, "model_name": config["model_name"], "dimension": config["dimension"]}
                                            )
                                  )
            
            log_info("Embedding model loaded successfully",
                     memory_mb = memory_mb,
                     device    = self.device,
                     dimension = config["dimension"],
                    )
            
            return model
            
        except Exception as e:
            log_error(e, context = {"component": "ModelLoader", "operation": "load_embedding_model", "model_name": self.config.EMBEDDING_MODEL["model_name"]})
            
            self.registry.register(ModelType.EMBEDDING,
                                   ModelInfo(name          = "embedding",
                                             type          = ModelType.EMBEDDING,
                                             status        = ModelStatus.ERROR,
                                             error_message = str(e),
                                            )
                                  )
            raise

    
    def ensure_models_downloaded(self):
        """
        Ensure all required models are downloaded before use
        """
        log_info("Ensuring all models are downloaded...")
        
        try:
            # Download Legal-BERT if needed
            if not self.registry.is_loaded(ModelType.LEGAL_BERT):
                config     = self.config.LEGAL_BERT
                local_path = config["local_path"]
                
                if not self._check_model_files_exist(local_path):
                    log_info("Pre-downloading Legal-BERT...")

                    model     = AutoModel.from_pretrained(pretrained_model_name_or_path = config["model_name"])
                    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = config["model_name"])
                    
                    local_path.mkdir(parents = True, exist_ok = True)
                    model.save_pretrained(save_directory = str(local_path))
                    tokenizer.save_pretrained(save_directory = str(local_path))

                    log_info("Legal-BERT pre-downloaded successfully")
            
            # Download embedding model if needed
            if not self.registry.is_loaded(ModelType.EMBEDDING):
                config     = self.config.EMBEDDING_MODEL
                local_path = config["local_path"]
                
                if not local_path.exists():
                    log_info("Pre-downloading embedding model...")
                    model = SentenceTransformer(model_name_or_path = config["model_name"])

                    local_path.mkdir(parents = True, exist_ok = True)

                    model.save(str(local_path))
                    log_info("Embedding model pre-downloaded successfully")
            
            # Note: Classifier model is a stub, no download needed
            log_info("Classifier model stub - no download required (uses Legal-BERT)")
                    
            log_info("All models are ready for use")
            
        except Exception as e:
            log_error(e, context={"component": "ModelLoader", "operation": "ensure_models_downloaded"})
            raise

    
    def get_registry_stats(self) -> dict:
        """
        Get statistics about loaded models
        """
        stats = self.registry.get_stats()
        log_info("Retrieved registry statistics",
                 total_models    = stats["total_models"],
                 loaded_models   = stats["loaded_models"],
                 total_memory_mb = stats["total_memory_mb"],
                )

        return stats
    

    def clear_cache(self):
        """
        Clear all models from memory
        """
        log_info("Clearing all models from cache")
        self.registry.clear_all()
        log_info("All models cleared from cache")


##################################
# model_manager/llm_manager.py
##################################
# DEPENDENCIES
import sys
import json
import time
import requests
from enum import Enum
from typing import Any
from typing import Dict
from typing import List
from pathlib import Path
from typing import Literal
from typing import Optional
from dataclasses import dataclass
from config.settings import settings

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.model_config import ModelConfig
from utils.logger import ContractAnalyzerLogger


# Optional imports for API providers
try:
    import openai
    OPENAI_AVAILABLE = True

except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True

except ImportError:
    ANTHROPIC_AVAILABLE = False


class LLMProvider(Enum):
    """
    Supported LLM providers
    """
    OLLAMA    = "ollama"
    OPENAI    = "openai"
    ANTHROPIC = "anthropic"


@dataclass
class LLMResponse:
    """
    Standardized LLM response
    """
    text            : str
    provider        : str
    model           : str
    tokens_used     : int
    latency_seconds : float
    success         : bool
    error_message   : Optional[str]            = None
    raw_response    : Optional[Dict[str, Any]] = None
    

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary
        """
        return {"text"            : self.text,
                "provider"        : self.provider,
                "model"           : self.model,
                "tokens_used"     : self.tokens_used,
                "latency_seconds" : round(self.latency_seconds, 3),
                "success"         : self.success,
                "error_message"   : self.error_message,
               }


class LLMManager:
    """
    Unified LLM manager for multiple providers : handles Ollama (local), OpenAI API, and Anthropic API
    """
    def __init__(self, default_provider: LLMProvider = LLMProvider.OLLAMA, ollama_base_url: Optional[str] = None,
                 openai_api_key: Optional[str] = None, anthropic_api_key: Optional[str] = None):
        """
        Initialize LLM Manager
        
        Arguments:
        ----------
            default_provider  : Default LLM provider to use
            
            ollama_base_url   : Ollama server URL (default: from settings)
            
            openai_api_key    : OpenAI API key (or set OPENAI_API_KEY env var)
            
            anthropic_api_key : Anthropic API key (or set ANTHROPIC_API_KEY env var)
        """
        self.default_provider   = default_provider
        self.logger             = ContractAnalyzerLogger.get_logger()
        
        # Configuration Variables Initialization
        self.config             = ModelConfig()
        
        # Ollama configuration 
        self.ollama_base_url    = ollama_base_url or settings.OLLAMA_BASE_URL
        self.ollama_model       = settings.OLLAMA_MODEL
        self.ollama_timeout     = settings.OLLAMA_TIMEOUT
        self.ollama_temperature = settings.OLLAMA_TEMPERATURE
        
        # OpenAI configuration 
        self.openai_api_key     = openai_api_key or settings.OPENAI_API_KEY
        
        if (OPENAI_AVAILABLE and self.openai_api_key):
            openai.api_key = self.openai_api_key
        
        # Anthropic configuration  
        self.anthropic_api_key = anthropic_api_key or settings.ANTHROPIC_API_KEY

        if (ANTHROPIC_AVAILABLE and self.anthropic_api_key):
            self.anthropic_client = anthropic.Anthropic(api_key = self.anthropic_api_key)
        
        else:
            self.anthropic_client = None
        
        # Rate limiting (simple token bucket)
        self._rate_limit_tokens      = settings.RATE_LIMIT_REQUESTS
        self._rate_limit_last_refill = time.time()
        self._rate_limit_refill_rate = settings.RATE_LIMIT_REQUESTS / settings.RATE_LIMIT_PERIOD
        
        # Generation settings 
        self.generation_config = self.config.LLM_GENERATION
        
        log_info("LLMManager initialized",
                 default_provider    = default_provider.value,
                 ollama_base_url     = self.ollama_base_url,
                 ollama_model        = self.ollama_model,
                 ollama_timeout      = self.ollama_timeout,
                 ollama_temperature  = self.ollama_temperature,
                 openai_available    = OPENAI_AVAILABLE and bool(self.openai_api_key),
                 anthropic_available = ANTHROPIC_AVAILABLE and bool(self.anthropic_api_key),
                 rate_limit_requests = settings.RATE_LIMIT_REQUESTS,
                 rate_limit_period   = settings.RATE_LIMIT_PERIOD,
                )


    # Provider Availability Check
    def _check_ollama_available(self) -> bool:
        """
        Check if Ollama server is available
        """
        try:
            response  = requests.get(f"{self.ollama_base_url}/api/tags", timeout = 30)
            available = (response.status_code == 200)

            if available:
                log_info("Ollama server is available", base_url = self.ollama_base_url)

            return available

        except Exception as e:
            log_error(e, context = {"component" : "LLMManager", "operation" : "check_ollama"})

            return False

    
    def get_available_providers(self) -> List[LLMProvider]:
        """
        Get list of available providers
        """
        available = list()
        
        if self._check_ollama_available():
            available.append(LLMProvider.OLLAMA)
        
        if OPENAI_AVAILABLE and self.openai_api_key:
            available.append(LLMProvider.OPENAI)
        
        if ANTHROPIC_AVAILABLE and self.anthropic_api_key:
            available.append(LLMProvider.ANTHROPIC)
        
        log_info("Available LLM providers", providers = [p.value for p in available])
        
        return available
    

    # Rate Limiting
    def _check_rate_limit(self) -> bool:
        """
        Check if rate limit allows request (simple token bucket)
        """
        now         = time.time()
        time_passed = now - self._rate_limit_last_refill
        
        # Refill tokens
        self._rate_limit_tokens      = min(settings.RATE_LIMIT_REQUESTS, self._rate_limit_tokens + time_passed * self._rate_limit_refill_rate)
        self._rate_limit_last_refill = now
        
        if (self._rate_limit_tokens >= 1):
            self._rate_limit_tokens -= 1

            return True
        
        log_info("Rate limit hit, waiting...", tokens_remaining = self._rate_limit_tokens)
        
        return False
    

    def _wait_for_rate_limit(self):
        """
        Wait until rate limit allows request
        """
        while not self._check_rate_limit():
            time.sleep(0.5)
    

    # UNIFIED COMPLETION METHOD
    @ContractAnalyzerLogger.log_execution_time("llm_complete")
    def complete(self, prompt: str, provider: Optional[LLMProvider] = None, model: Optional[str] = None, temperature: Optional[float] = None, 
                 max_tokens: Optional[int] = None, system_prompt: Optional[str] = None, json_mode: bool = False, retry_on_error: bool = True, 
                 fallback_providers: Optional[List[LLMProvider]] = None) -> LLMResponse:
        """
        Unified completion method for all providers
        
        Arguments:
        ----------
            prompt             : User prompt
            
            provider           : LLM provider (default: self.default_provider)
            
            model              : Model name (provider-specific)
            
            temperature        : Sampling temperature (0.0-1.0, default from settings/config)
            
            max_tokens         : Maximum tokens to generate (default from config)
            
            system_prompt      : System prompt (if supported)
            
            json_mode          : Force JSON output (if supported)
            
            retry_on_error     : Retry with fallback providers on error
            
            fallback_providers : List of fallback providers to try
        
        Returns:
        --------
            { LLMResponse }    : LLMResponse object
        """
        provider    = provider or self.default_provider
        temperature = temperature or self.ollama_temperature
        max_tokens  = max_tokens or self.generation_config["max_tokens"]
        
        log_info("LLM completion request",
                 provider      = provider.value,
                 prompt_length = len(prompt),
                 temperature   = temperature,
                 max_tokens    = max_tokens,
                 json_mode     = json_mode,
                )
        
        # Rate limiting
        self._wait_for_rate_limit()
        
        # Try primary provider
        try:
            if (provider == LLMProvider.OLLAMA):
                return self._complete_ollama(prompt        = prompt,
                                             model         = model, 
                                             temperature   = temperature,
                                             max_tokens    = max_tokens, 
                                             system_prompt = system_prompt, 
                                             json_mode     = json_mode,
                                            )

            elif (provider == LLMProvider.OPENAI):
                return self._complete_openai(prompt        = prompt, 
                                             model         = model, 
                                             temperature   = temperature, 
                                             max_tokens    = max_tokens, 
                                             system_prompt = system_prompt, 
                                             json_mode     = json_mode,
                                            )

            elif (provider == LLMProvider.ANTHROPIC):
                return self._complete_anthropic(prompt        = prompt, 
                                                model         = model, 
                                                temperature   = temperature, 
                                                max_tokens    = max_tokens, 
                                                system_prompt = system_prompt,
                                               )

            else:
                raise ValueError(f"Unsupported provider: {provider}")
        
        except Exception as e:
            log_error(e, context = {"component" : "LLMManager", "operation" : "complete", "provider" : provider.value})
            
            # Try fallback providers
            if (retry_on_error and fallback_providers):
                log_info("Trying fallback providers", fallbacks = [p.value for p in fallback_providers])
                
                for fallback_provider in fallback_providers:
                    if (fallback_provider == provider):
                        continue
                    
                    try:
                        log_info(f"Attempting fallback to {fallback_provider.value}")
                        # Prevent infinite recursion
                        return self.complete(prompt         = prompt,
                                             provider       = fallback_provider,
                                             model          = model,
                                             temperature    = temperature,
                                             max_tokens     = max_tokens,
                                             system_prompt  = system_prompt,
                                             json_mode      = json_mode,
                                             retry_on_error = False,  
                                            )

                    except Exception as fallback_error:
                        log_error(fallback_error, context = {"component" : "LLMManager", "operation" : "fallback_complete", "provider" : fallback_provider.value})
                        continue
            
            # All attempts failed
            return LLMResponse(text            = "",
                               provider        = provider.value,
                               model           = model or "unknown",
                               tokens_used     = 0,
                               latency_seconds = 0.0,
                               success         = False,
                               error_message   = str(e),
                              )


    # OLLAMA Provider
    def _complete_ollama(self, prompt: str, model: Optional[str], temperature: float, max_tokens: int, system_prompt: Optional[str], json_mode: bool) -> LLMResponse:
        """
        Complete using local Ollama
        """
        start_time  = time.time()
        model       = model or self.ollama_model
        
        # Construct full prompt with system prompt
        full_prompt = prompt
        
        if system_prompt:
            full_prompt = f"<|system|>\n{system_prompt}\n<|user|>\n{prompt}\n<|assistant|>"
        
        payload = {"model"   : model,
                   "prompt"  : full_prompt,
                   "stream"  : False,
                   "options" : {"temperature": temperature, "num_predict": max_tokens},
                  }
        
        if json_mode:
            payload["format"] = "json"
        
        log_info("Calling Ollama API",
                 model     = model,
                 base_url  = self.ollama_base_url,
                 json_mode = json_mode,
                )
        
        response       = requests.post(f"{self.ollama_base_url}/api/generate", json = payload, timeout = self.ollama_timeout)
        response.raise_for_status()
        
        result         = response.json()
        generated_text = result.get('response', '')
        
        latency        = time.time() - start_time
        
        # Estimate tokens (rough approximation)
        tokens_used    = len(prompt.split()) + len(generated_text.split())
        
        log_info("Ollama completion successful",
                 model           = model,
                 tokens_used     = tokens_used,
                 latency_seconds = round(latency, 3),
                )
        
        return LLMResponse(text            = generated_text,
                           provider        = "ollama",
                           model           = model,
                           tokens_used     = tokens_used,
                           latency_seconds = latency,
                           success         = True,
                           raw_response    = result,
                          )
    

    # Open-AI Provider
    def _complete_openai(self, prompt: str, model: Optional[str], temperature: float, max_tokens: int, system_prompt: Optional[str], json_mode: bool) -> LLMResponse:
        """
        Complete using OpenAI API
        """
        if not OPENAI_AVAILABLE or not self.openai_api_key:
            raise ValueError("OpenAI not available. Install with: pip install openai")
        
        start_time = time.time()
        model      = model or "gpt-3.5-turbo"
        
        # Construct messages
        messages   = list()

        if system_prompt:
            messages.append({"role"    : "system", 
                             "content" : system_prompt,
                           })

        messages.append({"role"    : "user",
                         "content" : prompt,
                       })

        
        log_info("Calling OpenAI API", model = model, json_mode = json_mode)
        
        # API call parameters
        api_params = {"model"       : model,
                      "messages"    : messages,
                      "temperature" : temperature,
                      "max_tokens"  : max_tokens,
                     }
        
        if json_mode:
            api_params["response_format"] = {"type": "json_object"}
        
        response       = openai.ChatCompletion.create(**api_params)
        generated_text = response.choices[0].message.content
        tokens_used    = response.usage.total_tokens
        latency        = time.time() - start_time
        
        log_info("OpenAI completion successful", model = model, tokens_used = tokens_used, latency_seconds = round(latency, 3))
        
        return LLMResponse(text            = generated_text,
                           provider        = "openai",
                           model           = model,
                           tokens_used     = tokens_used,
                           latency_seconds = latency,
                           success         = True,
                           raw_response    = response.to_dict(),
                          )
    
    # Anthropic Provider
    def _complete_anthropic(self, prompt: str, model: Optional[str], temperature: float, max_tokens: int, system_prompt: Optional[str]) -> LLMResponse:
        """
        Complete using Anthropic (Claude) API
        """
        if not ANTHROPIC_AVAILABLE or not self.anthropic_client:
            raise ValueError("Anthropic not available. Install with: pip install anthropic")
        
        start_time = time.time()
        model      = model or "claude-3-sonnet-20240229"
        
        log_info("Calling Anthropic API", model = model)
        
        # API call
        message        = self.anthropic_client.messages.create(model       = model,
                                                               max_tokens  = max_tokens,
                                                               temperature = temperature,
                                                               system      = system_prompt or "",
                                                               messages    = [{"role": "user", "content": prompt}],
                                                              )
        
        generated_text = message.content[0].text
        tokens_used    = message.usage.input_tokens + message.usage.output_tokens
        latency        = time.time() - start_time
        
        log_info("Anthropic completion successful", model = model, tokens_used = tokens_used, latency_seconds = round(latency, 3))
        
        return LLMResponse(text            = generated_text,
                           provider        = "anthropic",
                           model           = model,
                           tokens_used     = tokens_used,
                           latency_seconds = latency,
                           success         = True,
                           raw_response    = message.dict(),
                          )
    

    # Specialized Methods 
    def generate_structured_json(self, prompt: str, schema_description: str, provider: Optional[LLMProvider] = None, **kwargs) -> Dict[str, Any]:
        """
        Generate structured JSON output
        
        Arguments:
        ----------
            prompt             : User prompt

            schema_description : Description of expected JSON schema
            
            provider           : LLM provider
            
            **kwargs           : Additional arguments for complete()
        
        Returns:
        --------
               { dict }        : Parsed JSON dictionary
        """
        system_prompt = (f"You are a helpful assistant that returns valid JSON.\n"
                         f"Expected schema:\n{schema_description}\n\n"
                         f"Return ONLY valid JSON, no markdown, no explanation."
                        )
        
        response      = self.complete(prompt        = prompt,
                                      provider      = provider,
                                      system_prompt = system_prompt,
                                      json_mode     = True,
                                      **kwargs,
                                     )
        
        if not response.success:
            raise ValueError(f"LLM completion failed: {response.error_message}")
        
        # Parse JSON
        try:
            # Clean response (remove markdown code blocks if present)
            text   = response.text.strip()
            text   = text.replace("```json", "").replace("```", "").strip()
            
            parsed = json.loads(text)

            log_info("JSON parsing successful", keys = list(parsed.keys()))
            
            return parsed
            
        except json.JSONDecodeError as e:
            log_error(e, context = {"component" : "LLMManager", "operation" : "parse_json", "response_text" : response.text})
            raise ValueError(f"Failed to parse JSON response: {e}")
    

    def batch_complete(self, prompts: List[str], provider: Optional[LLMProvider] = None, **kwargs) -> List[LLMResponse]:
        """
        Complete multiple prompts (sequential for now)
        
        Arguments:
        ----------
            prompts   : List of prompts
            
            provider  : LLM provider
            
            **kwargs  : Additional arguments for complete()
        
        Returns:
        --------
            { list }  : List of LLMResponse objects
        """
        log_info("Batch completion started", batch_size=len(prompts))
        
        responses = list()

        for i, prompt in enumerate(prompts):
            log_info(f"Processing prompt {i+1}/{len(prompts)}")

            response = self.complete(prompt   = prompt, 
                                     provider = provider, 
                                     **kwargs,
                                    )

            responses.append(response)
        
        successful = sum(1 for r in responses if r.success)
        
        log_info("Batch completion finished",
                 total       = len(prompts),
                 successful  = successful,
                 failed      = len(prompts) - successful,
                )
        
        return responses
    

    # OLLAMA-Specific Methods
    def list_ollama_models(self) -> List[str]:
        """
        List available local Ollama models
        """
        try:
            response = requests.get(f"{self.ollama_base_url}/api/tags", timeout = 30)
            response.raise_for_status()
            
            models   = [model['name'] for model in response.json().get('models', [])]

            log_info("Ollama models listed", count = len(models), models = models)

            return models
            
        except Exception as e:
            log_error(e, context = {"component" : "LLMManager", "operation" : "list_ollama_models"})
            return []

    
    def pull_ollama_model(self, model_name: str) -> bool:
        """
        Pull/download an Ollama model
        """
        try:
            log_info(f"Pulling Ollama model: {model_name}")
            
            response = requests.post(f"{self.ollama_base_url}/api/pull",
                                     json    = {"name": model_name},
                                     stream  = True,
                                     timeout = 600,  # 10 minutes for download
                                    )

            response.raise_for_status()
            
            # Stream response to track progress
            for line in response.iter_lines():
                if line:
                    data = json.loads(line)
                    
                    if ('status' in data):
                        log_info(f"Pull status: {data['status']}")
            
            log_info(f"Model pulled successfully: {model_name}")
            return True
            
        except Exception as e:
            log_error(e, context = {"component" : "LLMManager", "operation" : "pull_ollama_model", "model" : model_name})
            return False
    

    # Utility Methods
    def get_provider_info(self, provider: LLMProvider) -> Dict[str, Any]:
        """
        Get information about a provider
        """
        info = {"provider"  : provider.value,
                "available" : False,
                "models"    : [],
               }
        
        if (provider == LLMProvider.OLLAMA):
            info["available"] = self._check_ollama_available()

            if info["available"]:
                info["models"]   = self.list_ollama_models()
                info["base_url"] = self.ollama_base_url
        
        elif (provider == LLMProvider.OPENAI):
            info["available"] = OPENAI_AVAILABLE and bool(self.openai_api_key)

            if info["available"]:
                info["models"] = ["gpt-3.5-turbo", 
                                  "gpt-4", 
                                  "gpt-4-turbo-preview",
                                 ]
        
        elif (provider == LLMProvider.ANTHROPIC):
            info["available"] = ANTHROPIC_AVAILABLE and bool(self.anthropic_client)

            if info["available"]:
                info["models"] = ["claude-3-opus-20240229",
                                  "claude-3-sonnet-20240229",
                                  "claude-3-haiku-20240307",
                                 ]
        
        return info
    

    def estimate_cost(self, prompt_tokens: int, completion_tokens: int, provider: LLMProvider, model: str) -> float:
        """
        Estimate API cost in USD
        
        Arguments:
        ----------
            prompt_tokens     : Number of prompt tokens
            
            completion_tokens : Number of completion tokens
            
            provider          : LLM provider
            
            model             : Model name
        
        Returns:
        --------
                { float }     : Estimated cost in USD
        """
        # Pricing per 1K tokens (as of 2025)
        pricing = {"openai"    : {"gpt-3.5-turbo"       : {"prompt": 0.0015, "completion": 0.002},
                                  "gpt-4"               : {"prompt": 0.03, "completion": 0.06},
                                  "gpt-4-turbo-preview" : {"prompt": 0.01, "completion": 0.03},
                                 },
                   "anthropic" : {"claude-3-opus-20240229"   : {"prompt": 0.015, "completion": 0.075},
                                  "claude-3-sonnet-20240229" : {"prompt": 0.003, "completion": 0.015},
                                  "claude-3-haiku-20240307"  : {"prompt": 0.00025, "completion": 0.00125},
                                 }
                  }
        
        if (provider == LLMProvider.OLLAMA):
            # Local models are free
            return 0.0  
        
        provider_pricing = pricing.get(provider.value, {}).get(model)
        
        if not provider_pricing:
            return 0.0
        
        cost = ((prompt_tokens / 1000) * provider_pricing["prompt"] + (completion_tokens / 1000) * provider_pricing["completion"])
        
        return round(cost, 6)


################################
# services/data_models.py
################################
# DEPENDENCIES
import sys
import numpy as np
from enum import Enum
from typing import Any
from typing import Dict
from typing import List
from typing import Tuple
from pathlib import Path
from typing import Optional
from dataclasses import field
from dataclasses import dataclass

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))




@dataclass
class ExtractedClause:
    """
    Extracted clause with comprehensive metadata
    """
    text              : str
    reference         : str    # e.g., "Section 5.2", "Clause 11.1"
    category          : str    # e.g., "termination", "compensation", "indemnification"
    confidence        : float  # 0.0-1.0
    start_pos         : int
    end_pos           : int
    extraction_method : str    # "structural", "semantic", "hybrid"
    risk_indicators   : List[str]            = field(default_factory = list)
    embeddings        : Optional[np.ndarray] = None
    subclauses        : List[str]            = field(default_factory = list)
    legal_bert_score  : float                = 0.0
    risk_score        : float                = 0.0  
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary for serialization
        """
        return {"text"              : self.text,
                "reference"         : self.reference,
                "category"          : self.category,
                "confidence"        : round(self.confidence, 3),
                "start_pos"         : self.start_pos,
                "end_pos"           : self.end_pos,
                "extraction_method" : self.extraction_method,
                "risk_indicators"   : self.risk_indicators,
                "subclauses"        : self.subclauses,
                "legal_bert_score"  : round(self.legal_bert_score, 3),
                "risk_score"        : round(self.risk_score, 3),
               }


@dataclass
class UnfavorableTerm:
    """
    Detected unfavorable term with comprehensive risk analysis
    """
    term             : str
    category         : str    # Risk category (e.g., "restrictive_covenants")
    severity         : str    # "critical", "high", "medium", "low"
    explanation      : str
    risk_score       : float  # 0-100 risk score
    clause_reference : Optional[str] = None
    suggested_fix    : Optional[str] = None
    contract_type    : Optional[str] = None
    specific_text    : Optional[str] = None
    benchmark_info   : Optional[str] = None  # Industry benchmark comparison
    legal_basis      : Optional[str] = None  # Legal principle violated
    
    def to_dict(self) -> Dict:
        """
        Convert to dictionary
        """
        return {"term"             : self.term,
                "category"         : self.category,
                "severity"         : self.severity,
                "explanation"      : self.explanation,
                "risk_score"       : round(self.risk_score, 2),
                "clause_reference" : self.clause_reference,
                "suggested_fix"    : self.suggested_fix,
                "contract_type"    : self.contract_type,
                "specific_text"    : self.specific_text,
                "benchmark_info"   : self.benchmark_info,
                "legal_basis"      : self.legal_basis,
               }


@dataclass
class ClauseInterpretation:
    """
    LLM interpretation of a clause with comprehensive analysis
    """
    clause_reference       : str  
    original_text          : str  
    plain_english_summary  : str
    key_points             : List[str]
    potential_risks        : List[str]
    suggested_improvements : List[str]
    favorability           : str = "neutral"  
    confidence_score       : float = 0.0
    risk_level             : str = "unknown"
    negotiation_priority   : str = "medium"  
    legal_precedents       : List[str]      = field(default_factory = list)
    negotiation_leverage   : List[str]      = field(default_factory = list)
    market_comparison      : Optional[str]  = None
    risk_score             : float          = 0.0
    
    def to_dict(self) -> Dict[str, Any]:
       
        return {"clause_reference"       : self.clause_reference,
                "original_text"          : self.original_text,
                "plain_english_summary"  : self.plain_english_summary,
                "key_points"             : self.key_points,
                "potential_risks"        : self.potential_risks,
                "suggested_improvements" : self.suggested_improvements,
                "favorability"           : self.favorability,
                "confidence_score"       : round(self.confidence_score, 3),
                "risk_level"             : self.risk_level,
                "negotiation_priority"   : self.negotiation_priority,
                "legal_precedents"       : self.legal_precedents,
                "negotiation_leverage"   : self.negotiation_leverage,
                "market_comparison"      : self.market_comparison,
                "risk_score"             : round(self.risk_score, 3),
               }


@dataclass
class MissingProtection:
    """
    Missing protection item with comprehensive risk analysis
    """
    protection_id      : str    # Internal identifier
    protection         : str
    importance         : str    # "critical", "high", "medium", "low"
    risk_score         : float  # 0-100 from risk_rules
    explanation        : str
    recommendation     : str
    categories         : List[str]
    contract_type      : Optional[str]       = None
    suggested_language : Optional[str]       = None
    legal_basis        : Optional[str]       = None
    affected_clauses   : Optional[List[str]] = None

    def to_dict(self) -> Dict:
        """
        Convert to dictionary
        """
        return {"protection_id"      : self.protection_id,  
                "protection"         : self.protection,
                "importance"         : self.importance,
                "risk_score"         : round(self.risk_score, 2),
                "explanation"        : self.explanation,
                "recommendation"     : self.recommendation,
                "categories"         : self.categories,
                "contract_type"      : self.contract_type,
                "suggested_language" : self.suggested_language,
                "legal_basis"        : self.legal_basis,
                "affected_clauses"   : self.affected_clauses or [],
               }


@dataclass
class ContractCategory:
    """
    Contract classification result with metadata
    """
    category               : str
    subcategory            : Optional[str]
    confidence             : float
    reasoning              : List[str]
    detected_keywords      : List[str]
    alternative_categories : List[Tuple[str, float]] = None  # (category, confidence) pairs
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary for serialization
        """
        return {"category"               : self.category,
                "subcategory"            : self.subcategory,
                "confidence"             : round(self.confidence, 3),
                "reasoning"              : self.reasoning,
                "detected_keywords"      : self.detected_keywords,
                "alternative_categories" : [{"category": cat, "confidence": round(conf, 3)} for cat, conf in (self.alternative_categories or [])]
               }



@dataclass
class RiskBreakdownItem:
    """
    Individual risk category breakdown
    """
    category : str
    score    : int  # 0-100
    summary  : str
    findings : List[str] = field(default_factory = list)
    

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary
        """
        return {"category" : self.category,
                "score"    : self.score,
                "summary"  : self.summary,
                "findings" : self.findings,
               }


@dataclass
class RiskScore:
    """
    Comprehensive risk score with detailed breakdown
    """
    overall_score        : int  # 0-100 
    risk_level           : str  # "CRITICAL", "HIGH", "MEDIUM", "LOW"
    category_scores      : Dict[str, int] 
    risk_factors         : List[str] 
    detailed_findings    : Dict[str, List[str]] 
    benchmark_comparison : Dict[str, str] 
    risk_breakdown       : List[RiskBreakdownItem]
    contract_type        : str
    unfavorable_terms    : List[Dict] 
    missing_protections  : List[Dict] 
    high_risk_clauses    : List[Dict]               = field(default_factory = list) 
    explanation          : str                      = "" 
    recommendations      : List[str]                = field(default_factory = list) 
    analysis_timestamp   : Optional[str]            = None 
    contract_subtype     : Optional[str]            = None 
    contract_metadata    : Optional[Dict[str, Any]] = field(default_factory = dict) 
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary for serialization
        """
        return {"overall_score"        : self.overall_score,
                "risk_level"           : self.risk_level,
                "category_scores"      : self.category_scores,
                "risk_factors"         : self.risk_factors, 
                "detailed_findings"    : self.detailed_findings, 
                "benchmark_comparison" : self.benchmark_comparison, 
                "risk_breakdown"       : [item.to_dict() for item in self.risk_breakdown],
                "contract_type"        : self.contract_type,
                "unfavorable_terms"    : self.unfavorable_terms, 
                "missing_protections"  : self.missing_protections, 
                "high_risk_clauses"    : self.high_risk_clauses,
                "explanation"          : self.explanation,
                "recommendations"      : self.recommendations,
                "analysis_timestamp"   : self.analysis_timestamp,
                "contract_subtype"     : self.contract_subtype,
                "contract_metadata"    : self.contract_metadata,
               }


@dataclass
class RiskInterpretation:
    """
    Comprehensive risk interpretation with LLM-enhanced explanations
    """
    overall_risk_explanation : str
    key_concerns             : List[str]
    negotiation_strategy     : str
    market_comparison        : str
    clause_interpretations   : List[ClauseInterpretation]
    
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary
        """
        return {"overall_risk_explanation" : self.overall_risk_explanation,
                "key_concerns"             : self.key_concerns,
                "negotiation_strategy"     : self.negotiation_strategy,
                "market_comparison"        : self.market_comparison,
                "clause_interpretations"   : [ci.to_dict() for ci in self.clause_interpretations],
               }


class NegotiationTactic(Enum):
    """
    Types of negotiation tactics
    """
    REMOVAL       = "removal"
    MODIFICATION  = "modification" 
    ADDITION      = "addition"
    LIMITATION    = "limitation"
    MUTUALIZATION = "mutualization"
    CLARIFICATION = "clarification"
    

@dataclass
class NegotiationPoint:
    """
    Negotiation talking point with strategic context
    """
    priority              : int  # 1 = highest, 5 = lowest
    category              : str
    issue                 : str
    current_language      : str
    proposed_language     : str
    rationale             : str
    tactic                : NegotiationTactic
    fallback_position     : Optional[str] = None
    estimated_difficulty  : str           = "medium"  # "easy", "medium", "hard"
    legal_basis           : Optional[str] = None
    business_impact       : Optional[str] = None
    counterparty_concerns : Optional[str] = None
    timing_suggestion     : Optional[str] = None
    bargaining_chips      : List[str]     = None

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary
        """
        return {"priority"              : self.priority,
                "category"              : self.category,
                "issue"                 : self.issue,
                "current_language"      : self.current_language,
                "proposed_language"     : self.proposed_language,
                "rationale"             : self.rationale,
                "tactic"                : self.tactic.value,
                "fallback_position"     : self.fallback_position,
                "estimated_difficulty"  : self.estimated_difficulty,
                "legal_basis"           : self.legal_basis,
                "business_impact"       : self.business_impact,
                "counterparty_concerns" : self.counterparty_concerns,
                "timing_suggestion"     : self.timing_suggestion,
                "bargaining_chips"      : self.bargaining_chips or [],
               }


@dataclass
class NegotiationPlaybook:
    """
    Comprehensive negotiation strategy
    """
    overall_strategy     : str
    critical_points      : List[NegotiationPoint]
    walk_away_items      : List[str]
    concession_items     : List[str]
    timing_guidance      : str
    risk_mitigation_plan : str

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert to dictionary
        """
        return {"overall_strategy"     : self.overall_strategy,
                "critical_points"      : [point.to_dict() for point in self.critical_points],
                "walk_away_items"      : self.walk_away_items,
                "concession_items"     : self.concession_items,
                "timing_guidance"      : self.timing_guidance,
                "risk_mitigation_plan" : self.risk_mitigation_plan,
               }


@dataclass
class SummaryContext:
    """
    Context data for comprehensive summary generation
    """
    contract_type         : str
    risk_score            : int
    risk_level            : str
    category_scores       : Dict[str, int]
    unfavorable_terms     : List[Dict]
    missing_protections   : List[Dict]
    clauses               : List
    key_findings          : List[str]
    risk_interpretation   : Optional[RiskInterpretation]  = None
    negotiation_playbook  : Optional[NegotiationPlaybook] = None
    contract_text_preview : Optional[str]                 = None
    contract_metadata     : Optional[Dict[str, Any]]      = None


@dataclass 
class ModelInfo:
    """
    Model metadata and state
    """
    name           : str
    type           : str  # "legal-bert", "embedding", "tokenizer", "classifier"
    status         : str  # "not_loaded", "loading", "loaded", "error"
    model          : Optional[Any]      = None
    tokenizer      : Optional[Any]      = None
    loaded_at      : Optional[str]      = None
    error_message  : Optional[str]      = None
    memory_size_mb : float              = 0.0
    access_count   : int                = 0
    last_accessed  : Optional[str]      = None
    metadata       : Dict[str, Any]     = field(default_factory = dict)
    

    def mark_accessed(self):
        """
        Update access statistics
        """
        self.access_count += 1
        # Simple timestamp 
        self.last_accessed = "now"
    

    def get_age_seconds(self) -> float:
        """
        Get seconds since last access (simplified)
        """
        return 0.0 if not self.last_accessed else 3600.0


#################################
# services/contract_classifier.py
#################################
# DEPENDENCIES
import re
import sys
import torch
import numpy as np
from typing import Any
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
from sentence_transformers import util

# Import utilities
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import ContractType
from config.model_config import ModelConfig
from utils.text_processor import TextProcessor
from utils.logger import ContractAnalyzerLogger
from services.data_models import ContractCategory


class ContractClassifier:
    """
    Contract categorization using:
    1. Legal-BERT embeddings + semantic similarity
    2. Multi-label classification (a contract can be multiple types)
    3. Hierarchical categories (Employment -> Full-Time/Contract/Internship)
    4. Confidence scoring with explanations
    """
    # CATEGORY HIERARCHY WITH KEYWORDS - UPDATED TO MATCH YOUR CATEGORIES
    CATEGORY_HIERARCHY   = {'employment'            : {'subcategories' : ['full_time', 'part_time', 'contract_worker', 'internship', 'executive'],
                                                       'keywords'      : ['employee', 'employment', 'employer', 'job', 'position', 'staff', 'salary', 'wages', 'compensation', 'payroll', 'benefits', 'health insurance', 'retirement', 'pension', '401(k)', 'vacation', 'paid time off', 'sick leave', 'holidays', 'probation', 'performance review', 'promotion', 'termination', 'job description', 'duties', 'responsibilities', 'work hours', 'overtime', 'timekeeping', 'attendance', 'confidentiality', 'non-compete', 'non-solicitation', 'intellectual property', 'inventions', 'work product', 'severance', 'notice period', 'resignation', 'dismissal'],
                                                       'weight'        : 1.1,
                                                      },
                            'consulting'            : {'subcategories' : ['independent_contractor', 'advisory', 'professional_services', 'freelance'],
                                                       'keywords'      : ['consultant', 'consulting', 'independent contractor', 'statement of work', 'deliverables', 'professional services', 'hourly rate', 'project scope', 'milestone', 'acceptance criteria', 'work product', '1099', 'self-employed', 'contractor', 'consulting services', 'expert advice', 'advisory services', 'project basis', 'task order'],
                                                       'weight'        : 1.0,
                                                      },
                            'nda'                   : {'subcategories' : ['mutual_nda', 'unilateral_nda', 'confidentiality_agreement'],
                                                       'keywords'      : ['non-disclosure', 'confidentiality', 'proprietary information', 'nda', 'disclosure agreement', 'trade secret', 'confidential information', 'receiving party', 'disclosing party', 'confidentiality obligation', 'non-use', 'non-circumvention', 'secrecy', 'protected information', 'confidentiality period', 'return of information'],
                                                       'weight'        : 1.0, 
                                                      },
                            'software'              : {'subcategories' : ['software_license', 'saas', 'cloud_services', 'development', 'api_access'],
                                                       'keywords'      : ['software', 'license', 'saas', 'subscription', 'source code', 'object code', 'api', 'cloud', 'hosting', 'maintenance', 'updates', 'support', 'uptime', 'service level', 'software as a service', 'platform', 'application', 'user license', 'perpetual license', 'subscription fee', 'end user license agreement', 'eula'],
                                                       'weight'        : 1.1,
                                                      },
                            'service'               : {'subcategories' : ['master_services', 'maintenance', 'support', 'subscription'],
                                                       'keywords'      : ['service provider', 'services', 'sla', 'service level agreement', 'uptime', 'response time', 'support', 'maintenance', 'service credits', 'performance metrics', 'implementation', 'professional services', 'service description', 'service fees', 'service term', 'service delivery', 'service scope'],
                                                       'weight'        : 1.0,
                                                      },
                            'partnership'           : {'subcategories' : ['business_partnership', 'joint_venture', 'strategic_alliance'],
                                                       'keywords'      : ['partnership', 'joint venture', 'equity', 'shares', 'profit sharing', 'loss allocation', 'management', 'governance', 'voting rights', 'dissolution', 'capital contribution', 'distribution', 'membership interest', 'operating agreement', 'board of directors', 'partnership agreement'],
                                                       'weight'        : 1.0,
                                                      },
                            'lease'                 : {'subcategories' : ['residential_lease', 'commercial_lease', 'sublease', 'equipment_lease'],
                                                       'keywords'      : ['landlord', 'tenant', 'lease', 'premises', 'rent', 'property', 'security deposit', 'utilities', 'maintenance', 'repairs', 'eviction', 'lease term', 'renewal', 'square footage', 'rental agreement', 'lessor', 'lessee', 'property management', 'common areas', 'quiet enjoyment'],
                                                       'weight'        : 1.0,
                                                      },
                            'purchase'              : {'subcategories' : ['asset_purchase', 'stock_purchase', 'goods_purchase'],
                                                       'keywords'      : ['purchase', 'sale', 'buyer', 'seller', 'goods', 'products', 'delivery', 'shipment', 'payment terms', 'invoice', 'purchase price', 'quantity', 'specifications', 'purchase order', 'sales agreement', 'bill of sale', 'title transfer', 'risk of loss', 'closing date'],
                                                       'weight'        : 1.0,
                                                      },
                            'general'               : {'subcategories' : ['standard_agreement', 'basic_contract'],
                                                       'keywords'      : ['agreement', 'contract', 'party', 'parties', 'terms and conditions', 'governing law', 'jurisdiction', 'dispute resolution', 'force majeure', 'notice', 'amendment', 'assignment', 'severability', 'entire agreement'],
                                                       'weight'        : 0.8,
                                                      },
                           } 
    
    # SUBCATEGORY DETECTION PATTERNS
    SUBCATEGORY_PATTERNS = {'full_time'                 : ['full-time', 'full time', 'permanent', 'regular employee', '40 hours', 'exempt employee', 'salary basis'],
                            'part_time'                 : ['part-time', 'part time', 'hours per week', 'non-exempt', 'hourly employee', 'temporary', 'seasonal'],
                            'contract_worker'           : ['independent contractor', 'contract', 'fixed term', 'temporary', 'contract period', 'contract worker', 'contract employee'],
                            'internship'                : ['intern', 'internship', 'student', 'training program', 'educational', 'college credit', 'unpaid intern'],
                            'executive'                 : ['executive', 'ceo', 'cfo', 'cto', 'president', 'vice president', 'director', 'officer', 'executive compensation', 'stock options', 'golden parachute'],
                            'independent_contractor'    : ['independent contractor', '1099', 'contractor', 'self-employed', 'freelance', 'consultant agreement'],
                            'advisory'                  : ['advisor', 'advisory', 'counsel', 'consulting services', 'expert advice', 'advisory board', 'strategic advisory'],
                            'professional_services'     : ['professional services', 'consulting services', 'engagement', 'service provider', 'professional firm'],
                            'freelance'                 : ['freelance', 'freelancer', 'gig', 'project-based', 'freelance work', 'gig economy'],
                            'mutual_nda'                : ['mutual', 'both parties', 'each party', 'reciprocal', 'mutual confidentiality', 'two-way'],
                            'unilateral_nda'            : ['one-way', 'receiving party', 'disclosing party', 'unilateral', 'single party', 'one party'],
                            'confidentiality_agreement' : ['confidentiality agreement', 'secrecy agreement', 'proprietary information agreement'],
                            'software_license'          : ['software license', 'license key', 'perpetual license', 'end user license', 'software agreement'],
                            'saas'                      : ['software as a service', 'saas', 'subscription', 'cloud-based', 'web-based', 'online service'],
                            'cloud_services'            : ['cloud services', 'cloud computing', 'infrastructure', 'iaas', 'paas', 'cloud hosting'],
                            'development'               : ['software development', 'custom development', 'development services', 'programming', 'coding'],
                            'api_access'                : ['api', 'application programming interface', 'api access', 'api key', 'rest api', 'graphql'],
                            'master_services'           : ['master services agreement', 'msa', 'master agreement', 'framework agreement'],
                            'maintenance'               : ['maintenance agreement', 'maintenance services', 'preventive maintenance', 'repair services'],
                            'support'                   : ['support agreement', 'technical support', 'customer support', 'help desk'],
                            'subscription'              : ['subscription agreement', 'subscription service', 'recurring billing', 'subscription fee'],
                            'business_partnership'      : ['partnership', 'general partnership', 'limited partnership', 'partnership agreement'],
                            'joint_venture'             : ['joint venture', 'jv agreement', 'joint venture agreement', 'strategic alliance'],
                            'strategic_alliance'        : ['strategic alliance', 'collaboration agreement', 'cooperation agreement'],
                            'residential_lease'         : ['residential', 'apartment', 'house', 'dwelling', 'residential property', 'tenant', 'landlord', 'rental'],
                            'commercial_lease'          : ['commercial', 'office space', 'retail space', 'commercial property', 'business premises', 'commercial tenant'],
                            'sublease'                  : ['sublease', 'sublet', 'subtenant', 'sublessee', 'sublessor'],
                            'equipment_lease'           : ['equipment lease', 'equipment rental', 'lease equipment', 'leased property'],
                            'asset_purchase'            : ['asset purchase', 'business assets', 'asset sale', 'purchase assets'],
                            'stock_purchase'            : ['stock purchase', 'share purchase', 'equity purchase', 'stock sale'],
                            'goods_purchase'            : ['goods purchase', 'product purchase', 'merchandise', 'inventory purchase'],
                            'standard_agreement'        : ['standard agreement', 'template agreement', 'boilerplate contract'],
                            'basic_contract'            : ['basic contract', 'simple agreement', 'standard terms'],
                           }

    DEFAULT_CONFIDENCE_THRESHOLD = 0.65 
    MULTI_LABEL_THRESHOLD        = 0.55  
                        

    def __init__(self, model_loader):
        """
        Initialize contract classifier
        
        Arguments:
        ----------
            model_loader : ModelLoader instance for accessing Legal-BERT and embeddings
         """
        self.model_loader         = model_loader
        self.embedding_model      = None
        self.legal_bert_model     = None
        self.legal_bert_tokenizer = None
        self.device               = None
        
        # Category template embeddings (computed once)
        self.category_embeddings  = dict()
        
        # Text processor for preprocessing : Don't need spaCy for classification
        self.text_processor       = TextProcessor(use_spacy = False)  
        
        # Logger
        self.logger               = ContractAnalyzerLogger.get_logger()
        
        # Lazy load models
        self._lazy_load()

    
    def _lazy_load(self):
        """
        Lazy load models on first use
        """
        if self.embedding_model is None:
            try:
                log_info("Loading models for contract classification...")
                
                # Load embedding model
                self.embedding_model                             = self.model_loader.load_embedding_model()
                
                # Load Legal-BERT
                self.legal_bert_model, self.legal_bert_tokenizer = self.model_loader.load_legal_bert()
                self.device                                      = self.model_loader.device
                
                # Prepare category embeddings
                self._prepare_category_embeddings()
                
                log_info("Contract classifier models loaded successfully")
                
            except Exception as e:
                log_error(e, context = {"component" : "ContractClassifier", "operation" : "model_loading"})
                raise

    
    def _extract_classification_context(self, full_text: str) -> str:
        """
        Extract key legal sections for more accurate classification
        Focuses on preamble, definitions, and core agreement sections
        
        Arguments:
        ----------
            full_text { str } : Full contract text
        
        Returns:
        --------
                { str }    : Context-rich excerpt for classification
        """
        sections = list()
        
        # First 2000 chars (usually contains parties, effective date, preamble)
        sections.append(full_text[:2000])
        
        # WHEREAS clauses (recitals - explains purpose and background)
        whereas_section = self._extract_section_between(full_text, "WHEREAS", "NOW THEREFORE")
        
        if whereas_section:
            sections.append(whereas_section)
        
        # AGREEMENT section (core contractual terms)
        agreement_section = self._extract_section_between(full_text, "AGREEMENT", "TERMS AND CONDITIONS")
        
        if not agreement_section:
            agreement_section = self._extract_section_containing(full_text, ["AGREES AS FOLLOWS", "HEREBY AGREES"])
        
        if agreement_section:
            sections.append(agreement_section)
        
        # Key definition sections
        definitions_section = self._extract_section_containing(full_text, ["DEFINITIONS", "MEANING OF TERMS"])
        
        if definitions_section:
            sections.append(definitions_section)
        
        # Combine and clean
        context = " ".join([section.strip() for section in sections if section and section.strip()])
        
        # Fallback to original text if context extraction failed
        return context if (len(context) > 500) else full_text


    def _extract_section_between(self, text: str, start_marker: str, end_marker: str) -> Optional[str]:
        """
        Extract text between two markers (case-insensitive)
        """
        try:
            pattern = re.compile(f"{re.escape(start_marker)}(.*?){re.escape(end_marker)}", re.IGNORECASE | re.DOTALL)
            match   = pattern.search(text)
            
            return match.group(1).strip() if match else None
        
        except Exception:
            return None


    def _extract_section_containing(self, text: str, markers: List[str]) -> Optional[str]:
        """
        Extract section containing any of the markers
        """
        for marker in markers:
            if marker.lower() in text.lower():
                # Extract 500 chars around the marker
                idx   = text.lower().find(marker.lower())
                start = max(0, idx - 250)
                end   = min(len(text), idx + len(marker) + 250)
                
                return text[start:end]
        
        return None

    
    def _prepare_category_embeddings(self):
        """
        Pre-compute embeddings for each category template
        """
        log_info("Preparing category embeddings...")
        
        # More specific templates for each category
        category_templates = {
            'employment': "Employment agreement between employer and employee covering salary benefits job duties work hours vacation sick leave performance reviews termination conditions confidentiality and intellectual property rights",
            'consulting': "Consulting services agreement with independent contractor statement of work deliverables hourly rate project scope milestones acceptance criteria work product ownership and payment terms for professional services",
            'nda': "Non-disclosure agreement protecting confidential information trade secrets proprietary data between parties with confidentiality obligations non-use provisions and return of information requirements",
            'software': "Software license agreement or SaaS subscription for technology services including source code access updates maintenance support service level agreements uptime guarantees and API access",
            'service': "Service level agreement for professional services maintenance support with performance metrics service credits response times uptime guarantees and implementation requirements",
            'partnership': "Business partnership joint venture agreement covering equity shares profit distribution management governance voting rights dissolution terms and capital contributions",
            'lease': "Real estate lease agreement for property rental covering premises description rent payments security deposits maintenance responsibilities utilities and eviction terms",
            'purchase': "Sales purchase agreement for goods products with buyer seller terms covering delivery shipment payment terms invoices purchase price quantity specifications and title transfer",
            'general': "General contract agreement with standard terms and conditions governing law jurisdiction dispute resolution force majeure notice provisions and general legal framework"
        }
        
        for category, template in category_templates.items():
            # Encode template
            embedding                          = self.embedding_model.encode(template, convert_to_tensor = True)
            self.category_embeddings[category] = embedding
        
        log_info(f"Prepared embeddings for {len(self.category_embeddings)} categories")
    

    # MAIN CLASSIFICATION METHOD
    @ContractAnalyzerLogger.log_execution_time("classify_contract")
    def classify_contract(self, contract_text: str, min_confidence: float = 0.50) -> ContractCategory:
        """
        Classify contract into granular categories with confidence scoring
        
        Process:
        1. Keyword-based initial scoring
        2. Semantic similarity with embeddings
        3. Legal-BERT enhanced classification
        4. Subcategory detection
        5. Confidence calibration
        
        Arguments:
        ----------
            contract_text   { str }  : Full contract text

            min_confidence { float } : Minimum confidence threshold (0.0-1.0)
        
        Returns:
        --------

            { ContractCategory }     : ContractCategory object with classification results
        """
        # Validate input
        if (not contract_text or (len(contract_text) < 100)):
            raise ValueError("Contract text too short for classification")
        
        # Use default threshold if not specified
        if min_confidence is None:
            min_confidence = self.DEFAULT_CONFIDENCE_THRESHOLD

        # Preprocess text 
        text_excerpt = self._extract_classification_context(full_text = contract_text)
        
        log_info("Starting contract classification", 
                 text_length    = len(contract_text),
                 excerpt_length = len(text_excerpt),
                )
        
        # Keyword scoring
        keyword_scores    = self._score_keywords(text_lower = contract_text.lower())

        # Semantic similarity
        semantic_scores   = self._semantic_similarity(text = text_excerpt)

        # Legal-BERT semantic similarity (enhanced)
        legal_bert_scores = self._legal_bert_similarity(text = text_excerpt)

        # Combine scores (weighted average)
        combined_scores   = self._combine_scores(keyword_scores    = keyword_scores,
                                                 semantic_scores   = semantic_scores,
                                                 legal_bert_scores = legal_bert_scores,
                                                )
        
        # Get primary category
        if not combined_scores:
            log_info("No categories detected, defaulting to 'general'")
            return ContractCategory(category          = "general",
                                    subcategory       = None,
                                    confidence        = 0.5,
                                    reasoning         = ["Unable to determine specific contract type"],
                                    detected_keywords = [],
                                   )
        
        primary_category       = max(combined_scores, key = combined_scores.get)
        confidence             = combined_scores[primary_category]
        
        # Detect subcategory
        subcategory            = self._detect_subcategory(text             = contract_text, 
                                                          primary_category = primary_category,
                                                         )
        
        # Generate reasoning
        reasoning              = self._generate_reasoning(contract_text     = contract_text,
                                                          primary_category  = primary_category,
                                                          subcategory       = subcategory,
                                                          keyword_scores    = keyword_scores,
                                                          semantic_scores   = semantic_scores,
                                                          legal_bert_scores = legal_bert_scores,
                                                          combined_scores   = combined_scores,
                                                         )
        
        # Extract detected keywords
        detected_keywords      = self._extract_detected_keywords(contract_text, primary_category)
        
        # Get alternative categories: Top 3 alternatives
        alternative_categories = sorted([(cat, score) for cat, score in combined_scores.items() if cat != primary_category],
                                        key     = lambda x: x[1],
                                        reverse = True,
                                       )[:3]
        
        result                 = ContractCategory(category               = primary_category,
                                                  subcategory            = subcategory,
                                                  confidence             = confidence,
                                                  reasoning              = reasoning,
                                                  detected_keywords      = detected_keywords,
                                                  alternative_categories = alternative_categories,
                                                 )
        
        log_info("Contract classified successfully",
                 category    = primary_category,
                 subcategory = subcategory,
                 confidence  = confidence,
                )
        
        return result
    
    
    def _score_keywords(self, text_lower: str) -> Dict[str, float]:
        """
        Score each category based on keyword presence

        Arguments:
        ----------
            text_lower { str } : Lowercase contract text

        Returns:
        --------
               { dict }        : Dictionary of {category: score}
        """
        scores = dict()
        for category, config in self.CATEGORY_HIERARCHY.items():
            keywords      = config['keywords']
            weight        = config['weight']
            
            # Count keyword matches with partial matching for multi-word terms
            keyword_count = 0
            
            for keyword in keywords:
                # Check for exact match or partial match for multi-word terms
                if ' ' in keyword:
                    # For multi-word terms, check if all words appear in text
                    words = keyword.split()
                    if all(word in text_lower for word in words):
                        keyword_count += 1
                
                else:
                    # For single words, exact word boundary match
                    if re.search(rf'\b{re.escape(keyword)}\b', text_lower):
                        keyword_count += 1

            # Normalize by number of keywords and apply weight
            normalized_score = (keyword_count / len(keywords)) * weight
            
            # Cap at 1.0
            scores[category] = min(normalized_score, 1.0)
        
        return scores
    

    def _semantic_similarity(self, text: str) -> Dict[str, float]:
        """
        Calculate semantic similarity to category templates using embeddings
        
        Arguments:
        ----------
            text { str } : Contract text excerpt
        
        Returns:
        --------
            { dict }     : Dictionary of {category: similarity_score}
        """
        # Encode contract text
        text_embedding = self.embedding_model.encode(text, convert_to_tensor = True)
        
        # Calculate similarity to each category
        similarities   = dict()

        for category, cat_embedding in self.category_embeddings.items():
            similarity             = util.cos_sim(text_embedding, cat_embedding)[0][0].item()
            similarities[category] = similarity
        
        return similarities

    
    def _legal_bert_similarity(self, text: str) -> Dict[str, float]:
        """
        Use Legal-BERT for semantic similarity calculation
        
        Arguments:
        ----------
            text { str } : Contract text excerpt
        
        Returns:
        --------
            { dict }     : Dictionary of {category: similarity_score} using Legal-BERT embeddings
        """
        # Get Legal-BERT embedding for the text
        text_embedding = self._get_legal_bert_embedding(text)
        
        # Calculate similarity to each category's Legal-BERT embedding
        similarities   = dict()
        
        for category in self.CATEGORY_HIERARCHY.keys():
            # Get pre-computed category embedding
            cat_embedding          = self._get_legal_bert_embedding(f"This is a {category.replace('_', ' ')} contract agreement")
            
            # Calculate cosine similarity
            similarity             = torch.nn.functional.cosine_similarity(torch.tensor(text_embedding).unsqueeze(0), torch.tensor(cat_embedding).unsqueeze(0)).item()
            
            similarities[category] = similarity
        
        return similarities
    
    
    def _get_legal_bert_embedding(self, text: str) -> np.ndarray:
        """
        Get Legal-BERT embedding for text using [CLS] token
        
        Arguments:
        ----------
            text { str }   : Input text
        
        Returns:
        --------
            { np.ndarray } : Embedding vector
        """
        # Tokenize
        inputs = self.legal_bert_tokenizer(text,
                                           return_tensors = "pt",
                                           padding        = True,
                                           truncation     = True,
                                           max_length     = 512,
                                          ).to(self.device)
        
        # Get embeddings
        with torch.no_grad():
            outputs       = self.legal_bert_model(**inputs)
            # Use [CLS] token embedding (first token)
            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
        
        return cls_embedding
    

    def _combine_scores(self, keyword_scores: Dict[str, float], semantic_scores: Dict[str, float], legal_bert_scores: Dict[str, float] = None) -> Dict[str, float]:
        """
        Combine scores from different methods (weighted average)
        
        Arguments:
        ----------
            keyword_scores    { dict } : Keyword-based scores

            semantic_scores   { dict } : Semantic similarity scores
            
            legal_bert_scores { dict } : Legal-BERT similarity scores (optional)
        
        Returns:
        --------
                   { dict }            : Combined scores dictionary
        """
        combined          = dict()
        
        # Weights for each method
        keyword_weight    = 0.35  
        semantic_weight   = 0.35 
        legal_bert_weight = 0.30 
        
        for category in self.CATEGORY_HIERARCHY.keys():
            score = (keyword_scores.get(category, 0) * keyword_weight + 
                     semantic_scores.get(category, 0) * semantic_weight + 
                     legal_bert_scores.get(category, 0) * legal_bert_weight
                    )
            
            combined[category] = score
        
        return combined
    
    
    def _detect_subcategory(self, text: str, primary_category: str) -> Optional[str]:
        """
        Detect specific subcategory within primary category
        
        Arguments:
        ----------
            text             { str } : Full contract text

            primary_category { str } : Detected primary category
        
        Returns:
        --------
                  { str }            : Subcategory name or None
        """
        text_lower    = text.lower()
        
        # Get subcategories for this category
        subcategories = self.CATEGORY_HIERARCHY[primary_category]['subcategories']
        
        # Score each subcategory
        subcat_scores = dict()

        for subcat in subcategories:
            if subcat in self.SUBCATEGORY_PATTERNS:
                patterns              = self.SUBCATEGORY_PATTERNS[subcat]
                score                 = sum(1 for pattern in patterns if pattern in text_lower)
                subcat_scores[subcat] = score
        
        # Return best match if any
        if (subcat_scores and (max(subcat_scores.values()) > 0)):
            best_subcat = max(subcat_scores, key = subcat_scores.get)
            log_info(f"Detected subcategory: {best_subcat}", 
                     category = primary_category,
                     score    = subcat_scores[best_subcat],
                    )

            return best_subcat
        
        return None
    

    def _generate_reasoning(self, contract_text: str, primary_category: str, subcategory: Optional[str], keyword_scores: Dict[str, float], 
                            semantic_scores: Dict[str, float], legal_bert_scores: Dict[str, float], combined_scores: Dict[str, float]) -> List[str]:
        """
        Generate human-readable reasoning for classification
        
        Returns:
        --------
            { list } : List of reasoning statements
        """
        reasoning        = list()
        
        # Primary category reasoning
        keyword_match    = keyword_scores.get(primary_category, 0)
        semantic_match   = semantic_scores.get(primary_category, 0)
        legal_bert_match = legal_bert_scores.get(primary_category, 0)
        
        # Keyword-based reasoning
        if (keyword_match > 0.6):
            reasoning.append(f"Strong keyword indicators for {primary_category.replace('_', ' ')} category ({int(keyword_match * 100)}% keyword match)")

        elif (keyword_match > 0.3):
            reasoning.append(f"Moderate keyword presence for {primary_category.replace('_', ' ')} ({int(keyword_match * 100)}% keyword match)")

        elif (keyword_match > 0.1):
            reasoning.append(f"Limited keyword indicators for {primary_category.replace('_', ' ')} ({int(keyword_match * 100)}% keyword match)")
        
        # Semantic similarity reasoning
        if (semantic_match > 0.70):
            reasoning.append(f"High semantic similarity to {primary_category.replace('_', ' ')} agreements (similarity: {semantic_match:.2f})")

        elif (semantic_match > 0.55):
            reasoning.append(f"Moderate semantic similarity to {primary_category.replace('_', ' ')} contracts (similarity: {semantic_match:.2f})"
                            )
        
        # Legal-BERT reasoning
        if (legal_bert_match > 0.65):
            reasoning.append(f"Legal-BERT analysis strongly supports {primary_category.replace('_', ' ')} classification (similarity: {legal_bert_match:.2f})"
                            )

        elif (legal_bert_match > 0.50):
            reasoning.append(f"Legal-BERT analysis moderately supports {primary_category.replace('_', ' ')} classification (similarity: {legal_bert_match:.2f})"
                            )
        
        # Subcategory reasoning
        if subcategory:
            reasoning.append(f"Specific subcategory identified: {subcategory.replace('_', ' ')}")
        
        # Alternative categories (if close)
        sorted_scores = sorted(combined_scores.items(), key = lambda x: x[1], reverse = True)
        
        if ((len(sorted_scores) > 1) and (sorted_scores[1][1] > 0.30)):
            alt_category, alt_score = sorted_scores[1]
            
            reasoning.append(f"Also contains elements of {alt_category.replace('_', ' ')} (secondary match: {alt_score:.2f})")
        
        # If no strong reasoning
        if not reasoning:
            reasoning.append("Classification based on general contract structure and terminology")
        
        return reasoning

    
    def _extract_detected_keywords(self, text: str, category: str) -> List[str]:
        """
        Extract which specific keywords were found
        
        Arguments:
        ----------
            text     { str } : Contract text

            category { str } : Detected category
        
        Returns:
        --------
                { list }     : List of detected keywords
        """
        text_lower = text.lower()
        keywords   = self.CATEGORY_HIERARCHY[category]['keywords']
        
        detected   = [kw for kw in keywords if kw in text_lower]

        # Return all detected keywords
        return detected
    
    
    @ContractAnalyzerLogger.log_execution_time("classify_multi_label")
    def classify_multi_label(self, text: str, threshold: float = None) -> List[ContractCategory]:
        """
        Classify as multiple categories if applicable (e.g., Employment + NDA, Consulting + IP Assignment)
        
        Arguments:
        ----------
            text       { str }  : Contract text

            threshold { float } : Minimum confidence threshold for multi-label
        
        Returns:
        --------
                 { list }       : List of ContractCategory objects (sorted by confidence)
        """
        # Use multi-label threshold if not specified
        if threshold is None:
            threshold = self.MULTI_LABEL_THRESHOLD

        log_info("Starting multi-label classification", threshold = threshold)
        
        # Get scores
        keyword_scores    = self._score_keywords(text_lower = text.lower())
        semantic_scores   = self._semantic_similarity(text = text)
        legal_bert_scores = self._legal_bert_similarity(text = text)
        combined_scores   = self._combine_scores(keyword_scores    = keyword_scores, 
                                                 semantic_scores   = semantic_scores, 
                                                 legal_bert_scores = legal_bert_scores,
                                                )
        
        # Get all categories above threshold
        matches         = list()

        for category, score in combined_scores.items():
            if (score >= threshold):
                subcategory = self._detect_subcategory(text             = text, 
                                                       primary_category = category,
                                                      )
                                                  
                reasoning   = self._generate_reasoning(contract_text     = text, 
                                                       primary_category  = category, 
                                                       subcategory       = subcategory, 
                                                       keyword_scores    = keyword_scores, 
                                                       semantic_scores   = semantic_scores, 
                                                       legal_bert_scores = legal_bert_scores, 
                                                       combined_scores   = combined_scores,
                                                      )

                keywords    = self._extract_detected_keywords(text     = text, 
                                                              category = category,
                                                             )
                
                matches.append(ContractCategory(category          = category,
                                                subcategory       = subcategory,
                                                confidence        = score,
                                                reasoning         = reasoning,
                                                detected_keywords = keywords,
                                               )
                              )
        
        # Sort by confidence
        matches.sort(key = lambda x: x.confidence, reverse = True)
        
        log_info(f"Multi-label classification found {len(matches)} categories")
        
        return matches if matches else [self.classify_contract(text)]
    

    def get_category_description(self, category: str) -> str:
        """
        Get human-readable description of a category
        """
        descriptions = {'employment'  : 'Employment agreements governing employer-employee relationships',
                        'consulting'  : 'Consulting and independent contractor agreements',
                        'nda'         : 'Non-disclosure and confidentiality agreements',
                        'software'    : 'Software licensing and technology service agreements',
                        'service'     : 'Professional service and maintenance agreements',
                        'partnership' : 'Partnership, joint venture, and corporate agreements',
                        'lease'       : 'Property lease, rental, and equipment lease agreements',
                        'purchase'    : 'Sales, purchase, and goods transfer agreements',
                        'general'     : 'General contract agreements with standard terms and conditions',
                       }

        return descriptions.get(category, 'General contract agreement')

    
    def get_all_categories(self) -> List[str]:
        """
        Get list of all supported categories
        """
        return list(self.CATEGORY_HIERARCHY.keys())
    

    def get_subcategories(self, category: str) -> List[str]:
        """
        Get subcategories for a specific category
        """
        return self.CATEGORY_HIERARCHY.get(category, {}).get('subcategories', [])


#########################################
# services/clause_extractor.py
#########################################
# DEPENDENCIES
import re
import sys
import torch
import numpy as np
from typing import Any
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional
from collections import defaultdict
from sentence_transformers import util

# Import utilities
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import RiskRules
from config.risk_rules import ContractType
from utils.text_processor import TextProcessor
from utils.logger import ContractAnalyzerLogger
from services.data_models import ExtractedClause
from model_manager.model_loader import ModelLoader
from services.data_models import ClauseInterpretation


class ComprehensiveClauseExtractor:
    """
    For general clause extraction across all contract types : Extracts and classifies clauses using Legal-BERT + structural patterns
    
    Will be used for: General document analysis, clause discovery, contract understanding
    """
    # COMPREHENSIVE CLAUSE CATEGORIES COVERING ALL LEGAL AREAS
    CLAUSE_CATEGORIES = {'compensation'          : {'keywords'            : ['salary', 'wage', 'compensation', 'pay', 'payment', 'bonus', 'commission', 'remuneration', 'fee', 'rate', 'benefits', 'equity', 'stock options', 'incentive'],
                                                    'representative_text' : ("The Employee shall receive an annual base salary of One Hundred Thousand Dollars payable in accordance with the Company's standard payroll practices. Additional compensation may include performance bonuses and stock options."),
                                                    'weight'              : 1.0,
                                                   },
                         'termination'           : {'keywords'            : ['termination', 'terminate', 'notice period', 'resignation', 'dismissal', 'severance', 'end of employment', 'cessation', 'notice', 'for cause', 'without cause'],
                                                    'representative_text' : ("Either party may terminate this Agreement upon thirty days written notice. The Company may terminate for cause immediately upon written notice to Employee. Upon termination, Employee shall receive severance compensation."),
                                                    'weight'              : 1.2,
                                                   },
                         'non_compete'           : {'keywords'            : ['non-compete', 'non-solicit', 'non-solicitation', 'restrictive covenant', 'competitive', 'competition', 'competing business', 'competitive activities', 'non-competition'],
                                                    'representative_text' : ("Employee agrees not to engage in any competitive business activities for a period of twelve months following termination within a fifty-mile radius. Employee shall not solicit Company clients or employees during this period."),
                                                    'weight'              : 1.5,
                                                   },
                         'confidentiality'       : {'keywords'            : ['confidential', 'proprietary', 'trade secret', 'disclosure', 'confidentiality', 'secret', 'private', 'non-disclosure', 'protected information'],
                                                    'representative_text' : ("Employee shall maintain the confidentiality of all proprietary information and trade secrets of the Company. Confidential Information includes business plans, customer lists, and technical data. These obligations survive termination."),
                                                    'weight'              : 1.1,
                                                   },
                         'indemnification'       : {'keywords'            : ['indemnify', 'indemnification', 'hold harmless', 'defend', 'liability', 'claims', 'losses', 'damages', 'indemnity'],
                                                    'representative_text' : ("Party A shall indemnify and hold harmless Party B from any claims, losses, or damages arising from Party A's breach or negligence. This indemnification includes reasonable attorneys' fees and costs of defense."),
                                                    'weight'              : 1.3,
                                                   },
                         'intellectual_property' : {'keywords'            : ['intellectual property', 'ip', 'copyright', 'patent', 'trademark', 'work product', 'inventions', 'creation', 'ownership', 'ip rights', 'proprietary rights'],
                                                    'representative_text' : ("All work product and inventions created by Employee during employment shall be the exclusive property of the Company. Employee assigns all intellectual property rights including patents, copyrights, and trade secrets to the Company."),
                                                    'weight'              : 1.2,
                                                   },
                         'liability'             : {'keywords'            : ['liable', 'liability', 'damages', 'limitation', 'consequential', 'indirect', 'punitive', 'cap', 'limited liability', 'responsibility'],
                                                    'representative_text' : ("In no event shall either party be liable for indirect, incidental, or consequential damages. Total liability under this Agreement shall not exceed the amounts paid in the twelve months preceding the claim."),
                                                    'weight'              : 1.2,
                                                   },
                         'warranty'              : {'keywords'            : ['warranty', 'warrant', 'representation', 'guarantee', 'assurance', 'promise', 'warranties', 'guaranty'],
                                                    'representative_text' : ("Company warrants that the Services will be performed in a professional manner. EXCEPT AS EXPRESSLY PROVIDED, COMPANY DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE."),
                                                    'weight'              : 0.9,
                                                   },
                         'dispute_resolution'    : {'keywords'            : ['arbitration', 'mediation', 'dispute', 'jurisdiction', 'governing law', 'venue', 'forum', 'resolution', 'litigation'],
                                                    'representative_text' : ("Any disputes arising under this Agreement shall be resolved through binding arbitration in accordance with the rules of the American Arbitration Association. This Agreement shall be governed by the laws of the State of California."),
                                                    'weight'              : 0.9,
                                                   },
                         'insurance'             : {'keywords'            : ['insurance', 'coverage', 'insured', 'policy', 'premium', 'insurer', 'liability insurance'],
                                                    'representative_text' : ("Contractor shall maintain general liability insurance with minimum coverage of one million dollars per occurrence. Proof of insurance shall be provided to Client. Company shall be named as additional insured on all policies."),
                                                    'weight'              : 0.8,
                                                   },
                         'assignment'            : {'keywords'            : ['assignment', 'assign', 'transfer', 'successor', 'binding', 'assignee', 'assignor'],
                                                    'representative_text' : ("This Agreement may not be assigned by either party without the prior written consent of the other party. This Agreement shall be binding upon and inure to the benefit of the parties' successors and permitted assigns."),
                                                    'weight'              : 0.8,
                                                   },
                         'amendment'             : {'keywords'            : ['amendment', 'modify', 'modification', 'change', 'alteration', 'waiver', 'amend'],
                                                    'representative_text' : ("This Agreement may not be amended or modified except by written instrument signed by both parties. No waiver of any provision shall be effective unless in writing. All modifications must be mutually agreed upon."),
                                                    'weight'              : 0.7,
                                                   },
                         'force_majeure'         : {'keywords'            : ['force majeure', 'act of god', 'unforeseeable', 'beyond control', 'natural disaster', 'unforeseen circumstances'],
                                                    'representative_text' : ("Neither party shall be liable for failure to perform due to causes beyond its reasonable control including acts of God, war, strikes, or natural disasters. Performance shall be suspended during the force majeure event."),
                                                    'weight'              : 0.7,
                                                   },
                         'entire_agreement'      : {'keywords'            : ['entire agreement', 'integration', 'supersedes', 'prior agreements', 'complete agreement', 'whole agreement'],
                                                    'representative_text' : ("This Agreement constitutes the entire agreement between the parties and supersedes all prior agreements, whether written or oral. No other representations or warranties shall be binding unless incorporated herein."),
                                                    'weight'              : 0.6,
                                                   },
                         'payment_terms'         : {'keywords'            : ['payment terms', 'net 30', 'due date', 'invoice', 'billing', 'payment due', 'late payment', 'interest'],
                                                    'representative_text' : ("Payment shall be due within thirty days of invoice date. Late payments shall accrue interest at the rate of 1.5% per month. All payments shall be made in US dollars."),
                                                    'weight'              : 0.9,
                                                   },
                         'governing_law'         : {'keywords'            : ['governing law', 'jurisdiction', 'venue', 'applicable law', 'state law', 'federal law'],
                                                    'representative_text' : ("This Agreement shall be governed by and construed in accordance with the laws of the State of Delaware. Any legal action shall be brought in the state or federal courts located in Wilmington, Delaware."),
                                                    'weight'              : 0.8,
                                                   },
                         'general'               : {'keywords'            : ['provision', 'term', 'condition', 'obligation', 'requirement', 'clause', 'section'],
                                                    'representative_text' : ("The parties agree to the following terms and conditions governing their relationship. Each party shall perform its obligations in good faith and in accordance with industry standards and applicable law."),
                                                    'weight'              : 0.5,
                                                   }
                        }
    

    def __init__(self, model_loader: ModelLoader):
        """
        Initialize comprehensive clause extractor
        
        Arguments:
        ----------
            model_loader { ModelLoader } : ModelLoader instance for accessing Legal-BERT
        """
        self.model_loader         = model_loader
        
        # Models (lazy loaded)
        self.legal_bert_model     = None
        self.legal_bert_tokenizer = None
        self.embedding_model      = None
        self.device               = None
        
        # Category embeddings (computed from representative texts)
        self.category_embeddings  = dict()
        
        # Text processor
        self.text_processor       = TextProcessor(use_spacy = False)
        
        # Logger
        self.logger               = ContractAnalyzerLogger.get_logger()
        
        # Lazy load
        self._lazy_load()

        # Risk Rules 
        self.risk_rules           = RiskRules()
    

    def _lazy_load(self):
        """
        Lazy load Legal-BERT and embedding models
        """
        if self.legal_bert_model is None:
            try:
                log_info("Loading Legal-BERT for comprehensive clause extraction...")
                
                # Load Legal-BERT (nlpaueb/legal-bert-base-uncased)
                self.legal_bert_model, self.legal_bert_tokenizer = self.model_loader.load_legal_bert()
                self.device                                      = self.model_loader.device
                
                # Load sentence transformer for embeddings
                self.embedding_model                             = self.model_loader.load_embedding_model()
                
                # Prepare category embeddings using Legal-BERT
                self._prepare_category_embeddings()
                
                log_info("Comprehensive clause extractor models loaded successfully")
                
            except Exception as e:
                log_error(e, context = {"component": "ComprehensiveClauseExtractor", "operation": "model_loading"})
                raise

    
    def _prepare_category_embeddings(self):
        """
        Pre-compute Legal-BERT embeddings for category representative texts
        """
        log_info("Computing Legal-BERT embeddings for comprehensive clause categories...")
        
        for category, config in self.CLAUSE_CATEGORIES.items():
            representative_text                = config['representative_text']
            
            # Get Legal-BERT embedding (using [CLS] token)
            embedding                          = self._get_legal_bert_embedding(text = representative_text)

            self.category_embeddings[category] = embedding
        
        log_info(f"Prepared Legal-BERT embeddings for {len(self.category_embeddings)} categories")
    

    def _get_legal_bert_embedding(self, text: str) -> np.ndarray:
        """
        Get Legal-BERT embedding for text using [CLS] token
        """
        # Tokenize
        inputs = self.legal_bert_tokenizer(text,
                                           return_tensors = "pt",
                                           padding        = True,
                                           truncation     = True,
                                           max_length     = 512,
                                          ).to(self.device)
        
        # Get embeddings
        with torch.no_grad():
            outputs       = self.legal_bert_model(**inputs)
            # Use [CLS] token embedding (first token)
            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
        
        return cls_embedding
    

    @ContractAnalyzerLogger.log_execution_time("extract_clauses")
    def extract_clauses(self, contract_text: str, max_clauses: int = 25) -> List[ExtractedClause]:
        """
        Extract and classify clauses from contract using hybrid approach
        
        Process:
        1. Structural extraction (numbered sections)
        2. Semantic chunking (for unstructured text)
        3. Legal-BERT classification
        4. Deduplicate and rank by confidence
        
        Arguments:
        ----------
            contract_text { str } : Full contract text

            max_clauses   { int } : Maximum number of clauses to return
        
        Returns:
        --------
                 { list }         : List of ExtractedClause objects sorted by confidence
        """
        
        log_info("Starting comprehensive clause extraction", 
                 text_length = len(contract_text),
                 max_clauses = max_clauses,
                )
        
        # Extract using structural patterns
        structural_clauses = self._extract_structural_clauses(contract_text)
        log_info(f"Extracted {len(structural_clauses)} structural clauses")
        
        # Semantic chunking for unstructured parts
        semantic_chunks    = self._semantic_chunking(contract_text, structural_clauses)
        log_info(f"Created {len(semantic_chunks)} semantic chunks")
        
        # Combine all candidates
        all_candidates     = structural_clauses + semantic_chunks
        log_info(f"Total candidates: {len(all_candidates)}")
        
        # Classify with Legal-BERT
        classified_clauses = self._classify_clauses_with_legal_bert(all_candidates)
        log_info(f"Classified {len(classified_clauses)} clauses")
        
        # Deduplicate and rank
        final_clauses      = self._deduplicate_and_rank(classified_clauses, max_clauses)
        log_info(f"Final output: {len(final_clauses)} clauses")
        
        return final_clauses
    

    def generate_clause_analysis(self, clause: ExtractedClause, llm_interpretation: ClauseInterpretation = None) -> Dict[str, str]:
        """
        Generate analysis and recommendation for a clause
        
        Arguments:
        ----------
            clause               { ExtractedClause }    : ExtractedClause object

            llm_interpretation { ClauseInterpretation } : Optional ClauseInterpretation from LLM
        
        Returns:
        --------
                           { dict }                     : Dictionary with 'analysis' and 'recommendation' keys
        """
        if llm_interpretation:
            # Use LLM interpretation if available
            analysis = llm_interpretation.plain_english_summary
            
            # Combine key points into analysis
            if llm_interpretation.key_points:
                analysis += " " + " ".join(llm_interpretation.key_points[:2])
            
            # Combine potential risks into analysis
            if llm_interpretation.potential_risks:
                risk_text = " Key risks: " + ", ".join(llm_interpretation.potential_risks[:2])
                analysis += risk_text
            
            # Use suggested improvements as recommendation
            if llm_interpretation.suggested_improvements:
                recommendation = " ".join(llm_interpretation.suggested_improvements[:2])
            
            else:
                recommendation = "Review this clause with legal counsel for specific recommendations."
        
        else:
            # Fallback: Generate analysis from risk indicators and category
            risk_indicators = clause.risk_indicators if clause.risk_indicators else []
            risk_score      = getattr(clause, 'risk_score', 0)
            
            # Generate specific analysis based on category and risk
            analysis        = self._generate_fallback_analysis(clause          = clause, 
                                                               risk_indicators = risk_indicators, 
                                                               risk_score      = risk_score,
                                                              )

            recommendation  = self._generate_fallback_recommendation(clause          = clause, 
                                                                     risk_indicators = risk_indicators, 
                                                                     risk_score      = risk_score,
                                                                    )
        
        return {'analysis'       : analysis,
                'recommendation' : recommendation,
               }


    def _generate_fallback_analysis(self, clause: ExtractedClause, risk_indicators: List[str], risk_score: float) -> str:
        """
        Generate fallback analysis when LLM unavailable
        """
        category_analyses = {'compensation'          : f"This compensation clause {'contains concerning terms' if risk_score > 50 else 'appears standard'} regarding payment obligations and structures. ",
                             'termination'           : f"This termination clause {'creates significant imbalance' if risk_score > 60 else 'establishes'} the conditions and procedures for ending the agreement. ",
                             'non_compete'           : f"This restrictive covenant {'is overly broad and' if risk_score > 60 else ''} limits future business activities and employment opportunities. ",
                             'confidentiality'       : f"This confidentiality provision {'has excessive scope' if risk_score > 50 else 'defines'} the obligations to protect sensitive information. ",
                             'indemnification'       : f"This indemnification clause {'creates one-sided liability exposure' if risk_score > 60 else 'allocates'} responsibility for claims and losses. ",
                             'intellectual_property' : f"This IP clause {'may claim overly broad ownership' if risk_score > 50 else 'addresses'} rights to work product and inventions. ",
                             'liability'             : f"This liability provision {'lacks adequate caps or limitations' if risk_score > 60 else 'establishes'} the financial exposure for damages. ",
                            }
        
        analysis          = category_analyses.get(clause.category, f"This {clause.category} clause establishes specific rights and obligations. ")
        
        # Add risk-specific details
        if risk_indicators:
            analysis += f"Specific concerns include: {', '.join(risk_indicators[:3])}. "
        
        if (risk_score > 70):
            analysis += "This clause requires immediate attention and likely modification."

        elif (risk_score > 50):
            analysis += "This clause should be reviewed carefully and potentially negotiated."

        else:
            analysis += "This clause appears to contain standard provisions for this type of agreement."
        
        return analysis


    def _generate_fallback_recommendation(self, clause: ExtractedClause, risk_indicators: List[str], risk_score: float) -> str:
        """
        Generate fallback recommendation when LLM unavailable
        """
        if (risk_score > 70):
            return f"Strongly recommend negotiating substantial changes to this clause. Seek legal counsel to address the identified risks and ensure your interests are protected."
        
        elif (risk_score > 50):
            return f"Negotiate modifications to balance the terms more fairly. Consider adding protective language or limiting the scope of obligations."
        
        elif (risk_score > 30):
            return f"Review with legal counsel to ensure the terms are clear and acceptable. Minor clarifications may be beneficial."
        
        else:
            return f"Standard clause - review for consistency with the overall agreement and your business needs."

    
    def _extract_structural_clauses(self, text: str) -> List[Dict]:
        """
        Extract clauses using structural numbering patterns
        """
        candidates = list()
        
        # Clean text
        text       = re.sub(r'\s+', ' ', text)
        
        # Patterns for legal numbering
        patterns   = [(r'(\d+\.\d+(?:\.\d+)*)\.\s*([^\n]{30,800}?)(?=\d+\.\d+(?:\.\d+)*\.|$)', 'numbered'),
                      (r'(Article\s+(?:\d+(?:\.\d+)*|[IVXLCDM]+))\.\s*([^\n]{30,800}?)(?=Article\s+(?:\d+|[IVXLCDM]+)|$)', 'article'),
                      (r'(Section\s+\d+(?:\.\d+)*)\.\s*([^\n]{30,800}?)(?=Section\s+\d+|$)', 'section'),
                      (r'(Clause\s+\d+(?:\.\d+)*)\.\s*([^\n]{30,800}?)(?=Clause\s+\d+|$)', 'clause'),
                      (r'\(([a-z]|[ivxlcdm]+)\)\s*([^\n]{30,500}?)(?=\([a-z]|[ivxlcdm]+\)|\n\n|$)', 'subclause'),
                     ]
        
        for pattern, ref_type in patterns:
            matches = re.finditer(pattern, text, re.IGNORECASE | re.DOTALL)

            for match in matches:
                clause_text = match.group(2).strip()
                
                # Filter out boilerplate/definitions
                if not self._is_boilerplate(clause_text):
                    # Check for meaningful content
                    if self._has_meaningful_content(clause_text):
                        candidates.append({'text'      : clause_text,
                                           'reference' : match.group(1).strip(),
                                           'start'     : match.start(),
                                           'end'       : match.end(),
                                           'type'      : 'structural',
                                           'ref_type'  : ref_type,
                                         })
        
        # Remove overlapping clauses
        candidates = self._remove_overlapping(candidates)
        
        return candidates

    
    def _is_boilerplate(self, text: str) -> bool:
        """
        Check if text is boilerplate/definitional rather than substantive
        """
        boilerplate_indicators = ['shall mean', 
                                  'means and includes', 
                                  'defined as', 
                                  'definition of',
                                  'hereinafter referred to', 
                                  'for purposes of this', 
                                  'interpretation of',
                                  'as used in this', 
                                  'the term', 
                                  'shall include', 
                                  'includes but not limited',
                                 ]
        
        text_lower             = text.lower()
        # Must have at least one strong indicator AND be definition-heavy
        has_indicator          = any(indicator in text_lower for indicator in boilerplate_indicators)
        is_short_definition    = len(text.split()) < 50 and '"' in text
        
        return has_indicator or is_short_definition
    

    def _has_meaningful_content(self, text: str) -> bool:
        """
        Check if text has meaningful legal content
        """
        # Must have minimum length
        if (len(text.split()) < 15):
            return False
        
        # Check for legal action verbs
        action_verbs   = ['shall', 
                          'must', 
                          'will', 
                          'may', 
                          'agrees', 
                          'undertakes', 
                          'covenants', 
                          'warrants', 
                          'represents', 
                          'acknowledges', 
                          'certifies', 
                          'indemnifies', 
                          'waives', 
                          'terminates',
                         ]
        
        text_lower     = text.lower()
        has_action     = any(verb in text_lower for verb in action_verbs)
        
        # Check for legal subjects
        legal_subjects = ['party', 
                          'parties', 
                          'employee', 
                          'employer', 
                          'company', 
                          'contractor', 
                          'consultant', 
                          'client',
                          'vendor', 
                          'buyer', 
                          'seller', 
                          'landlord', 
                          'tenant', 
                          'licensor', 
                          'licensee',
                         ]
        
        has_subject    = any(subj in text_lower for subj in legal_subjects)
        
        return has_action or has_subject
    

    def _remove_overlapping(self, candidates: List[Dict]) -> List[Dict]:
        """
        Remove overlapping clause extractions
        """
        if not candidates:
            return []
        
        # Sort by start position
        candidates.sort(key = lambda x: x['start'])
        
        non_overlapping = [candidates[0]]
        
        for candidate in candidates[1:]:
            last = non_overlapping[-1]
            
            # Check if overlaps
            if (candidate['start'] >= last['end']):
                non_overlapping.append(candidate)

            elif (len(candidate['text']) > len(last['text'])):
                # Keep longer clause if overlapping
                non_overlapping[-1] = candidate
        
        return non_overlapping
    
    
    def _semantic_chunking(self, text: str, structural_clauses: List[Dict], chunk_size: int = 200) -> List[Dict]:
        """
        Chunk unstructured text semantically uses sentence boundaries
        """
        # Get covered ranges from structural clauses
        covered_ranges = [(c['start'], c['end']) for c in structural_clauses]
        
        # Split into sentences
        sentences      = self.text_processor.extract_sentences(text)
        
        chunks         = list()
        current_chunk  = list()
        current_length = 0
        current_start  = 0
        
        for sentence in sentences:
            # Check if sentence is already covered by structural extraction
            sentence_start = text.find(sentence, current_start)
            if (sentence_start == -1):
                continue
                
            if self._is_in_range(sentence_start, covered_ranges):
                current_start = sentence_start + len(sentence)
                continue
            
            current_chunk.append(sentence)
            current_length += len(sentence.split())
            
            # Create chunk when reaching size limit
            if (current_length >= chunk_size):
                chunk_text = ' '.join(current_chunk).strip()
                
                if (len(chunk_text) >= 50) and (not self._is_boilerplate(chunk_text)):
                    if self._has_meaningful_content(chunk_text):
                        chunks.append({'text'      : chunk_text,
                                       'reference' : f'Semantic-{len(chunks)+1}',
                                       'start'     : sentence_start,
                                       'end'       : sentence_start + len(chunk_text),
                                       'type'      : 'semantic',
                                       'ref_type'  : 'semantic',
                                     })
                
                current_chunk  = list()
                current_length = 0
            
            current_start = sentence_start + len(sentence)
        
        # Add final chunk if exists
        if current_chunk:
            chunk_text = ' '.join(current_chunk).strip()
            
            if ((len(chunk_text) >= 50) and (not self._is_boilerplate(chunk_text))):
                if self._has_meaningful_content(chunk_text):
                    sentence_start = text.find(current_chunk[0])
                    chunks.append({'text'      : chunk_text,
                                   'reference' : f'Semantic-{len(chunks)+1}',
                                   'start'     : sentence_start,
                                   'end'       : sentence_start + len(chunk_text),
                                   'type'      : 'semantic',
                                   'ref_type'  : 'semantic',
                                 })
        
        return chunks
    

    def _is_in_range(self, position: int, ranges: List[Tuple[int, int]]) -> bool:
        """
        Check if position is within any of the ranges
        """
        return any(start <= position <= end for start, end in ranges)
    
    
    def _classify_clauses_with_legal_bert(self, candidates: List[Dict]) -> List[ExtractedClause]:
        """
        Classify clauses using Legal-BERT embeddings + keyword matching
        """
        classified = list()
        
        for candidate in candidates:
            # Get Legal-BERT embedding for clause
            clause_embedding                       = self._get_legal_bert_embedding(text = candidate['text'])
            
            # Classify using hybrid approach
            category, confidence, legal_bert_score = self._classify_single_clause(text             = candidate['text'], 
                                                                                  clause_embedding = clause_embedding,
                                                                                 )
            
            # Extract risk indicators
            risk_indicators                        = self._extract_risk_indicators(text = candidate['text'])
            
            # Extract sub-clauses if any
            subclauses                             = self._extract_subclauses(text = candidate['text'])
            
            classified.append(ExtractedClause(text              = candidate['text'],
                                              reference         = candidate['reference'],
                                              category          = category,
                                              confidence        = confidence,
                                              start_pos         = candidate['start'],
                                              end_pos           = candidate['end'],
                                              extraction_method = candidate['type'],
                                              risk_indicators   = risk_indicators,
                                              embeddings        = clause_embedding,
                                              subclauses        = subclauses,
                                              legal_bert_score  = legal_bert_score,
                                             )
                             )
        
        return classified
    

    def _classify_single_clause(self, text: str, clause_embedding: np.ndarray) -> Tuple[str, float, float]:
        """
        Classify single clause using Legal-BERT + keyword matching
        """
        text_lower     = text.lower()
        
        # Keyword matching
        keyword_scores = dict()

        for category, config in self.CLAUSE_CATEGORIES.items():
            keywords                 = config['keywords']
            weight                   = config['weight']
            
            keyword_count            = sum(1 for kw in keywords if kw in text_lower)
            keyword_scores[category] = (keyword_count / len(keywords)) * weight
        
        # Legal-BERT semantic similarity
        semantic_scores         = dict()
        clause_embedding_tensor = torch.tensor(clause_embedding).unsqueeze(0)
        
        for category, category_embedding in self.category_embeddings.items():
            category_embedding_tensor = torch.tensor(category_embedding).unsqueeze(0)
            similarity                = torch.nn.functional.cosine_similarity(clause_embedding_tensor, category_embedding_tensor).item()
            semantic_scores[category] = similarity
        
        # Combine scores (70% semantic, 30% keyword)
        combined_scores = dict()

        for category in self.CLAUSE_CATEGORIES.keys():
            combined                  = (semantic_scores.get(category, 0) * 0.70 + keyword_scores.get(category, 0) * 0.30)
            combined_scores[category] = combined
        
        # Get best category
        best_category    = max(combined_scores, key = combined_scores.get)
        confidence       = combined_scores[best_category]
        legal_bert_score = semantic_scores[best_category]
        
        return best_category, confidence, legal_bert_score
    

    def _extract_risk_indicators(self, text: str) -> List[str]:
        """
        Extract risk indicator keywords from clause text using RiskRule with the central risk rules
        """
        text_lower      = text.lower()
        risk_indicators = list()

        # Check for matches against CRITICAL_KEYWORDS from RiskRules
        for keyword in self.risk_rules.CRITICAL_KEYWORDS.keys():
            if keyword in text_lower:
                risk_indicators.append(keyword)

        # Check for matches against HIGH_RISK_KEYWORDS from RiskRules
        for keyword in self.risk_rules.HIGH_RISK_KEYWORDS.keys():
            if keyword in text_lower:
                risk_indicators.append(keyword)

        # Check for matches against MEDIUM_RISK_KEYWORDS from RiskRules
        for keyword in self.risk_rules.MEDIUM_RISK_KEYWORDS.keys():
            if keyword in text_lower:
                risk_indicators.append(keyword)

        # Check for matches against RISKY_PATTERNS from RiskRules
        for pattern, score, description in self.risk_rules.RISKY_PATTERNS:
            if re.search(pattern, text_lower):
                # Use the description from RiskRules as the indicator
                risk_indicators.append(description)

        # Remove duplicates while preserving order
        seen              = set()
        unique_indicators = list()

        for indicator in risk_indicators:
            if indicator not in seen:
                seen.add(indicator)
                unique_indicators.append(indicator)
        
        return unique_indicators
    

    def _extract_subclauses(self, text: str) -> List[str]:
        """
        Extract sub-clauses from main clause (e.g., (a), (b), (i), (ii))
        """
        # Pattern for sub-clauses: (a), (i), etc.
        subclause_pattern = r'\(([a-z]|[ivxlcdm]+)\)\s*([^()]{20,200}?)(?=\([a-z]|[ivxlcdm]+\)|$)'
        matches           = re.findall(subclause_pattern, text, re.IGNORECASE)
        
        subclauses        = list()

        for ref, subtext in matches:
            clean_text = subtext.strip()
            
            if (len(clean_text) >= 20):
                subclauses.append(f"({ref}) {clean_text}")
        
        # Max 25 sub-clauses
        return subclauses[:25]  
    
    
    def _deduplicate_and_rank(self, clauses: List[ExtractedClause], max_clauses: int) -> List[ExtractedClause]:
        """
        Remove duplicates and rank by confidence + legal_bert_score
        """
        if not clauses:
            return []
        
        # Sort by combined score (confidence * 0.6 + legal_bert_score * 0.4)
        clauses.sort(key = lambda x: (x.confidence * 0.6 + x.legal_bert_score * 0.4), reverse = True)
        
        # Deduplicate by text similarity
        unique_clauses = list()
        seen_texts     = set()
        
        for clause in clauses:
            # Simple deduplication by first 100 chars
            text_key     = clause.text[:100].lower().strip()
            
            # Also check similarity to already added clauses
            is_duplicate = False
            
            for existing in unique_clauses:
                similarity = self._text_similarity(clause.text, existing.text)
                if (similarity > 0.85):
                    is_duplicate = True
                    break
            
            if text_key not in seen_texts and not is_duplicate:
                unique_clauses.append(clause)
                seen_texts.add(text_key)
                
                if (len(unique_clauses) >= max_clauses):
                    break
        
        return unique_clauses
    

    def _text_similarity(self, text1: str, text2: str) -> float:
        """
        Calculate text similarity (simple Jaccard similarity)
        """
        words1       = set(text1.lower().split())
        words2       = set(text2.lower().split())
        
        intersection = len(words1 & words2)
        union        = len(words1 | words2)
        
        return intersection / union if union > 0 else 0.0
    
    
    def get_category_distribution(self, clauses: List[ExtractedClause]) -> Dict[str, int]:
        """
        Get distribution of clause categories
        """
        distribution = defaultdict(int)
        
        for clause in clauses:
            distribution[clause.category] += 1
        
        log_info("Clause category distribution", distribution=dict(distribution))
        
        return dict(distribution)
    

    def get_high_risk_clauses(self, clauses: List[ExtractedClause]) -> List[ExtractedClause]:
        """
        Get clauses with risk indicators
        """
        risky = [c for c in clauses if c.risk_indicators]

        risky.sort(key = lambda x: len(x.risk_indicators), reverse = True)
        
        top_25_risky_clauses = risky[:25]

        return top_25_risky_clauses





class RiskClauseExtractor:
    """
    Risk-Focused Clause Extractor: Specifically for risk analysis using RiskRules framework for contract-type specific risk assessment
    
    This will be used for: Risk analysis, protection gap detection, contract-type specific assessment
    """
    def __init__(self, model_loader: ModelLoader, contract_type: ContractType):
        """
        Initialize risk-focused clause extractor
        
        Arguments:
        ----------
            model_loader  { ModelLoader }  : ModelLoader instance

            contract_type { ContractType } : Contract type for risk rule adjustments
        """
        self.model_loader             = model_loader
        self.contract_type            = contract_type
        self.risk_rules               = RiskRules()
        
        # Models (lazy loaded)
        self.legal_bert_model         = None
        self.legal_bert_tokenizer     = None
        self.embedding_model          = None
        self.device                   = None
        
        # Risk category embeddings
        self.risk_category_embeddings = dict()
        
        # Text processor
        self.text_processor           = TextProcessor(use_spacy = False)
        
        # Logger
        self.logger                   = ContractAnalyzerLogger.get_logger()
        
        # Contract-type specific weights
        self.category_weights         = self.risk_rules.get_adjusted_weights(contract_type)
         
        # Lazy load
        self._lazy_load()
    

    def _lazy_load(self):
        """
        Lazy load models for risk analysis
        """
        if self.legal_bert_model is None:
            try:
                log_info("Loading models for risk-focused clause extraction...")
                
                # Load Legal-BERT
                self.legal_bert_model, self.legal_bert_tokenizer = self.model_loader.load_legal_bert()
                self.device                                      = self.model_loader.device
                
                # Load embedding model
                self.embedding_model                             = self.model_loader.load_embedding_model()
                
                # Prepare risk category embeddings
                self._prepare_risk_category_embeddings()
                
                log_info("Risk clause extractor models loaded successfully")
                
            except Exception as e:
                log_error(e, context = {"component": "RiskClauseExtractor", "operation": "model_loading"})
                raise

    
    def _prepare_risk_category_embeddings(self):
        """
        Prepare embeddings for risk categories using RiskRules framework
        """
        log_info("Preparing risk category embeddings...")
        
        # Create representative texts for each risk category
        risk_category_texts = {'restrictive_covenants' : "Non-compete non-solicitation restrictive covenants competition limitations duration geographic scope industry restrictions",
                               'termination_rights'    : "Termination notice period severance for cause without cause immediate termination at-will employment end of agreement",
                               'penalties_liability'   : "Penalties liquidated damages liability limitations unlimited liability consequential damages indemnification hold harmless",
                               'compensation_benefits' : "Compensation salary wages benefits bonus commission equity stock options retirement health insurance paid time off",
                               'intellectual_property' : "Intellectual property IP ownership copyright patent trademark trade secrets work product inventions proprietary rights",
                               'confidentiality'       : "Confidentiality non-disclosure proprietary information trade secrets protection secrecy confidential obligations",
                               'liability_indemnity'   : "Liability indemnification hold harmless defense costs claims losses damages responsibility accountability",
                               'governing_law'         : "Governing law jurisdiction venue dispute resolution arbitration mediation legal forum applicable law",
                               'payment_terms'         : "Payment terms due date invoice billing net 30 late payment interest fees compensation remuneration",
                               'warranties'            : "Warranties representations guarantees disclaimers merchantability fitness for purpose product quality service standards",
                               'dispute_resolution'    : "Dispute resolution arbitration mediation litigation legal proceedings costs attorneys fees jurisdiction",
                               'assignment_change'     : "Assignment transfer change control amendment modification consent approval successor parties",
                               'insurance'             : "Insurance coverage liability insurance professional indemnity proof of insurance additional insured policy requirements",
                               'force_majeure'         : "Force majeure act of god unforeseen circumstances beyond control natural disasters performance suspension"
                              }
        
        for category, text in risk_category_texts.items():
            embedding                               = self._get_legal_bert_embedding(text)
            self.risk_category_embeddings[category] = embedding
        
        log_info(f"Prepared risk embeddings for {len(self.risk_category_embeddings)} categories")
    

    def _get_legal_bert_embedding(self, text: str) -> np.ndarray:
        """
        Get Legal-BERT embedding for risk analysis
        """
        inputs = self.legal_bert_tokenizer(text,
                                           return_tensors = "pt",
                                           padding        = True,
                                           truncation     = True,
                                           max_length     = 512,
                                          ).to(self.device)
        
        with torch.no_grad():
            outputs       = self.legal_bert_model(**inputs)
            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()[0]
        
        return cls_embedding
    

    @ContractAnalyzerLogger.log_execution_time("extract_risk_clauses")
    def extract_risk_clauses(self, contract_text: str, max_clauses: int = 20) -> List[ExtractedClause]:
        """
        Extract clauses specifically for risk analysis using RiskRules framework
        
        Process:
        1. Focus on high-weight categories for this contract type
        2. Use risk patterns from RiskRules
        3. Calculate risk scores based on RiskRules factors
        4. Prioritize clauses with high risk indicators
        
        Arguments:
        ----------
            contract_text { str } : Full contract text

            max_clauses   { int } : Maximum clauses to return
        
        Returns:
        --------
                 { list }         : Risk-focused clauses with calculated risk scores
        """
        log_info("Starting risk-focused clause extraction",
                 contract_type = self.contract_type.value,
                 max_clauses   = max_clauses,
                )
        
        # Use comprehensive extractor as base
        comprehensive_extractor = ComprehensiveClauseExtractor(self.model_loader)
        all_clauses             = comprehensive_extractor.extract_clauses(contract_text = contract_text, 
                                                                          max_clauses   = 50,
                                                                         )
        
        # Re-classify using risk framework
        risk_clauses            = self._reclassify_with_risk_framework(clauses = all_clauses)
        
        # Calculate risk scores
        risk_clauses            = self._calculate_risk_scores(clauses = risk_clauses)
        
        # Prioritize by risk score and contract-type relevance
        prioritized             = self._prioritize_risk_clauses(clauses     = risk_clauses, 
                                                                max_clauses = max_clauses,
                                                               )
        
        log_info(f"Extracted {len(prioritized)} risk-focused clauses")
        
        return prioritized
    

    def _reclassify_with_risk_framework(self, clauses: List[ExtractedClause]) -> List[ExtractedClause]:
        """
        Re-classify clauses using RiskRules categories and weights
        """
        risk_classified = list()
        
        for clause in clauses:
            # Map to risk categories and calculate relevance
            risk_category, risk_confidence = self._classify_with_risk_categories(text = clause.text)
            
            # Update clause with risk classification
            clause.category                = risk_category
            clause.confidence              = risk_confidence
            
            risk_classified.append(clause)
        
        return risk_classified
    

    def _classify_with_risk_categories(self, text: str) -> Tuple[str, float]:
        """
        Classify text using RiskRules categories with contract-type weights
        """
        text_lower     = text.lower()
        
        # Keyword matching with risk categories
        keyword_scores = dict()
        
        for risk_category in self.risk_rules.CATEGORY_WEIGHTS.keys():
            # Get keywords from risk rules patterns
            keywords                      = self._get_keywords_for_risk_category(risk_category = risk_category)
            
            keyword_count                 = sum(1 for kw in keywords if kw in text_lower)
            base_score                    = (keyword_count / max(len(keywords), 1)) * 100
            
            # Apply contract-type specific weight
            weight                        = self.category_weights.get(risk_category, 1.0)
            keyword_scores[risk_category] = base_score * weight
        
        # Legal-BERT similarity with risk categories
        semantic_scores = dict()
        text_embedding  = self._get_legal_bert_embedding(text = text)
        text_tensor     = torch.tensor(text_embedding).unsqueeze(0)
        
        for risk_category, category_embedding in self.risk_category_embeddings.items():
            cat_tensor                     = torch.tensor(category_embedding).unsqueeze(0)
            similarity                     = torch.nn.functional.cosine_similarity(text_tensor, cat_tensor).item()
            semantic_scores[risk_category] = similarity * 100  # Scale to 0-100
        
        # Combine scores (60% semantic, 40% keyword)
        combined_scores = dict()
        
        for risk_category in self.risk_rules.CATEGORY_WEIGHTS.keys():
            combined                       = (semantic_scores.get(risk_category, 0) * 0.6 + keyword_scores.get(risk_category, 0) * 0.4)
            combined_scores[risk_category] = combined
        
        # Get best category
        best_category = max(combined_scores, key = combined_scores.get)

        # Normalize to 0-1
        confidence    = min(combined_scores[best_category] / 100, 1.0) 
        
        return best_category, confidence
    

    def _get_keywords_for_risk_category(self, risk_category: str) -> List[str]:
        """
        Get relevant keywords for a risk category from RiskRules patterns
        """
        # Map risk categories to relevant keywords from RiskRules
        keyword_map = {'restrictive_covenants' : ['non-compete', 'non-solicit', 'restrictive', 'covenant', 'competition', 'geographic', 'duration'],
                       'termination_rights'    : ['termination', 'notice', 'severance', 'dismissal', 'resignation', 'for cause', 'without cause'],
                       'penalties_liability'   : ['penalty', 'liquidated damages', 'liability', 'indemnification', 'hold harmless', 'damages'],
                       'compensation_benefits' : ['compensation', 'salary', 'benefits', 'bonus', 'commission', 'equity', 'insurance'],
                       'intellectual_property' : ['intellectual property', 'ip', 'copyright', 'patent', 'trademark', 'inventions'],
                       'confidentiality'       : ['confidential', 'proprietary', 'trade secret', 'non-disclosure'],
                       'liability_indemnity'   : ['liability', 'indemnification', 'hold harmless', 'defend', 'claims'],
                       'governing_law'         : ['governing law', 'jurisdiction', 'venue', 'dispute resolution'],
                       'payment_terms'         : ['payment', 'due', 'invoice', 'net 30', 'late payment'],
                       'warranties'            : ['warranty', 'representation', 'guarantee', 'disclaimer'],
                       'dispute_resolution'    : ['arbitration', 'mediation', 'dispute', 'litigation'],
                       'assignment_change'     : ['assignment', 'transfer', 'amendment', 'modification'],
                       'insurance'             : ['insurance', 'coverage', 'policy', 'insured'],
                       'force_majeure'         : ['force majeure', 'act of god', 'beyond control'],
                      }
        
        return keyword_map.get(risk_category, [])
    

    def _calculate_risk_scores(self, clauses: List[ExtractedClause]) -> List[ExtractedClause]:
        """
        Calculate risk scores for clauses based on RiskRules factors
        """
        for clause in clauses:
            risk_score        = self._calculate_single_clause_risk(clause = clause)
            clause.risk_score = risk_score
        
        return clauses
    

    def _calculate_single_clause_risk(self, clause: ExtractedClause) -> float:
        """
        Calculate risk score using RiskRules framework
        """
        base_score      = 0.0
        text_lower      = clause.text.lower()

        # Base risk from category weight (adjusted for contract type)
        category_weight = self.category_weights.get(clause.category, 1.0)
        base_score     += category_weight  

        # Add risk from CLAUSE_RISK_FACTORS (red flags)
        factor_config   = self.risk_rules.CLAUSE_RISK_FACTORS.get(clause.category)
        
        if factor_config:
            for red_flag, adjustment in factor_config["red_flags"].items():
                if red_flag in text_lower:
                    base_score += adjustment

        # Add risk from RISKY_PATTERNS (with actual scores)
        for pattern, score, description in self.risk_rules.RISKY_PATTERNS:
            if re.search(pattern, text_lower):
                base_score += score

        # Add risk from CRITICAL_KEYWORDS
        for keyword, risk_score in self.risk_rules.CRITICAL_KEYWORDS.items():
            if re.search(rf'\b{re.escape(keyword)}\b', text_lower):
                base_score += risk_score

        # Cap final score at 100
        return min(max(base_score, 0), 100)
    

    def _extract_risk_indicators(self, text: str) -> List[str]:
        """
        Extract risk indicators using RiskRules patterns
        """
        text_lower = text.lower()
        indicators = list()
        
        # Check critical risk patterns
        for pattern, score, description in self.risk_rules.RISKY_PATTERNS:
            if re.search(pattern, text_lower):
                indicators.append(description)
        
        # Check keyword risk indicators
        for indicator in self.risk_rules.CRITICAL_KEYWORDS.keys():
            if indicator in text_lower:
                indicators.append(indicator)
        
        for indicator in self.risk_rules.HIGH_RISK_KEYWORDS.keys():
            if indicator in text_lower:
                indicators.append(indicator)
        
        return indicators
    

    def _check_risk_patterns(self, text: str) -> float:
        """
        Check for high-risk patterns from RiskRules
        """
        text_lower   = text.lower()
        pattern_risk = 0.0
        
        # Check risky patterns
        for pattern, score, description in self.risk_rules.RISKY_PATTERNS:
            if re.search(pattern, text_lower):
                pattern_risk += score
        
        # Cap pattern risk
        return min(pattern_risk, 20)
    

    def _prioritize_risk_clauses(self, clauses: List[ExtractedClause], max_clauses: int) -> List[ExtractedClause]:
        """
        Prioritize clauses by risk score and contract-type relevance
        """
        # Sort by risk score (descending)
        clauses.sort(key = lambda x: x.risk_score, reverse = True)
        
        # Take top clauses
        return clauses[:max_clauses]
    

    def detect_missing_protections(self, extracted_clauses: List[ExtractedClause]) -> List[Dict]:
        """
        Detect missing critical protections based on contract type
        """
        missing   = list()
        checklist = self.risk_rules.PROTECTION_CHECKLIST
        
        for protection, config in checklist.items():
            if not self._has_protection(extracted_clauses, protection, config['categories']):
                missing.append({"protection"      : protection,
                                "importance"      : config['importance'],
                                "risk_if_missing" : config['risk_if_missing'],
                                "categories"      : config['categories'],
                              })
        
        log_info(f"Detected {len(missing)} missing protections")
        return missing
    

    def _has_protection(self, clauses: List[ExtractedClause], protection: str, categories: List[str]) -> bool:
        """
        Check if protection exists in extracted clauses
        """
        protection_patterns = {'for_cause_definition'     : ['for cause', 'cause defined', 'termination for cause'],
                               'severance_provision'      : ['severance', 'severance pay', 'termination benefits'],
                               'mutual_indemnification'   : ['mutual indemnification', 'both parties indemnify'],
                               'liability_cap'            : ['liability cap', 'limited liability', 'maximum liability'],
                               'prior_ip_exclusion'       : ['prior inventions', 'pre-existing ip', 'prior intellectual property'],
                               'confidentiality_duration' : ['confidentiality period', 'duration of confidentiality'],
                               'dispute_resolution'       : ['dispute resolution', 'arbitration', 'mediation'],
                               'change_control_process'   : ['change control', 'amendment process', 'modification procedure'],
                               'insurance_requirements'   : ['insurance requirements', 'maintain insurance'],
                               'force_majeure'            : ['force majeure', 'act of god'],
                              }
        
        patterns = protection_patterns.get(protection, [])
        
        for clause in clauses:
            if clause.category in categories:
                text_lower = clause.text.lower()
                if any(pattern in text_lower for pattern in patterns):
                    return True
        
        return False


###################################
# services/term_analyzer.py
###################################
# DEPENDENCIES
import re
import sys
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional
from collections import Counter

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import RiskRules
from config.risk_rules import ContractType
from utils.logger import ContractAnalyzerLogger
from services.data_models import ExtractedClause
from services.data_models import UnfavorableTerm


class TermAnalyzer:
    """
    Detect unfavorable and one-sided terms in contracts using RiskRules framework and integrated with comprehensive risk analysis system
    """
    def __init__(self, contract_type: ContractType = ContractType.GENERAL):
        """
        Initialize term analyzer with contract-type specific risk rules
        
        Arguments:
        ----------
            contract_type { ContractType } : Contract type for risk rule adjustments
        """
        self.contract_type    = contract_type
        self.risk_rules       = RiskRules()
        self.logger           = ContractAnalyzerLogger.get_logger()
        
        # Contract-type specific weights
        self.category_weights = self.risk_rules.get_adjusted_weights(contract_type)
        
        log_info("TermAnalyzer initialized", 
                 contract_type    = contract_type.value,
                 category_weights = self.category_weights,
                )

    
    def _map_to_risk_category(self, clause_category: str) -> str:
        """
        Map clause category to risk category for proper risk scoring for ensureing unfavorable terms are correctly attributed to risk categories
        for score calculation
        """
        # Clause categories  Risk categories
        mapping                          = {"non_compete"           : "restrictive_covenants",
                                            "confidentiality"       : "restrictive_covenants",
                                            "termination"           : "termination_rights",
                                            "indemnification"       : "liability_indemnity",
                                            "liability"             : "penalties_liability",
                                            "compensation"          : "compensation_benefits",
                                            "intellectual_property" : "intellectual_property",
                                            "warranty"              : "warranties",
                                            "dispute_resolution"    : "dispute_resolution",
                                            "assignment"            : "assignment_change",
                                            "amendment"             : "assignment_change",
                                            "insurance"             : "insurance",
                                            "force_majeure"         : "force_majeure",
                                            "general"               : "general",
                                            "payment"               : "payment_terms",
                                            "governing_law"         : "governing_law",
                                           }

        risk_category_by_clause_category = mapping.get(clause_category, clause_category)
        
        return risk_category_by_clause_category
    

    @ContractAnalyzerLogger.log_execution_time("analyze_unfavorable_terms")
    def analyze_unfavorable_terms(self, contract_text: str, clauses: List[ExtractedClause], contract_type: Optional[ContractType] = None) -> List[UnfavorableTerm]:
        """
        Detect all unfavorable terms using RiskRules framework
        
        Arguments:
        ----------
            contract_text { str }          : Full contract text
            
            clauses       { list }         : Extracted clauses
            
            contract_type { ContractType } : Override contract type
        
        Returns:
        --------
                      { list }             : List of UnfavorableTerm objects
        """
        # Update contract type if provided
        if contract_type:
            self.contract_type    = contract_type
            self.category_weights = self.risk_rules.get_adjusted_weights(contract_type)
        
        log_info("Starting unfavorable terms analysis",
                 text_length    = len(contract_text),
                 num_clauses    = len(clauses),
                 contract_type  = self.contract_type.value,
                )
        
        unfavorable_terms = list()
        
        # Clause-level analysis using RiskRules patterns
        for clause in clauses:
            terms = self._analyze_clause_with_risk_rules(clause = clause)
            unfavorable_terms.extend(terms)
        
        # Cross-clause analysis for systemic issues
        cross_clause_terms = self._analyze_cross_clause_issues(text    = contract_text, 
                                                               clauses = clauses,
                                                              )
        unfavorable_terms.extend(cross_clause_terms)
        
        # PHASE 3: Missing protections analysis
        missing_protections = self._analyze_missing_protections(clauses = clauses)
        unfavorable_terms.extend(missing_protections)
        
        # PHASE 4: Industry benchmark analysis
        benchmark_issues = self._analyze_against_benchmarks(clauses = clauses)
        unfavorable_terms.extend(benchmark_issues)
        
        # Deduplicate and prioritize by risk
        final_terms = self._deduplicate_and_prioritize(terms = unfavorable_terms)
        
        log_info("Unfavorable terms analysis complete",
                 total_found = len(final_terms),
                 critical    = sum(1 for t in final_terms if (t.severity == "critical")),
                 high        = sum(1 for t in final_terms if (t.severity == "high")))
        
        return final_terms
    

    def _analyze_clause_with_risk_rules(self, clause: ExtractedClause) -> List[UnfavorableTerm]:
        """
        Analyze clause using comprehensive RiskRules framework
        """
        terms      = list()
        text_lower = clause.text.lower()
        
        # Map clause category to risk category for consistency
        risk_category = self._map_to_risk_category(clause_category = clause.category)
        
        # Risky Patterns Analysis from RiskRules
        for pattern, risk_score, description in self.risk_rules.RISKY_PATTERNS:
            matches = re.finditer(pattern, text_lower, re.IGNORECASE)
            
            for match in matches:
                severity = self._score_to_severity(risk_score)
                
                terms.append(UnfavorableTerm(term             = description,
                                             category         = risk_category,
                                             severity         = severity,
                                             explanation      = self._generate_pattern_explanation(description, match.group()),
                                             risk_score       = risk_score,
                                             clause_reference = clause.reference,
                                             suggested_fix    = self._generate_pattern_fix(description, clause.category),
                                             contract_type    = self.contract_type.value,
                                             specific_text    = match.group(),
                                             legal_basis      = self._get_legal_basis(description),
                                            )
                            )
        
        # Critical Keyword Analysis from RiskRules
        for keyword, risk_score in self.risk_rules.CRITICAL_KEYWORDS.items():
            if re.search(rf'\b{re.escape(keyword)}\b', text_lower):
                severity = self._score_to_severity(risk_score)
                
                terms.append(UnfavorableTerm(term             = f"Critical Risk: {keyword.title()}",
                                             category         = risk_category,
                                             severity         = severity,
                                             explanation      = self._generate_keyword_explanation(keyword, clause.category),
                                             risk_score       = risk_score,
                                             clause_reference = clause.reference,
                                             suggested_fix    = self._generate_keyword_fix(keyword, clause.category),
                                             contract_type    = self.contract_type.value,
                                             specific_text    = keyword,
                                             legal_basis      = self._get_legal_basis(keyword),
                                            )
                            )
                                
        # High Risk Keyword Analysis 
        for keyword, risk_score in self.risk_rules.HIGH_RISK_KEYWORDS.items():
            if re.search(rf'\b{re.escape(keyword)}\b', text_lower):
                severity = self._score_to_severity(risk_score)
                
                terms.append(UnfavorableTerm(term             = f"High Risk: {keyword.title()}",
                                             category         = risk_category, 
                                             severity         = severity,
                                             explanation      = self._generate_keyword_explanation(keyword, clause.category),
                                             risk_score       = risk_score,
                                             clause_reference = clause.reference,
                                             suggested_fix    = self._generate_keyword_fix(keyword, clause.category),
                                             contract_type    = self.contract_type.value,
                                             specific_text    = keyword,
                                             legal_basis      = self._get_legal_basis(keyword),
                                            )
                            )
        
        # Clause-specific Risk Factors From RiskRules.CLAUSE_RISK_FACTORS
        clause_risk_analysis = self._analyze_clause_risk_factors(clause)
        terms.extend(clause_risk_analysis)
        
        return terms
    

    def _analyze_clause_risk_factors(self, clause: ExtractedClause) -> List[UnfavorableTerm]:
        """
        Analyze clause using CLAUSE_RISK_FACTORS from RiskRules
        """
        terms            = list()
        
        # Map clause categories to risk factors
        category_mapping = {'non_compete'           : 'restrictive_covenants',
                            'termination'           : 'termination_rights', 
                            'indemnification'       : 'liability_indemnity',
                            'compensation'          : 'compensation_benefits',
                            'intellectual_property' : 'intellectual_property',
                            'confidentiality'       : 'confidentiality',
                            'liability'             : 'penalties_liability',
                            'warranty'              : 'warranties',
                            'dispute_resolution'    : 'dispute_resolution',
                            'assignment'            : 'assignment_change',
                            'insurance'             : 'insurance',
                            'force_majeure'         : 'force_majeure',
                           }
        
        risk_factors_key = category_mapping.get(clause.category)
        if not risk_factors_key or risk_factors_key not in self.risk_rules.CLAUSE_RISK_FACTORS:
            return terms
        
        risk_factors = self.risk_rules.CLAUSE_RISK_FACTORS[risk_factors_key]
        text_lower   = clause.text.lower()
        
        # Map clause category to risk category for consistency
        risk_category = self._map_to_risk_category(clause_category = clause.category)

        # Check for red flags in this clause
        for red_flag, risk_adjustment in risk_factors["red_flags"].items():
            if (red_flag in text_lower):
                base_risk    = risk_factors["base_risk"]
                total_risk   = base_risk + risk_adjustment
                severity     = self._score_to_severity(total_risk)
                
                terms.append(UnfavorableTerm(term             = f"Risk Factor: {red_flag.replace('_', ' ').title()}",
                                             category         = risk_category,
                                             severity         = severity,
                                             explanation      = f"Base risk {base_risk} + {risk_adjustment} for '{red_flag}'. {self._get_risk_factor_explanation(risk_factors_key, red_flag)}",
                                             risk_score       = total_risk,
                                             clause_reference = clause.reference,
                                             suggested_fix    = self._get_risk_factor_fix(risk_factors_key, red_flag),
                                             contract_type    = self.contract_type.value,
                                             specific_text    = red_flag,
                                             legal_basis      = self._get_legal_basis(red_flag)
                                            )
                            )
        
        return terms
    

    def _analyze_cross_clause_issues(self, text: str, clauses: List[ExtractedClause]) -> List[UnfavorableTerm]:
        """
        Detect systemic issues spanning multiple clauses
        """
        terms = list()
        
        # Notice period imbalance (from your original but enhanced)
        notice_imbalance = self._check_notice_imbalance(clauses = clauses)
        if notice_imbalance:
            # Ensure the category used is a risk category
            notice_imbalance.category = self._map_to_risk_category(clause_category = "termination") 
            terms.append(notice_imbalance)
        
        # Missing reciprocal provisions
        missing_reciprocal = self._check_missing_reciprocal(text    = text, 
                                                            clauses = clauses,
                                                           )
        for item in missing_reciprocal:
            # Ensure the category used is a risk category
            item.category = self._map_to_risk_category(clause_category = "indemnification")
        terms.extend(missing_reciprocal)
        
        # Conflicting clauses
        conflicts = self._check_conflicting_clauses(clauses = clauses)
        for item in conflicts:
            # Ensure the category used is a risk category
            item.category = self._map_to_risk_category(clause_category = item.category) 
        terms.extend(conflicts)
        
        # One-sided discretionary powers
        one_sided_powers = self._check_one_sided_discretion(clauses = clauses)
        for item in one_sided_powers:
            # Ensure the category used is a risk category
            item.category = self._map_to_risk_category(clause_category = item.category)
        terms.extend(one_sided_powers)
        
        return terms
    

    def _analyze_missing_protections(self, clauses: List[ExtractedClause]) -> List[UnfavorableTerm]:
        """
        Analyze missing critical protections using PROTECTION_CHECKLIST
        """
        terms = list()
        
        for protection, config in self.risk_rules.PROTECTION_CHECKLIST.items():
            if not self._has_protection(clauses, protection, config['categories']):
                # For missing protections, map the first associated category to a risk category
                # This assumes config['categories'][0] is a clause category like "termination"
                risk_category = self._map_to_risk_category(clause_category = config['categories'][0]) if config['categories'] else "general"
                
                terms.append(UnfavorableTerm(term             = f"Missing Protection: {protection.replace('_', ' ').title()}",
                                             category         = risk_category,
                                             severity         = self._score_to_severity(config['risk_if_missing']),
                                             explanation      = f"Missing critical protection: {protection}. {self._get_missing_protection_explanation(protection)}",
                                             risk_score       = config['risk_if_missing'],
                                             suggested_fix    = self._get_missing_protection_fix(protection),
                                             contract_type    = self.contract_type.value,
                                             legal_basis      = f"Standard protection in {self.contract_type.value} contracts",
                                            )
                            )
        
        return terms
    

    def _analyze_against_benchmarks(self, clauses: List[ExtractedClause]) -> List[UnfavorableTerm]:
        """
        Compare terms against industry benchmarks
        """
        terms = list()
        
        for clause in clauses:
            benchmark_issues = self._check_benchmark_compliance(clause = clause)
            for item in benchmark_issues:
                # Ensure the category used is a risk category
                item.category = self._map_to_risk_category(clause_category = clause.category) 

            terms.extend(benchmark_issues)
        
        return terms
    

    def _check_notice_imbalance(self, clauses: List[ExtractedClause]) -> Optional[UnfavorableTerm]:
        """
        Enhanced notice period imbalance detection
        """
        term_clauses = [c for c in clauses if (c.category == "termination")]
        
        if not term_clauses:
            return None
        
        text             = " ".join([c.text for c in term_clauses])
        
        # Pattern matching for notice periods
        notice_patterns = [r'(\d+)\s*days?\s*notice',
                           r'notice\s*of\s*(\d+)\s*days',
                           r'(\d+)\s*days?\s*prior\s*notice',
                           r'written\s*notice\s*of\s*(\d+)\s*days',
                          ]
        
        all_periods     = list()

        for pattern in notice_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            all_periods.extend([int(m) for m in matches])
        
        if (len(all_periods) >= 2):
            min_period = min(all_periods)
            max_period = max(all_periods)
            ratio      = max_period / min_period
            
            if (ratio >= 2):
                severity      = "critical" if (ratio >= 3) else "high"
                risk_score    = 80 if (ratio >= 3) else 60
                
                # Use the risk category mapping for termination
                risk_category = self._map_to_risk_category(clause_category = "termination")
                
                return UnfavorableTerm(term             = "Imbalanced Notice Periods",
                                       category         = risk_category,
                                       severity         = severity,
                                       explanation      = f"Significant notice period imbalance: {max_period} days vs {min_period} days (ratio: {ratio:.1f}x). Creates unfair burden.",
                                       risk_score       = risk_score,
                                       clause_reference = term_clauses[0].reference,
                                       suggested_fix    = f"Equalize notice periods to reasonable duration (e.g., 30 days mutual notice).",
                                       contract_type    = self.contract_type.value,
                                       benchmark_info   = f"Industry standard: Mutual 30-day notice periods",
                                      )
        
        return None
    

    def _check_missing_reciprocal(self, text: str, clauses: List[ExtractedClause]) -> List[UnfavorableTerm]:
        """
        Enhanced reciprocal provision analysis
        """
        terms         = list()
        
        # Check indemnification reciprocity
        indem_clauses = [c for c in clauses if (c.category == "indemnification")]
        
        if indem_clauses:
            has_one_sided = any(re.search(r'(you|employee|consultant|contractor)\s+shall\s+indemnify', c.text, re.IGNORECASE) for c in indem_clauses)
            has_mutual    = any("mutual" in c.text.lower() or "both parties" in c.text.lower() or "each party" in c.text.lower() for c in indem_clauses)
            
            if has_one_sided and not has_mutual:
                # Use the risk category mapping for indemnification
                risk_category = self._map_to_risk_category(clause_category = "indemnification")
                
                terms.append(UnfavorableTerm(term             = "Non-Reciprocal Indemnification",
                                             category         = risk_category,
                                             severity         = "critical",
                                             explanation      = "One-sided indemnification creates unlimited liability exposure without reciprocal protection.",
                                             risk_score       = 85,
                                             clause_reference = indem_clauses[0].reference,
                                             suggested_fix    = "Change to mutual indemnification: 'Each party shall indemnify the other for losses arising from their respective breach or negligence.'",
                                             contract_type    = self.contract_type.value,
                                             legal_basis      = "Mutuality of obligation principle",
                                            )
                            )
        
        return terms
    

    def _check_conflicting_clauses(self, clauses: List[ExtractedClause]) -> List[UnfavorableTerm]:
        """
        Detect conflicting clauses
        """
        terms       = list()
        
        # Group clauses by category for conflict analysis
        by_category = dict()

        for clause in clauses:
            # Map the clause category to the risk category for grouping purposes
            risk_cat = self._map_to_risk_category(clause_category = clause.category)
            if risk_cat not in by_category:
                by_category[risk_cat] = []
            
            by_category[risk_cat].append(clause)
        
        # Check for conflicts within each category
        for risk_category, category_clauses in by_category.items():
            if (len(category_clauses) >= 2):
                for i, clause1 in enumerate(category_clauses):
                    for clause2 in category_clauses[i+1:]:
                        if (self._are_clauses_conflicting(clause1, clause2)):
                            terms.append(UnfavorableTerm(term             = f"Conflicting {risk_category.title()} Clauses",
                                                         category         = risk_category,
                                                         severity         = "high",
                                                         explanation      = f"Clauses {clause1.reference} and {clause2.reference} contain conflicting terms creating legal ambiguity.",
                                                         risk_score       = 70,
                                                         clause_reference = f"{clause1.reference}, {clause2.reference}",
                                                         suggested_fix    = "Consolidate into single consistent clause or clarify precedence.",
                                                         contract_type    = self.contract_type.value,
                                                        )
                                        )
        
        return terms
    

    def _check_one_sided_discretion(self, clauses: List[ExtractedClause]) -> List[UnfavorableTerm]:
        """
        Check for one-sided discretionary powers
        """
        terms = list()
        
        for clause in clauses:
            text_lower = clause.text.lower()
            
            # Look for one-sided discretionary language
            if re.search(r'(sole|absolute|unfettered|unilateral)\s+(discretion|right|authority)', text_lower):
                if not re.search(r'(mutual|both parties|reasonable)\s+(discretion|agreement)', text_lower):
                    # Use the risk category mapping for the clause's category
                    risk_category = self._map_to_risk_category(clause_category = clause.category)
                    
                    terms.append(UnfavorableTerm(term             = "One-Sided Discretionary Power",
                                                 category         = risk_category,
                                                 severity         = "high",
                                                 explanation      = "Gives one party unilateral decision-making power without accountability standards.",
                                                 risk_score       = 75,
                                                 clause_reference = clause.reference,
                                                 suggested_fix    = "Change to 'reasonable discretion' or require 'mutual agreement'.",
                                                 contract_type    = self.contract_type.value,
                                                 legal_basis      = "Doctrine of good faith and fair dealing",
                                                )
                                )
        
        return terms
    

    def _check_benchmark_compliance(self, clause: ExtractedClause) -> List[UnfavorableTerm]:
        """
        Check clause against industry benchmarks
        """
        terms = list()
        
        # Non-compete duration benchmark
        if (clause.category == "non_compete"):
            duration_match = re.search(r'(\d+)\s*(month|year)', clause.text.lower())
            
            if duration_match:
                duration           = int(duration_match.group(1))
                unit               = duration_match.group(2)
                
                # Convert to months for comparison
                total_months       = duration * (12 if (unit == "year") else 1)
                
                benchmarks         = self.risk_rules.INDUSTRY_BENCHMARKS.get('non_compete_duration', {})
                industry_benchmark = benchmarks.get(self.contract_type.value, benchmarks.get('general', {}))
                
                if industry_benchmark:
                    reasonable = industry_benchmark.get('reasonable', 12)
                    excessive  = industry_benchmark.get('excessive', 24)
                    
                    if (total_months > excessive):
                        # Use the risk category mapping for non_compete
                        risk_category = self._map_to_risk_category(clause_category = clause.category)
                        
                        terms.append(UnfavorableTerm(term             = "Excessive Non-Compete Duration",
                                                     category         = risk_category,
                                                     severity         = "critical",
                                                     explanation      = f"{duration} {unit} non-compete exceeds industry excessive threshold of {excessive} months.",
                                                     risk_score       = 90,
                                                     clause_reference = clause.reference,
                                                     suggested_fix    = f"Reduce to {reasonable} months maximum.",
                                                     contract_type    = self.contract_type.value,
                                                     benchmark_info   = f"Industry standard: {reasonable} months reasonable, {excessive} months excessive",
                                                    )
                                    )
        
        return terms
    

    def _has_protection(self, clauses: List[ExtractedClause], protection: str, categories: List[str]) -> bool:
        """
        Check if protection exists in clauses
        """
        protection_patterns = {'for_cause_definition'     : ['for cause', 'cause defined', 'termination for cause', 'just cause'],
                               'severance_provision'      : ['severance', 'severance pay', 'termination benefits', 'separation pay'],
                               'mutual_indemnification'   : ['mutual indemnification', 'both parties indemnify', 'each party shall indemnify'],
                               'liability_cap'            : ['liability cap', 'limited liability', 'maximum liability', 'cap on damages'],
                               'prior_ip_exclusion'       : ['prior inventions', 'pre-existing ip', 'prior intellectual property', 'background ip'],
                               'confidentiality_duration' : ['confidentiality period', 'duration of confidentiality', 'term of confidentiality'],
                               'dispute_resolution'       : ['dispute resolution', 'arbitration', 'mediation', 'alternative dispute resolution'],
                               'change_control_process'   : ['change control', 'amendment process', 'modification procedure', 'change order'],
                               'insurance_requirements'   : ['insurance requirements', 'maintain insurance', 'proof of insurance'],
                               'force_majeure'            : ['force majeure', 'act of god', 'unforeseeable circumstances'],
                              }
        
        patterns            = protection_patterns.get(protection, [])
        relevant_clauses    = [c for c in clauses if not categories or c.category in categories]
        
        for clause in relevant_clauses:
            text_lower = clause.text.lower()
            if any(pattern in text_lower for pattern in patterns):
                return True
        
        return False
    

    # HELPER METHODS FOR EXPLANATIONS AND FIXES
    def _score_to_severity(self, score: float) -> str:
        """
        Convert risk score to severity level
        """
        if (score >= 80):
            return "critical"

        elif (score >= 60):
            return "high" 

        elif (score >= 40):
            return "medium"

        else:
            return "low"
    

    def _generate_pattern_explanation(self, pattern_desc: str, matched_text: str) -> str:
        """
        Generate explanation for pattern matches
        """
        explanations = {"Long duration restrictive covenant"     : f"Overly long restrictive period found: '{matched_text}'. May unreasonably restrict future employment.",
                        "Overly broad geographic/industry scope" : f"Excessively broad scope: '{matched_text}'. Could prevent working in entire industries or regions.",
                        "Unequal notice periods"                 : f"Imbalanced notice requirements: '{matched_text}'. Creates unfair advantage for one party.",
                        "Unlimited liability exposure"           : f"Uncapped liability: '{matched_text}'. Exposes to potentially catastrophic financial risk.",
                       }

        return explanations.get(pattern_desc, f"Risk pattern detected: {pattern_desc}")
    

    def _generate_pattern_fix(self, pattern_desc: str, category: str) -> str:
        """
        Generate fix suggestions for patterns
        """
        fixes = {"Long duration restrictive covenant"     : "Limit to 6-12 months maximum with reasonable geographic scope.",
                 "Overly broad geographic/industry scope" : "Narrow to specific competitors and reasonable geographic area.",
                 "Unequal notice periods"                 : "Equalize notice periods for both parties (e.g., 30 days mutual notice).",
                 "Unlimited liability exposure"           : "Add mutual liability cap (e.g., fees paid in preceding 12 months).",
                }

        return fixes.get(pattern_desc, "Review and modify to reasonable industry standards.")
    

    def _generate_keyword_explanation(self, keyword: str, category: str) -> str:
        """
        Generate explanations for keyword risks
        """
        explanations = {"non-compete"         : "Restrictive covenant limiting future employment opportunities.",
                        "unlimited liability" : "No cap on financial exposure - potentially catastrophic risk.",
                        "sole discretion"     : "Unilateral decision-making power without accountability.",
                        "at-will"             : "Termination without cause or protection - high job insecurity."
                       }

        return explanations.get(keyword, f"High-risk term '{keyword}' detected in {category} clause.")
    

    def _generate_keyword_fix(self, keyword: str, category: str) -> str:
        """
        Generate fixes for keyword risks
        """
        fixes = {"non-compete"         : "Limit duration to 12 months maximum and narrow geographic scope.",
                 "unlimited liability" : "Add mutual liability cap based on contract value.",
                 "sole discretion"     : "Change to 'reasonable discretion' or require 'mutual agreement'.",
                 "at-will"             : "Add 'for cause' definition and reasonable notice period.",
                }

        return fixes.get(keyword, "Modify to reasonable industry standards.")
    

    def _get_legal_basis(self, issue: str) -> str:
        """
        Get legal basis for risk issue
        """
        legal_bases = {"non-compete"         : "Reasonableness standard for restrictive covenants",
                       "unlimited liability" : "Unconscionability doctrine",
                       "sole discretion"     : "Doctrine of good faith and fair dealing", 
                       "at-will"             : "Employment protection statutes",
                       "unequal notice"      : "Mutuality of obligation principle",
                      }

        return legal_bases.get(issue, "General contract law principles")
    

    def _get_risk_factor_explanation(self, risk_category: str, red_flag: str) -> str:
        """
        Get explanation for risk factor red flags
        """
        explanations = {"restrictive_covenants": {"entire industry" : "Prohibits working in entire industry, not just direct competitors",
                                                  "worldwide"       : "Geographic scope is unreasonably broad",
                                                 }
                       }

        return explanations.get(risk_category, {}).get(red_flag, "Increases risk exposure")
    

    def _get_risk_factor_fix(self, risk_category: str, red_flag: str) -> str:
        """
        Get fix for risk factor issues
        """
        fixes = {"restrictive_covenants": {"entire industry" : "Limit to direct competitors only",
                                           "worldwide"       : "Narrow to specific geographic regions",
                                          }
                }

        return fixes.get(risk_category, {}).get(red_flag, "Modify to reasonable standards")
    

    def _get_missing_protection_explanation(self, protection: str) -> str:
        """
        Get explanation for missing protections
        """
        explanations = {"liability_cap"          : "No limit on potential financial damages",
                        "mutual_indemnification" : "One-sided liability protection",
                        "prior_ip_exclusion"     : "Could claim ownership of your existing work",
                        }

        return explanations.get(protection, "Critical protection missing from contract")
    

    def _get_missing_protection_fix(self, protection: str) -> str:
        """
        Get fix for missing protections
        """
        fixes = {"liability_cap"          : "Add mutual liability cap clause",
                 "mutual_indemnification" : "Add reciprocal indemnification",
                 "prior_ip_exclusion"     : "Add prior IP exclusion clause",
                }

        return fixes.get(protection, "Add appropriate protection clause")
    

    def _are_clauses_conflicting(self, clause1: ExtractedClause, clause2: ExtractedClause) -> bool:
        """
        Conflict detection between clauses
        """
        # Extract key numbers and terms
        nums1 = set(re.findall(r'\b\d+\b', clause1.text))
        nums2 = set(re.findall(r'\b\d+\b', clause2.text))
        
        # If both have numbers but no overlap, potential conflict
        if nums1 and nums2 and not nums1.intersection(nums2):
            return True
        
        # Check for contradictory language
        contradictions = [("shall", "shall not"),
                          ("must", "may not"), 
                          ("required", "prohibited"),
                         ]
        
        for positive, negative in contradictions:
            if (positive in clause1.text.lower() and negative in clause2.text.lower()) or (positive in clause2.text.lower() and negative in clause1.text.lower()):
                return True
        
        return False
    

    def _deduplicate_and_prioritize(self, terms: List[UnfavorableTerm]) -> List[UnfavorableTerm]:
        """
        Remove duplicates and sort by risk score
        """
        seen         = set()
        unique_terms = list()
        
        for term in terms:
            # Create unique key based on term, category, and specific text
            key = (term.term, term.category, term.specific_text)
            
            if key not in seen:
                seen.add(key)
                unique_terms.append(term)
        
        # Sort by risk score (descending)
        unique_terms.sort(key = lambda t: t.risk_score, reverse = True)
        
        # Return top 25 most critical terms
        return unique_terms[:25]
    

    def get_severity_distribution(self, terms: List[UnfavorableTerm]) -> Dict[str, int]:
        """
        Get distribution by severity
        """
        distribution = {"critical" : 0, 
                        "high"     : 0, 
                        "medium"   : 0, 
                        "low"      : 0,
                       }
        
        for term in terms:
            distribution[term.severity] = distribution.get(term.severity, 0) + 1
        
        log_info("Unfavorable terms severity distribution", **distribution)
        
        return distribution
    

    def get_category_distribution(self, terms: List[UnfavorableTerm]) -> Dict[str, int]:
        """
        Get distribution by category
        """
        categories   = [t.category for t in terms]
        distribution = dict(Counter(categories))
        
        log_info("Unfavorable terms category distribution", **distribution)
        
        return distribution


####################################
# services/protection_checker.py
####################################
# DEPENDENCIES
import re
import sys
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import RiskRules
from config.risk_rules import ContractType
from utils.logger import ContractAnalyzerLogger
from services.data_models import ExtractedClause
from services.data_models import MissingProtection


class ProtectionChecker:
    """
    Check for missing critical protections in contracts using RiskRules framework
    """
    def __init__(self, contract_type: ContractType = ContractType.GENERAL):
        """
        Initialize protection checker with contract-type specific analysis

        Arguments:
        ----------
            contract_type { ContractType } : Contract type for protection prioritization
        """
        self.contract_type         = contract_type
        self.rules                 = RiskRules()
        self.logger                = ContractAnalyzerLogger.get_logger()

        # Contract-type specific protection priorities
        self.protection_priorities = self._get_contract_type_priorities()

        log_info("ProtectionChecker initialized",
                 contract_type    = self.contract_type.value,
                 protection_count = len(self.rules.PROTECTION_CHECKLIST),
                )


    def _get_contract_type_priorities(self) -> Dict[str, List[str]]:
        """
        Get protection priorities by contract type
        """
        priorities = {ContractType.EMPLOYMENT.value : ['for_cause_definition', 'severance_provision', 'prior_ip_exclusion', 'confidentiality_duration'],
                      ContractType.SOFTWARE.value   : ['liability_cap', 'prior_ip_exclusion', 'mutual_indemnification', 'dispute_resolution'],
                      ContractType.CONSULTING.value : ['liability_cap', 'mutual_indemnification', 'payment_terms', 'change_control_process'],
                      ContractType.NDA.value        : ['confidentiality_duration', 'prior_ip_exclusion', 'dispute_resolution'],
                      ContractType.LEASE.value      : ['dispute_resolution', 'change_control_process', 'insurance_requirements'],
                      ContractType.PURCHASE.value   : ['liability_cap', 'warranty_protection', 'dispute_resolution'],
                      ContractType.GENERAL.value    : ['liability_cap', 'mutual_indemnification', 'dispute_resolution'],
                     }

        return priorities.get(self.contract_type.value, [])


    @ContractAnalyzerLogger.log_execution_time("check_missing_protections")
    def check_missing_protections(self, contract_text: str, clauses: List[ExtractedClause], contract_type: Optional[ContractType] = None) -> List[MissingProtection]:
        """
        Identify all missing protections using comprehensive RiskRules framework

        Arguments:
        ----------
            contract_text { str }          : Full contract text

            clauses       { list }         : Extracted clauses

            contract_type { ContractType } : Override contract type

        Returns:
        --------
                      { list }             : List of MissingProtection objects
        """

        # Update contract type if provided
        if contract_type:
            self.contract_type         = contract_type
            self.protection_priorities = self._get_contract_type_priorities()

        log_info("Starting missing protections analysis",
                 text_length   = len(contract_text),
                 num_clauses   = len(clauses),
                 contract_type = self.contract_type.value,
                )

        missing    = list()
        text_lower = contract_text.lower()

        # Check each protection in RiskRules PROTECTION_CHECKLIST
        for protection_id, config in self.rules.PROTECTION_CHECKLIST.items():
            is_present, found_in_clauses = self._check_protection_comprehensive(protection_id = protection_id,
                                                                                text_lower    = text_lower,
                                                                                clauses       = clauses,
                                                                               )

            if not is_present:
                missing_protection = self._create_missing_protection(protection_id    = protection_id,
                                                                     config           = config,
                                                                     found_in_clauses = found_in_clauses,
                                                                    )

                missing.append(missing_protection)

        # Prioritize by contract type and risk score
        final_missing = self._prioritize_missing_protections(missing_protections = missing)

        log_info("Missing protections analysis complete",
                 total_missing = len(final_missing),
                 critical      = sum(1 for p in final_missing if (p.importance == "critical")),
                 high          = sum(1 for p in final_missing if (p.importance == "high")),
                )

        return final_missing


    def _check_protection_comprehensive(self, protection_id: str, text_lower: str, clauses: List[ExtractedClause]) -> Tuple[bool, List[str]]:
        """
        Comprehensive protection detection using multiple methods

        Returns:
        --------
            { tuple } : (is_present, list of clause references where protection was found)
        """
        found_in_clauses    = list()

        # Enhanced protection patterns with regex for better matching
        protection_patterns = self._get_protection_patterns(protection_id = protection_id)

        # Check in full text with regex patterns
        for pattern in protection_patterns:
            if re.search(pattern, text_lower, re.IGNORECASE):
                return True, found_in_clauses

        # Check in relevant clauses with context awareness
        relevant_categories = self.rules.PROTECTION_CHECKLIST[protection_id]["categories"]
        relevant_clauses    = [c for c in clauses if c.category in relevant_categories]

        for clause in relevant_clauses:
            clause_text_lower = clause.text.lower()

            for pattern in protection_patterns:
                if re.search(pattern, clause_text_lower, re.IGNORECASE):
                    found_in_clauses.append(clause.reference)
                    
                    return True, found_in_clauses

        # Additional semantic checks for complex protections
        if self._check_protection_semantic(protection_id=protection_id, text_lower=text_lower, clauses=clauses):
            return True, found_in_clauses

        return False, found_in_clauses


    def _get_protection_patterns(self, protection_id: str) -> List[str]:
        """
        Get comprehensive regex patterns for each protection
        """
        patterns = {"for_cause_definition"     : [r'for\s+cause\s+means', r'cause\s+defined\s+as', r'grounds?\s+for\s+termination', r'termination\s+for\s+cause', r'just\s+cause\s+definition',],
                    "severance_provision"      : [r'severance\s+(pay|compensation|benefits)', r'separation\s+(pay|package|compensation)', r'termination\s+(pay|benefits)', r'upon\s+termination.*pay', r'severance.*equal\s+to',],
                    "mutual_indemnification"   : [r'mutual\s+indemnification', r'each\s+party\s+shall\s+indemnify', r'both\s+parties\s+indemnify', r'reciprocal\s+indemnification', r'indemnification.*mutual',],
                    "liability_cap"            : [r'liability.*cap', r'maximum\s+liability', r'limited\s+to.*\$?\d+', r'not\s+exceed.*\$?\d+', r'liability\s+shall\s+not\s+exceed', r'cap.*liability',],
                    "prior_ip_exclusion"       : [r'prior\s+intellectual\s+property', r'existing\s+ip', r'background\s+ip', r'pre-existing', r'prior\s+inventions', r'personal\s+projects',],
                    "confidentiality_duration" : [r'confidentiality.*period\s+of', r'for\s+\d+\s+years\s+from', r'confidentiality.*expire', r'confidentiality.*term', r'duration.*confidentiality',],
                    "dispute_resolution"       : [r'arbitration', r'mediation', r'dispute\s+resolution', r'resolution\s+of\s+disputes', r'alternative\s+dispute', r'adr',],
                    "change_control_process"   : [r'change\s+order', r'change\s+request', r'amendment.*writing', r'modification.*writing', r'written\s+consent', r'change\s+control',],
                    "insurance_requirements"   : [r'insurance\s+requirements', r'maintain\s+insurance', r'proof\s+of\s+insurance', r'coverage.*\$?\d+', r'liability\s+insurance',],
                    "force_majeure"            : [ r'force\s+majeure', r'act\s+of\s+god', r'unforeseeable', r'beyond\s+control', r'natural\s+disaster',],
                   }

        return patterns.get(protection_id, [rf'\b{protection_id}\b'])


    def _check_protection_semantic(self, protection_id: str, text_lower: str, clauses: List[ExtractedClause]) -> bool:
        """
        Semantic checks for complex protections that need context understanding
        """
        if (protection_id == "mutual_indemnification"):
            # Check if there's any indemnification that's not mutual
            has_indemnification = bool(re.search(r'indemnif', text_lower))
            has_mutual_language = bool(re.search(r'mutual|each party|both parties', text_lower))

            return has_indemnification and has_mutual_language

        elif (protection_id == "liability_cap"):
            # Check if there's liability language but no cap
            has_liability = bool(re.search(r'liability|liable', text_lower))
            has_cap       = bool(re.search(r'cap|limit|maximum|not exceed', text_lower))

            return has_liability and has_cap

        elif (protection_id == "prior_ip_exclusion"):
            # Check if there's IP assignment but no exclusion
            has_ip_assignment = bool(re.search(r'intellectual property|work product|inventions', text_lower))
            has_exclusion     = bool(re.search(r'prior|existing|background|exclude', text_lower))

            return has_ip_assignment and has_exclusion

        return False


    def _create_missing_protection(self, protection_id: str, config: Dict, found_in_clauses: List[str]) -> MissingProtection:
        """
        Create comprehensive MissingProtection object
        """
        # Use centralized map for display name
        protection_name = self.rules.get_protection_display_name(protection_id)

        return MissingProtection(protection_id      = protection_id, 
                                 protection         = protection_name,
                                 importance         = config["importance"],
                                 risk_score         = config["risk_if_missing"],
                                 explanation        = self._get_comprehensive_explanation(protection_id = protection_id),
                                 recommendation     = self._get_detailed_recommendation(protection_id = protection_id),
                                 categories         = config["categories"],
                                 contract_type      = self.contract_type.value,
                                 suggested_language = self._get_suggested_language(protection_id = protection_id),
                                 legal_basis        = self._get_legal_basis(protection_id = protection_id),
                                 affected_clauses   = found_in_clauses,
                                )


    def _get_comprehensive_explanation(self, protection_id: str) -> str:
        """
        Get detailed explanation for why this protection matters
        """
        explanations = {"for_cause_definition"     : ("Without a clear 'for cause' definition, termination grounds remain ambiguous and subject to interpretation abuse. "
                                                      "This creates significant job insecurity and potential for arbitrary termination without proper recourse."
                                                     ),
                        "severance_provision"      : ("Missing severance provision means zero financial protection if terminated without cause. "
                                                      "Industry standards provide 2-3 months salary to support transition and mitigate sudden income loss."
                                                     ),
                        "mutual_indemnification"   : ("One-sided indemnification creates asymmetric liability exposure. Mutual protection ensures both parties share "
                                                      "responsibility for their respective breaches, negligence, or misconduct."
                                                     ),
                        "liability_cap"            : ("Unlimited liability exposes you to catastrophic financial risk beyond reasonable business expectations. "
                                                      "Standard practice caps liability at fees paid or a reasonable multiple of contract value."
                                                     ),
                        "prior_ip_exclusion"       : ("Without prior IP exclusion, your existing intellectual property and personal projects could be claimed by the other party. "
                                                      "This protection preserves ownership of work created before and outside this engagement."
                                                     ),
                        "confidentiality_duration" : ("Indefinite confidentiality obligations unreasonably restrict future business activities indefinitely. "
                                                      "Industry standards limit confidentiality to 3-5 years post-termination for most information."
                                                     ),
                        "dispute_resolution"       : ("Without formal dispute resolution, conflicts escalate directly to costly litigation. Mediation and arbitration "
                                                      "provide efficient, cost-effective alternatives with specialized expertise."
                                                     ),
                        "change_control_process"   : ("Lack of change control enables scope creep and verbal modifications that create ambiguity. Formal processes "
                                                      "ensure all changes are documented, approved, and properly scoped."
                                                     ),
                        "insurance_requirements"   : ("Missing insurance requirements leave you exposed to uncovered liabilities. "
                                                      "Proper coverage transfers risk and provides financial protection for both parties."
                                                     ),
                        "force_majeure"            : ("Without force majeure protection, you remain liable for performance during unforeseeable events beyond control. "
                                                      "This clause provides reasonable relief during extraordinary circumstances."
                                                     ),
                       }

        return explanations.get(protection_id, "This protection is critical for balanced risk allocation and legal fairness.")


    def _get_detailed_recommendation(self, protection_id: str) -> str:
        """
        Get detailed recommendation for adding this protection
        """
        recommendations = {"for_cause_definition"     : ("Add clear 'For Cause' definition including: gross negligence, willful misconduct, material breach after "
                                                         "30-day cure period, conviction of felony, or fraud. Require written notice specifying grounds."
                                                        ),
                           "severance_provision"      : ("Include severance equal to 2-3 months base salary for termination without cause, payable within 30 days. "
                                                         "Add pro-rated bonus calculation and continuation of benefits during severance period."
                                                        ),
                           "mutual_indemnification"   : ("Replace one-sided language with: 'Each party shall indemnify, defend, and hold harmless the other party "
                                                         "from claims arising from their respective breach, negligence, or willful misconduct.'"
                                                        ),
                           "liability_cap"            : ("Add: 'Total liability of either party under this Agreement shall not exceed the greater of (a) fees paid "
                                                         "in the 12 months preceding the claim, or (b) $[reasonable amount]. Exclude liability for indirect damages.'"
                                                        ),
                           "prior_ip_exclusion"       : ("Include: 'Work Product excludes Employee's prior intellectual property, existing inventions, personal projects "
                                                         "unrelated to Company business, and open source contributions. Attach prior IP list as Exhibit A.'"
                                                        ),
                           "confidentiality_duration" : ("Specify: 'Confidentiality obligations shall survive termination for 3-5 years. Trade secrets protected "
                                                         "indefinitely but must be specifically identified. Publicly available information excluded.'"
                                                        ),
                           "dispute_resolution"       : ("Add: 'Disputes shall first be subject to 30-day good faith mediation. If unresolved, binding arbitration "
                                                         "under [rules] in [neutral location]. Each party bears own costs, arbitrator may award fees to prevailing party.'"
                                                        ),
                           "change_control_process"   : ("Include: 'All amendments require written change orders signed by both parties. Change orders must specify "
                                                         "scope, timeline, cost, and acceptance criteria. Verbal agreements are not binding.'"
                                                        ),
                           "insurance_requirements"   : ("Specify: 'Contractor shall maintain general liability insurance of $1M per occurrence, professional liability "
                                                         "insurance of $2M, and workers' compensation. Provide certificates of insurance before commencement.'"
                                                        ),
                           "force_majeure"            : ("Add: 'Neither party liable for failure to perform due to causes beyond reasonable control including acts of God, "
                                                         "war, strikes, or natural disasters. Performance suspended during event, resume when practicable.'"
                                                        ),
                          }

        return recommendations.get(protection_id, "Negotiate to include this standard protection for balanced risk allocation.")


    def _get_suggested_language(self, protection_id: str) -> str:
        """
        Get actual suggested clause language
        """
        language_library = {"for_cause_definition"     : ("\"For Cause\" means: (a) gross negligence or willful misconduct; (b) material breach of this Agreement after 30-day written notice and cure period; (c) conviction of a felony; or (d) fraud, dishonesty, or embezzlement."),
                            "severance_provision"      : ("Upon termination without cause, Company shall pay Employee severance equal to three months of base salary, payable within 30 days of termination. Employee shall also receive pro-rated annual bonus and continuation of health benefits during severance period."),
                            "mutual_indemnification"   : ("Each party shall indemnify, defend, and hold harmless the other party from and against any and all claims, damages, losses, and expenses arising from the indemnifying party's breach of this Agreement, negligence, or willful misconduct."),
                            "liability_cap"            : ("Notwithstanding anything to the contrary, the total liability of either party under this Agreement shall not exceed the greater of (a) the fees paid by Customer to Provider in the twelve months preceding the claim, or (b) $500,000. Neither party shall be liable for any indirect, special, incidental, or consequential damages."),
                            "prior_ip_exclusion"       : ("Work Product excludes any intellectual property, inventions, or creative works developed by Employee prior to this Agreement or developed outside the scope of employment without using Company resources. Employee has listed prior IP in Exhibit A. Background IP remains the property of its respective owner."),
                            "confidentiality_duration" : ("The obligations of confidentiality shall survive termination of this Agreement for a period of five years. Trade secrets shall be protected indefinitely. Confidential Information shall not include information that is or becomes publicly available through no fault of Receiving Party."),
                            "dispute_resolution"       : ("Any dispute arising under this Agreement shall first be submitted to mediation with a mutually acceptable mediator. If mediation fails after 30 days, either party may initiate binding arbitration under the rules of the American Arbitration Association. The prevailing party in any dispute shall be entitled to recover reasonable attorneys' fees and costs."),
                            "change_control_process"   : ("No amendment, modification, or waiver of any provision of this Agreement shall be effective unless in writing and signed by both parties. All change requests must be submitted in writing as Change Orders, specifying the changes, associated costs, timeline impacts, and acceptance criteria."),
                            "insurance_requirements"   : ("Contractor shall maintain at its own expense: (a) Commercial General Liability insurance with limits of $1,000,000 per occurrence; (b) Professional Liability insurance with limits of $2,000,000 per claim; and (c) Workers' Compensation insurance as required by law. Certificates of insurance shall be provided to Client upon request."),
                            "force_majeure"            : ("Neither party shall be liable for any failure or delay in performance under this Agreement due to causes beyond its reasonable control, including acts of God, war, terrorism, labor disputes, or governmental actions. The affected party shall notify the other party promptly and resume performance as soon as practicable."),
                           }

        return language_library.get(protection_id, "Standard protection clause appropriate for this contract type.")


    def _get_legal_basis(self, protection_id: str) -> str:
        """
        Get legal basis for why this protection is important
        """
        legal_bases = {"for_cause_definition"     : "Employment protection statutes and doctrine of good faith and fair dealing",
                       "severance_provision"      : "Industry standards and reasonable notice requirements",
                       "mutual_indemnification"   : "Principle of mutuality and unconscionability doctrine",
                       "liability_cap"            : "Commercial reasonableness and risk allocation principles",
                       "prior_ip_exclusion"       : "Intellectual property rights and prior ownership protection",
                       "confidentiality_duration" : "Reasonableness standard for restrictive covenants",
                       "dispute_resolution"       : "Efficient dispute resolution and access to justice",
                       "change_control_process"   : "Contract formation and modification requirements",
                       "insurance_requirements"   : "Risk management and liability transfer principles",
                       "force_majeure"            : "Impossibility of performance and commercial impracticability",
                      }

        return legal_bases.get(protection_id, "Standard contractual protection for balanced risk allocation")


    def _prioritize_missing_protections(self, missing_protections: List[MissingProtection]) -> List[MissingProtection]:
        """
        Prioritize missing protections by contract type and risk score
        """
        if not missing_protections:
            return []

        # Sort by risk score (descending)
        missing_protections.sort(key = lambda p: p.risk_score, reverse = True)

        # Boost priority for contract-type specific critical protections
        for protection in missing_protections:
            # Use the protection_id for the check
            if protection.protection_id in self.protection_priorities:
                # Boost for contract relevance
                protection.risk_score += 10

        # Re-sort with boosted scores
        missing_protections.sort(key = lambda p: p.risk_score, reverse = True)
        

        # Return top 15 most critical missing protections
        top_missing_protections = missing_protections[:15]
        
        return top_missing_protections


    def get_critical_missing(self, protections: List[MissingProtection]) -> List[MissingProtection]:
        """
        Filter to only critical missing protections
        """
        critical = [p for p in protections if (p.importance == "critical")]

        log_info(f"Found {len(critical)} critical missing protections")

        return critical


    def get_by_category(self, protections: List[MissingProtection], category: str) -> List[MissingProtection]:
        """
        Filter protections by category
        """
        filtered = [p for p in protections if category in p.categories]

        log_info(f"Found {len(filtered)} missing protections in category '{category}'")

        return filtered


    def get_importance_distribution(self, protections: List[MissingProtection]) -> Dict[str, int]:
        """
        Get distribution by importance level
        """
        distribution = {"critical" : 0, 
                        "high"     : 0, 
                        "medium"   : 0, 
                        "low"      : 0,
                       }

        for protection in protections:
            distribution[protection.importance] = distribution.get(protection.importance, 0) + 1

        log_info("Missing protections importance distribution", **distribution)

        return distribution


    def get_risk_score_summary(self, protections: List[MissingProtection]) -> Dict[str, float]:
        """
        Get risk score summary statistics
        """
        if not protections:
            return {"total_risk"   : 0, 
                    "average_risk" : 0, 
                    "max_risk"     : 0,
                   }

        scores       = [p.risk_score for p in protections]
        total_risk   = sum(scores)
        average_risk = total_risk / len(scores)
        max_risk     = max(scores)

        summary      = {"total_risk"   : round(total_risk, 2),
                        "average_risk" : round(average_risk, 2),
                        "max_risk"     : round(max_risk, 2),
                       }

        log_info("Missing protections risk score summary", **summary)

        return summary


###################################
# services/risk_analyzer.py
###################################
# DEPENDENCIES
import sys
from typing import Any
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional
from dataclasses import field
from collections import defaultdict

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import RiskRules
from config.risk_rules import ContractType
from services.data_models import RiskScore
from services.term_analyzer import TermAnalyzer
from utils.logger import ContractAnalyzerLogger
from services.data_models import ExtractedClause
from services.data_models import UnfavorableTerm
from services.data_models import MissingProtection
from services.data_models import RiskBreakdownItem
from services.protection_checker import ProtectionChecker
from services.clause_extractor import RiskClauseExtractor
from services.contract_classifier import ContractCategory
from services.contract_classifier import ContractClassifier
from services.clause_extractor import ComprehensiveClauseExtractor


class RiskAnalyzer:
    """
    Orchestrates all analysis components and calculates comprehensive risk scores
    
    Analysis Pipeline:
    1. Contract Classification
    2. Clause Extraction 
    3. Term Analysis 
    4. Protection Checking 
    5. Risk Scoring
    """
    def __init__(self, model_loader):
        """
        Initialize the risk analyzer with all required components
        
        Arguments:
        ----------
            model_loader : ModelLoader instance for accessing AI models
        """
        self.model_loader           = model_loader
        self.rules                  = RiskRules()
        self.logger                 = ContractAnalyzerLogger.get_logger()
        
        # Initialize all analysis components
        self.contract_classifier    = ContractClassifier(model_loader = model_loader)
        self.risk_clause_extractor  = None  # Will be initialized with contract type
        self.term_analyzer          = TermAnalyzer()
        self.protection_checker     = ProtectionChecker()
        
        log_info("RiskAnalyzer initialized - All components ready")
    

    @ContractAnalyzerLogger.log_execution_time("analyze_contract_risk")
    def analyze_contract_risk(self, contract_text: str) -> RiskScore:
        """
        Comprehensive contract risk analysis
        
        Arguments:
        ----------
            contract_text { str } : Full contract text
        
        Returns:
        --------
               { RiskScore }      : Complete risk assessment with 0-100 score and detailed breakdown
        """
        
        log_info("Starting Comprehensive Contract Risk Analysis...", text_length = len(contract_text))
        
        # Contract Classification
        contract_category   = self._classify_contract(contract_text = contract_text)
        log_info("Contract classified", contract_type = contract_category.category)
        
        # Clause Extraction: RiskClauseExtractor 
        clauses             = self._extract_clauses(contract_text     = contract_text, 
                                                    contract_category = contract_category,
                                                   )

        log_info("Clauses extracted", num_clauses = len(clauses))
        
        # Unfavourable Term Analysis 
        unfavorable_terms   = self._analyze_unfavorable_terms(contract_text     = contract_text,
                                                              clauses           = clauses, 
                                                              contract_category = contract_category,
                                                             )

        log_info("Unfavorable terms analyzed", num_unfavorable_terms = len(unfavorable_terms))
        
        # MISSING PROTECTIONS ANALYSIS
        missing_protections = self._analyze_missing_protections(contract_text     = contract_text, 
                                                                clauses           = clauses, 
                                                                contract_category = contract_category,
                                                               )

        log_info("Missing protections analyzed", num_missing_protections = len(missing_protections))
        
        # RISK SCORING & AGGREGATION
        risk_score          = self._calculate_comprehensive_risk(contract_category    = contract_category,
                                                                 clauses              = clauses,
                                                                 unfavorable_terms    = unfavorable_terms,
                                                                 missing_protections  = missing_protections,
                                                                 contract_text        = contract_text,
                                                                )
        
        log_info("Risk Analysis Complete", 
                 overall_score = risk_score.overall_score,
                 risk_level    = risk_score.risk_level,
                 contract_type = risk_score.contract_type,
                )
        
        return risk_score
    

    def _classify_contract(self, contract_text: str):
        """
        Classify contract type
        """
        log_info("Classifying contract type...")
        
        try:
            classification = self.contract_classifier.classify_contract(contract_text = contract_text)
            
            log_info("Contract classification successful",
                     category    = classification.category,
                     confidence  = classification.confidence,
                     subcategory = classification.subcategory)
            
            return classification
            
        except Exception as e:
            log_error(e, context = {"component": "RiskAnalyzer", "operation": "contract_classification"})
            
            # Fallback to general classification
            return ContractCategory(category          = "general",
                                    subcategory       = None,
                                    confidence        = 0.5,
                                    reasoning         = ["Classification failed, using general fallback"],
                                    detected_keywords = [],
                                   )
    

    def _extract_clauses(self, contract_text: str, contract_category) -> List:
        """
        Extract clauses from contract using RiskClauseExtractor
        """
        log_info("Extracting RISK-FOCUSED clauses from contract...")
        
        try:
            # Get contract type enum
            contract_type_enum         = self._get_contract_type_enum(category_str = contract_category.category)
            
            # Initialize RiskClauseExtractor (NOT ComprehensiveClauseExtractor)
            self.risk_clause_extractor = RiskClauseExtractor(model_loader  = self.model_loader,
                                                             contract_type = contract_type_enum,
                                                            )
            
            # Use RiskClauseExtractor which outputs risk categories
            clauses                    = self.risk_clause_extractor.extract_risk_clauses(contract_text = contract_text,
                                                                                         max_clauses   = 50,
                                                                                        )
            
            log_info("Risk-focused clause extraction successful",
                     total_clauses = len(clauses),
                     categories    = [c.category for c in clauses])
            
            return clauses
            
        except Exception as e:
            log_error(e, context = {"component": "RiskAnalyzer", "operation": "clause_extraction"})
            return []
    

    def _analyze_unfavorable_terms(self, contract_text: str, clauses: List, contract_category) -> List[UnfavorableTerm]:
        """
        Analyze for unfavorable terms (using risk categories from RiskClauseExtractor)
        """
        log_info("Analyzing unfavorable terms...")
        
        try:
            # Initialize term analyzer with contract type
            contract_type_enum = self._get_contract_type_enum(category_str = contract_category.category)
            self.term_analyzer = TermAnalyzer(contract_type = contract_type_enum)
            
            unfavorable_terms = self.term_analyzer.analyze_unfavorable_terms(contract_text = contract_text,
                                                                             clauses       = clauses)
            
            log_info("Unfavorable terms analysis successful",
                     total_terms = len(unfavorable_terms),
                     critical    = sum(1 for t in unfavorable_terms if (t.severity == "critical")))
            
            return unfavorable_terms
            
        except Exception as e:
            log_error(e, context = {"component": "RiskAnalyzer", "operation": "unfavorable_terms_analysis"})
            return []
    

    def _analyze_missing_protections(self, contract_text: str, clauses: List, contract_category) -> List[MissingProtection]:
        """
        Analyze for missing protections
        """
        log_info("Analyzing missing protections...")
        
        try:
            # Initialize protection checker with contract type
            contract_type_enum      = self._get_contract_type_enum(category_str = contract_category.category)
            self.protection_checker = ProtectionChecker(contract_type = contract_type_enum)
            
            missing_protections     = self.protection_checker.check_missing_protections(contract_text = contract_text,
                                                                                        clauses       = clauses)
            
            log_info("Missing protections analysis successful",
                     total_missing = len(missing_protections),
                     critical      = sum(1 for p in missing_protections if (p.importance == "critical")))
            
            return missing_protections
            
        except Exception as e:
            log_error(e, context = {"component": "RiskAnalyzer", "operation": "missing_protections_analysis"})
            return []
    

    def _calculate_comprehensive_risk(self, contract_category, clauses: List, unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection],
                                      contract_text: str) -> RiskScore:
        """
        Calculate comprehensive risk score using all analysis results
        """
        log_info("Calculating comprehensive risk score...")
        
        # Get contract type for risk rule adjustments
        contract_type_enum = self._get_contract_type_enum(category_str = contract_category.category)
        adjusted_weights   = self.rules.get_adjusted_weights(contract_type_enum)
        
        # Initialize scoring containers
        category_scores    = defaultdict(int)
        detailed_findings  = defaultdict(list)
        risk_factors       = list()
        
        # Calculate risk for each category
        for risk_category in adjusted_weights.keys():
            category_risk                    = self._calculate_category_risk(risk_category        = risk_category,
                                                                             contract_type        = contract_type_enum,
                                                                             clauses              = clauses,
                                                                             unfavorable_terms    = unfavorable_terms,
                                                                             missing_protections  = missing_protections,
                                                                             contract_text        = contract_text,
                                                                            )
            
            category_scores[risk_category]   = category_risk["score"]
            detailed_findings[risk_category] = category_risk["findings"]
            
            # Add to risk factors if high risk
            if (category_risk["score"] >= self.rules.RISK_THRESHOLDS["high"]):
                risk_factors.append(risk_category)
        
        # Calculate weighted overall score
        overall_score            = self._calculate_weighted_score(category_scores   = category_scores,
                                                                  adjusted_weights  = adjusted_weights)
        
        risk_level               = self._get_risk_level(score = overall_score)
        
        # Create risk breakdown
        risk_breakdown           = self._create_risk_breakdown(category_scores   = dict(category_scores),
                                                               detailed_findings = dict(detailed_findings))
        
        # Benchmark comparison
        benchmark_comparison     = self._compare_to_benchmarks(category_scores = category_scores,
                                                               contract_type   = contract_type_enum)
        
        # Prepare output data
        unfavorable_terms_dict   = [term.to_dict() for term in unfavorable_terms]
        missing_protections_dict = [protection.to_dict() for protection in missing_protections]
        
        return RiskScore(overall_score        = overall_score,
                         risk_level           = risk_level,
                         category_scores      = dict(category_scores),
                         risk_factors         = risk_factors,
                         detailed_findings    = dict(detailed_findings),
                         benchmark_comparison = benchmark_comparison,
                         risk_breakdown       = risk_breakdown,
                         contract_type        = contract_category.category,
                         unfavorable_terms    = unfavorable_terms_dict,
                         missing_protections  = missing_protections_dict,
                        )
    

    def _calculate_category_risk(self, risk_category: str, contract_type: ContractType, clauses: List, unfavorable_terms: List[UnfavorableTerm],
                                 missing_protections: List[MissingProtection], contract_text: str) -> Dict:
        """
        Calculate risk score for a specific category using all available data
        """
        base_score     = 0
        findings       = list()
        
        # Score from unfavorable terms in this category
        category_terms = [t for t in unfavorable_terms if (t.category == risk_category)]
        
        for term in category_terms:
            # Scale appropriately
            base_score += term.risk_score * 0.4  

            findings.append(f"{term.term}: {term.explanation}")
        
        # Score from missing protections affecting this category
        category_protections = [p for p in missing_protections if risk_category in p.categories]
        
        for protection in category_protections:
            base_score += protection.risk_score * 0.3
            
            findings.append(f"Missing: {protection.protection}")
        
        # Score from clauses in this category
        category_clauses = self._get_clauses_for_risk_category(clauses         = clauses,
                                                               risk_category   = risk_category,
                                                              )
        
        for clause in category_clauses:
            clause_risk = self._analyze_clause_risk(clause          = clause,
                                                    risk_category   = risk_category,
                                                    contract_type   = contract_type,
                                                   )

            base_score += clause_risk["score"] * 0.3

            findings.extend(clause_risk["findings"])
        
        # Apply contract-type specific adjustments
        category_weight = self.rules.CONTRACT_TYPE_ADJUSTMENTS.get(contract_type.value, {}).get(risk_category, 1.0)
        adjusted_score  = base_score * category_weight
        
        # Cap score between 0-100
        final_score     = max(0, min(100, int(adjusted_score)))
        
        # Top 25 findings
        return {"score"    : final_score,
                "findings" : findings[:25] 
               }
    

    def _get_clauses_for_risk_category(self, clauses: List, risk_category: str) -> List:
        """
        Map clauses to risk categories (now clauses are already in risk categories)
        """
        # clauses.category is already a risk category from RiskClauseExtractor
        clauses_for_risk_category = [c for c in clauses if (c.category == risk_category)]
        
        return clauses_for_risk_category


    def _analyze_clause_risk(self, clause, risk_category: str, contract_type: ContractType) -> Dict:
        """
        Analyze individual clause risk using RiskRules factors
        """
        risk_factors        = self.rules.CLAUSE_RISK_FACTORS
        
        # Map RISK category (e.g., "restrictive_covenants") to CLAUSE category (e.g., "non_compete")
        factor_mapping      = {"restrictive_covenants" : "non_compete",
                               "termination_rights"    : "termination", 
                               "liability_indemnity"   : "indemnification",
                               "compensation_benefits" : "compensation",
                               "intellectual_property" : "intellectual_property",
                               "confidentiality"       : "confidentiality",
                               "penalties_liability"   : "liability",
                               "warranties"            : "warranty",
                               "dispute_resolution"    : "dispute_resolution",
                               "assignment_change"     : "assignment", 
                               "insurance"             : "insurance",
                               "force_majeure"         : "force_majeure",
                              }
        
        clause_category_key = factor_mapping.get(risk_category)
        
        if not clause_category_key or clause_category_key not in risk_factors:
            return {"score": 0, "findings": []}
        
        factor_config = risk_factors[clause_category_key]
        base_risk     = factor_config.get("base_risk", 50)
        text_lower    = clause.text.lower()
        
        risk_score    = base_risk
        findings      = list()
        
        # Check red flags
        for red_flag, adjustment in factor_config["red_flags"].items():
            if red_flag in text_lower:
                risk_score += adjustment
                severity    = "increases" if adjustment > 0 else "decreases"

                findings.append(f"Red flag: '{red_flag}' ({severity} risk by {abs(adjustment)})")
        
        # Apply contract-type specific multiplier
        type_adjustments    = self.rules.CONTRACT_TYPE_ADJUSTMENTS.get(contract_type.value, {})
        category_multiplier = type_adjustments.get(risk_category, 1.0)
        
        risk_score         *= category_multiplier
        
        return {"score"    : max(0, min(100, risk_score)),
                "findings" : findings,
               }
    

    def _calculate_weighted_score(self, category_scores: Dict[str, int], adjusted_weights: Dict[str, float]) -> int:
        """
        Calculate weighted overall risk score
        """
        total_score  = 0
        total_weight = 0
        
        for category, score in category_scores.items():
            weight        = adjusted_weights.get(category, 1.0)
            total_score  += score * weight
            total_weight += weight
        
        return int(total_score / total_weight) if (total_weight > 0) else 50
    

    def _get_risk_level(self, score: int) -> str:
        """
        Convert numeric score to risk level
        """
        if (score >= self.rules.RISK_THRESHOLDS["critical"]):
            return "CRITICAL"
        
        elif (score >= self.rules.RISK_THRESHOLDS["high"]):
            return "HIGH"
        
        elif (score >= self.rules.RISK_THRESHOLDS["medium"]):
            return "MEDIUM"
        
        elif (score >= self.rules.RISK_THRESHOLDS["low"]):
            return "LOW"
        
        return "VERY LOW"
    

    def _create_risk_breakdown(self, category_scores: Dict[str, int], detailed_findings: Dict[str, List[str]]) -> List[RiskBreakdownItem]:
        """
        Create detailed risk breakdown for reporting
        """
        breakdown             = list()
        
        category_descriptions = self.rules.CATEGORY_DESCRIPTIONS
        
        for category, score in category_scores.items():
            if category in category_descriptions:
                # Get appropriate description based on score
                if (score >= 70):
                    risk_level = "high"
                
                elif (score >= 40):
                    risk_level = "medium"
                
                else:
                    risk_level = "low"
                
                summary = category_descriptions[category][risk_level]
            
            else:
                summary = f"Risk assessment for {category.replace('_', ' ')}"
            
            findings = detailed_findings.get(category, [])
            
            breakdown.append(RiskBreakdownItem(category = category.replace('_', ' ').title(),
                                               score    = score,
                                               summary  = summary,
                                               findings = findings[:25],  # Top 25 findings
                                              )
                            )
        
        # Sort by score (highest risk first)
        breakdown.sort(key = lambda x: x.score, reverse = True)
        
        return breakdown
    

    def _compare_to_benchmarks(self, category_scores: Dict[str, int], contract_type: ContractType) -> Dict[str, str]:
        """
        Compare risk scores to industry benchmarks
        """
        comparisons   = dict()
        
        # Overall risk comparison
        overall_score = sum(category_scores.values()) / len(category_scores) if category_scores else 50
        
        if (overall_score >= 70):
            comparisons["overall"] = " Significantly above market risk levels"
        
        elif (overall_score >= 55):
            comparisons["overall"] = " Above typical market risk levels"
        
        elif (overall_score >= 45):
            comparisons["overall"] = " Within typical market risk range"
        
        else:
            comparisons["overall"] = " Below market risk levels (favorable)"
        
        # Key category comparisons
        high_risk_categories = [cat for cat, score in category_scores.items() if score >= 60]
        
        if high_risk_categories:
            comparisons["high_risk_areas"] = f"High risk in: {', '.join(high_risk_categories)}"
        
        return comparisons
    

    def _get_contract_type_enum(self, category_str: str) -> ContractType:
        """
        Convert category string to ContractType enum
        """
        mapping = {"employment"  : ContractType.EMPLOYMENT,
                   "consulting"  : ContractType.CONSULTING,
                   "nda"         : ContractType.NDA,
                   "software"    : ContractType.SOFTWARE,
                   "service"     : ContractType.SERVICE,
                   "partnership" : ContractType.PARTNERSHIP,
                   "lease"       : ContractType.LEASE,
                   "purchase"    : ContractType.PURCHASE,
                   "general"     : ContractType.GENERAL,
                  }
        
        return mapping.get(category_str, ContractType.GENERAL)


#####################################
# services/llm_interpreter.py
#####################################
# DEPENDENCIES
import sys
import json
from typing import Any
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import RiskRules
from config.risk_rules import ContractType
from utils.logger import ContractAnalyzerLogger
from model_manager.llm_manager import LLMManager
from services.data_models import UnfavorableTerm
from model_manager.llm_manager import LLMProvider
from services.data_models import RiskInterpretation
from services.data_models import ClauseInterpretation
from services.clause_extractor import ExtractedClause
from services.protection_checker import MissingProtection


class LLMClauseInterpreter:
    """
    Uses LLM to generate plain-English explanations for legal clauses and integrated with RiskAnalyzer results and RiskRules framework
    """
    def __init__(self, llm_manager: LLMManager, default_provider: LLMProvider = LLMProvider.OLLAMA):
        """
        Initialize LLM interpreter
        
        Arguments:
        ----------
            llm_manager      { LLMManager }  : LLMManager instance
            default_provider { LLMProvider } : Default LLM provider to use
        """
        self.llm_manager      = llm_manager
        self.default_provider = default_provider
        self.risk_rules       = RiskRules()
        self.logger           = ContractAnalyzerLogger.get_logger()
        
        log_info("LLMClauseInterpreter initialized", default_provider = default_provider.value)
    

    # Interpret with full risk context
    @ContractAnalyzerLogger.log_execution_time("interpret_with_risk_context")
    def interpret_with_risk_context(self, clauses: List[ExtractedClause], unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection],
                                    contract_type: ContractType, overall_risk_score: int, max_clauses: int = 50, provider: Optional[LLMProvider] = None) -> RiskInterpretation:
        """
        Generate comprehensive risk interpretation with full context
        
        Arguments:
        ----------
            clauses              { list }         : Extracted clauses with risk scores
            
            unfavorable_terms    { list }         : Detected unfavorable terms
            
            missing_protections  { list }         : Missing critical protections
            
            contract_type        { ContractType } : Type of contract for context
            
            overall_risk_score   { int }          : Overall risk score (0-100)
            
            max_clauses          { int }          : Maximum clauses to interpret
            
            provider             { LLMProvider }  : LLM provider to use
        
        Returns:
        --------
                  { RiskInterpretation }          : Comprehensive RiskInterpretation with explanations
        """
        provider = provider or self.default_provider
        
        log_info("Starting comprehensive risk interpretation",
                 contract_type           = contract_type.value,
                 overall_risk_score      = overall_risk_score,
                 num_clauses             = len(clauses),
                 num_unfavorable_terms   = len(unfavorable_terms),
                 num_missing_protections = len(missing_protections),
                )
        
        # Interpret key clauses with risk context
        clause_interpretations = self.interpret_clauses(clauses     = clauses,
                                                        max_clauses = max_clauses,
                                                        provider    = provider,
                                                       )
                                                    
        # Generate overall risk explanation
        overall_explanation    = self._generate_overall_risk_explanation(overall_risk_score  = overall_risk_score,
                                                                         contract_type       = contract_type,
                                                                         unfavorable_terms   = unfavorable_terms,
                                                                         missing_protections = missing_protections,
                                                                         provider            = provider,
                                                                        )
                                                                    
        # Extract key concerns
        key_concerns           = self._extract_key_concerns(unfavorable_terms      = unfavorable_terms,
                                                            missing_protections    = missing_protections,
                                                            clause_interpretations = clause_interpretations,
                                                           )
        
        # Generate negotiation strategy
        negotiation_strategy   = self._generate_negotiation_strategy(contract_type       = contract_type,
                                                                     unfavorable_terms   = unfavorable_terms,
                                                                     missing_protections = missing_protections,
                                                                     overall_risk_score  = overall_risk_score,
                                                                     provider            = provider,
                                                                    )
        
        # Market comparison
        market_comparison      = self._generate_market_comparison(contract_type      = contract_type,
                                                                  overall_risk_score = overall_risk_score,
                                                                  provider           = provider,
                                                                 )
        
        interpretation         = RiskInterpretation(overall_risk_explanation = overall_explanation,
                                                    key_concerns             = key_concerns,
                                                    negotiation_strategy     = negotiation_strategy,
                                                    market_comparison        = market_comparison,
                                                    clause_interpretations   = clause_interpretations,
                                                   )
                                                
        log_info("Comprehensive risk interpretation complete")
        
        return interpretation


    @ContractAnalyzerLogger.log_execution_time("interpret_clauses")
    def interpret_clauses(self, clauses: List[ExtractedClause], max_clauses: int = 50, provider: Optional[LLMProvider] = None) -> List[ClauseInterpretation]:
        """
        Generate plain-English interpretations for multiple clauses
        
        Arguments:
        ----------
            clauses       { list }     : List of extracted clauses
           
            max_clauses    { int }     : Maximum number to interpret (for cost control)
           
            provider   { LLMProvider } : LLM provider to use (default: self.default_provider)
        
        Returns:
        --------
                   { list }            : List of ClauseInterpretation objects
        """
        provider = provider or self.default_provider
        
        log_info(f"Starting clause interpretation", num_clauses = min(len(clauses), max_clauses), provider = provider.value)
        
        # Prioritize clauses by risk indicators and confidence
        prioritized     = self._prioritize_clauses(clauses, max_clauses)
          
        interpretations = list()
        
        for clause in prioritized:
            try:
                interpretation = self._interpret_single_clause(clause, provider)
                interpretations.append(interpretation)

            except Exception as e:
                log_error(e, context = {"component": "LLMClauseInterpreter", "operation": "interpret_single_clause", "clause_reference": clause.reference})
                # Continue with other clauses even if one fails
                continue
        
        log_info(f"Clause interpretation complete", successful = len(interpretations), failed = len(prioritized) - len(interpretations))
        
        return interpretations

    
    def _prioritize_clauses(self, clauses: List[ExtractedClause], max_clauses: int) -> List[ExtractedClause]:
        """
        Prioritize clauses for interpretation (high-risk first)
        """
        # Scoring with risk_score
        scored = list()

        for clause in clauses:
            # Base score from original logic
            base_score       = (len(clause.risk_indicators) * 3 +   # Risk indicators
                                clause.confidence * 2 +             # Confidence
                                (1 if clause.category in ['non_compete', 'termination', 'indemnification'] else 0) * 2
                               )
            
            # Add risk_score if available (from RiskAnalyzer)
            risk_score_boost = getattr(clause, 'risk_score', 0) / 10
            total_score      = base_score + risk_score_boost
            
            scored.append((clause, total_score))
        
        # Sort by score (descending)
        scored.sort(key = lambda x: x[1], reverse = True)
        
        return [clause for clause, _ in scored[:max_clauses]]

    
    def _interpret_single_clause(self, clause: ExtractedClause, provider: LLMProvider) -> ClauseInterpretation:
        """
        Generate plain-English interpretation for a single clause
        """
        # Create enhanced prompt with risk context
        prompt             = self._create_interpretation_prompt(clause)
        
        # Call LLM with structured output
        schema_description = """
                                {
                                    "plain_english_summary": "string (1-2 sentence summary in simple terms)",
                                    "key_points": ["string", "string", ...] (3-5 key points),
                                    "potential_risks": ["string", "string", ...] (2-4 potential risks),
                                    "favorability": "string (one of: favorable, neutral, unfavorable)",
                                    "suggested_improvements": ["string", "string", ...] (2-3 improvement suggestions)
                                }
                             """
        
        try:
            result               = self.llm_manager.generate_structured_json(prompt             = prompt,
                                                                             schema_description = schema_description,
                                                                             provider           = provider,
                                                                             temperature        = 0.3,
                                                                             max_tokens         = 1200,
                                                                             fallback_providers = [LLMProvider.OPENAI, LLMProvider.ANTHROPIC],
                                                                            )
            
            # Calculate negotiation priority
            negotiation_priority = self._calculate_negotiation_priority(favorability    = result.get("favorability", "neutral"),
                                                                        risk_indicators = clause.risk_indicators,
                                                                        risk_score      = getattr(clause, 'risk_score', 0),
                                                                       )
            
            # Parse result
            interpretation       = ClauseInterpretation(clause_reference       = clause.reference,
                                                        original_text          = clause.text[:500] + "..." if len(clause.text) > 500 else clause.text,
                                                        plain_english_summary  = result.get("plain_english_summary", "Unable to generate summary"),
                                                        key_points             = result.get("key_points", []),
                                                        potential_risks        = result.get("potential_risks", []),
                                                        favorability           = result.get("favorability", "neutral"),
                                                        confidence_score       = 0.85,  # High confidence if LLM succeeded
                                                        risk_score             = getattr(clause, 'risk_score', 0),
                                                        negotiation_priority   = negotiation_priority,
                                                        suggested_improvements = result.get("suggested_improvements", []),
                                                       )
            
            log_info(f"Clause interpreted successfully",
                     clause_reference     = clause.reference,
                     favorability         = interpretation.favorability,
                     negotiation_priority = negotiation_priority,
                    )
            
            return interpretation
            
        except Exception as e:
            log_error(e, context = {"component": "LLMClauseInterpreter", "operation": "_interpret_single_clause", "clause_reference": clause.reference})
            
            # Enhanced fallback with risk context
            return self._fallback_interpretation(clause)
    

    def _create_interpretation_prompt(self, clause: ExtractedClause) -> str:
        """
        Create concise prompt for clause interpretation
        """
        risk_context = ""

        if clause.risk_indicators:
            risk_context = f"\nRisk Keywords: {', '.join(clause.risk_indicators[:3])}"
        
        risk_score_context = ""

        if hasattr(clause, 'risk_score'):
            if (clause.risk_score >= 70):
                risk_level = "CRITICAL RISK"

            elif (clause.risk_score >= 50):
                risk_level = "HIGH RISK"

            else:
                risk_level = "Moderate risk"
            
            risk_score_context = f"\nRisk Level: {risk_level} ({clause.risk_score}/100)"
        
        prompt = f"""
                     Explain this legal clause in plain English.

                     CLAUSE: {clause.reference} - {clause.category.replace('_', ' ').title()}{risk_score_context}{risk_context}

                     TEXT: "{clause.text}..."

                     Provide:
                     1. SUMMARY: 1-2 sentences explaining what this means
                     2. KEY_POINTS: 3 bullet points of what to know
                     3. POTENTIAL_RISKS: 2-3 specific risks or concerns
                     4. FAVORABILITY: "favorable", "neutral", or "unfavorable"
                     5. IMPROVEMENTS: 2 specific suggestions to fix this

                     Keep each section CONCISE. Total response should be ~150 words.

                     Return ONLY valid JSON:
                     {{
                        "plain_english_summary": "...",
                        "key_points": ["...", "...", "..."],
                        "potential_risks": ["...", "..."],
                        "favorability": "unfavorable",
                        "suggested_improvements": ["...", "..."]
                     }}
                  """
        
        return prompt
    

    def _calculate_negotiation_priority(self, favorability: str, risk_indicators: List[str], risk_score: float) -> str:
        """
        Calculate negotiation priority based on multiple factors
        """
        if (favorability == "unfavorable") and ((len(risk_indicators) >= 3) or (risk_score >= 70)):
            return "high"

        elif (favorability == "unfavorable") or ((len(risk_indicators) >= 2) or (risk_score >= 50)):
            return "medium"

        else:
            return "low"

    
    def _map_risk_score_to_level(self, risk_score: float) -> str:
        """
        Map numeric risk score to risk level string
        """
        if (risk_score >= 70):
            return "critical"

        elif (risk_score >= 50):
            return "high" 

        elif (risk_score >= 30):
            return "medium"
            
        else:
            return "low"
    

    def _fallback_interpretation(self, clause: ExtractedClause) -> ClauseInterpretation:
        """
        Fallback rule-based interpretation with risk context
        """
        category_summaries = {"compensation"          : "This clause defines payment terms, including salary, bonuses, and benefits.",
                              "termination"           : "This clause specifies conditions for ending the agreement, including notice periods and grounds for termination.",
                              "non_compete"           : "This clause restricts future employment opportunities with competitors.",
                              "confidentiality"       : "This clause requires protection of sensitive business information.",
                              "indemnification"       : "This clause defines financial responsibility for claims or losses.",
                              "intellectual_property" : "This clause determines ownership rights for work created.",
                              "liability"             : "This clause limits financial exposure for damages or breaches.",
                              "warranty"              : "This clause contains promises about quality or performance.",
                              "dispute_resolution"    : "This clause outlines processes for resolving disagreements.",
                             }
        
        summary            = category_summaries.get(clause.category, f"This {clause.category} clause defines specific rights and obligations.")
        
        key_points         = [f"Classified as {clause.category} clause",
                              f"Reference: {clause.reference}",
                              f"Extraction confidence: {clause.confidence:.2f}"
                             ]
        
        if clause.risk_indicators:
            key_points.append(f"Risk indicators: {', '.join(clause.risk_indicators[:3])}")
        
        potential_risks = clause.risk_indicators[:4] if clause.risk_indicators else ["Standard clause - review recommended"]
        
        # Favorability based on risk indicators and score
        risk_score = getattr(clause, 'risk_score', 0)
        
        if (len(clause.risk_indicators) >= 3) or (risk_score >= 70):
            favorability = "unfavorable"

        elif (len(clause.risk_indicators) >= 1) or (risk_score >= 40):
            favorability = "neutral"

        else:
            favorability = "favorable"
        
        negotiation_priority   = self._calculate_negotiation_priority(favorability    = favorability, 
                                                                      risk_indicators = clause.risk_indicators, 
                                                                      risk_score      = risk_score,
                                                                     )
        
        suggested_improvements = ["Review with legal counsel",
                                  "Compare with industry standards",
                                  "Consider impact on business operations"
                                 ]
        
        return ClauseInterpretation(clause_reference       = clause.reference,
                                    original_text          = clause.text[:500] + "..." if len(clause.text) > 500 else clause.text,
                                    plain_english_summary  = summary,
                                    key_points             = key_points,
                                    potential_risks        = potential_risks,
                                    favorability           = favorability,
                                    confidence_score       = 0.50,  # Medium confidence for fallback
                                    risk_score             = risk_score,
                                    negotiation_priority   = negotiation_priority,
                                    suggested_improvements = suggested_improvements,
                                   )
    

    def _generate_overall_risk_explanation(self, overall_risk_score: int, contract_type: ContractType, unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection], 
                                           provider: LLMProvider) -> str:
        """
        Generate concise overall risk explanation
        """
        # Handle both object and dictionary formats for unfavorable_terms
        critical_terms       = list()
        high_terms           = list()
        issues_summary       = list()
        critical_protections = list()
        
        for term in unfavorable_terms:
            severity = ""
            
            if isinstance(term, UnfavorableTerm):
                severity = term.severity
            
            elif isinstance(term, dict):
                severity = term.get('severity', '')
            
            else:
                severity = getattr(term, 'severity', '')
                
            if (severity == "critical"):
                critical_terms.append(term)
            
            elif (severity == "high"):
                high_terms.append(term)
        
        # Handle both object and dictionary formats for missing_protections
        for protection in missing_protections:
            importance = ""

            if isinstance(protection, MissingProtection):
                importance = protection.importance
            
            elif isinstance(protection, dict):
                importance = protection.get('importance', '')
            
            else:
                importance = getattr(protection, 'importance', '')
                
            if (importance == "critical"):
                critical_protections.append(protection)
        
        # Create issues summary
        if critical_terms:
            issues_summary.append(f"{len(critical_terms)} CRITICAL unfavorable terms")
        
        if high_terms:
            issues_summary.append(f"{len(high_terms)} HIGH-risk unfavorable terms")
        
        if critical_protections:
            issues_summary.append(f"{len(critical_protections)} CRITICAL missing protections")
        
        if not issues_summary:
            issues_summary = ["Multiple concerning provisions identified"]
        
        prompt = f"""
                   Risk Level: {overall_risk_score}/100 for {contract_type.value} contract

                   Top Issues:
                   {chr(10).join(issues_summary)}

                   Write ONE sentence (max 25 words) explaining what this risk score means for someone signing this contract.

                   Example: "This contract creates severe financial and legal exposure through unlimited liability and one-sided termination rights."

                   Your turn:
                """
                                        
        try:
            response = self.llm_manager.complete(prompt      = prompt,
                                                 provider    = provider,
                                                 temperature = 0.2,
                                                 max_tokens  = 100,
                                                ) 
            
            explanation = response.text.strip() if response.success else self._fallback_risk_explanation(overall_risk_score)
            
            # Ensure single sentence
            sentences = explanation.split('.')
            return sentences[0].strip() + '.' if sentences else explanation
            
        except Exception as e:
            log_error(e, context={"operation": "generate_overall_risk_explanation"})
            return self._fallback_risk_explanation(overall_risk_score)
    

    def _fallback_risk_explanation(self, risk_score: int) -> str:
        """
        Fallback risk explanation
        """
        if (risk_score >= 80):
            return "This contract presents very high risk with multiple critical issues that require immediate attention and significant negotiation."

        elif (risk_score >= 60):
            return "This contract has substantial risk factors that need careful review and important modifications before signing."

        elif (risk_score >= 40):
            return "This contract has moderate risk with some areas that should be reviewed and potentially improved."

        else:
            return "This contract appears to have reasonable risk levels, but professional review is still recommended."
    

    def _extract_key_concerns(self, unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection], clause_interpretations: List[ClauseInterpretation]) -> List[str]:
        """
        Extract key concerns from all analysis results
        """
        concerns       = list()
        
        # From unfavorable terms
        critical_terms = list()

        for term in unfavorable_terms:
            if isinstance(term, UnfavorableTerm):
                if (term.severity == "critical"):
                    critical_terms.append(term)
            
            elif isinstance(term, dict):
                if (term.get("severity") == "critical"):
                    critical_terms.append(term)
        
        # Top 10 critical terms
        for term in critical_terms[:10]:  
            term_name        = ""
            term_explanation = ""
            
            if isinstance(term, UnfavorableTerm):
                term_name        = term.term
                term_explanation = term.explanation
            
            elif isinstance(term, dict):
                term_name        = term.get('term', 'Unfavorable term')
                term_explanation = term.get('explanation', 'Standard risk identified')

            concerns.append(f"Critical: {term_name} - {term_explanation}")
        
        # From missing protections
        critical_protections = list()

        for protection in missing_protections:
            if isinstance(protection, MissingProtection):
                if (protection.importance == "critical"):
                    critical_protections.append(protection)
            
            elif isinstance(protection, dict):
                if (protection.get("importance") == "critical"):
                    critical_protections.append(protection)
        
        # Top 10 critical protections
        for protection in critical_protections[:10]:  
            protection_name = ""
            
            if isinstance(protection, MissingProtection):
                protection_name = protection.protection
            
            elif isinstance(protection, dict):
                protection_name = protection.get('protection', 'Critical protection')

            concerns.append(f"Missing: {protection_name}")
        
        # From clause interpretations
        high_priority_clauses = [c for c in clause_interpretations if (c.negotiation_priority == "high")]
        
        # Top 10 high priority clauses
        for clause in high_priority_clauses[:10]:  
            concerns.append(f"High priority: {clause.clause_reference} - {clause.plain_english_summary}")
        
        # Return top 20 concerns
        return concerns[:20]

    
    def _generate_negotiation_strategy(self, contract_type: ContractType, unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection],
                                       overall_risk_score: int, provider: LLMProvider) -> str:
        """
        Generate negotiation strategy using LLM
        """
        prompt = f"""
                     As a negotiation expert, provide strategic advice for contract negotiations.

                     CONTRACT TYPE: {contract_type.value}
                     RISK LEVEL: {overall_risk_score}/100
                     KEY ISSUES: {len(unfavorable_terms)} unfavorable terms, {len(missing_protections)} missing protections

                     Provide 3-4 bullet points of negotiation strategy focusing on the most critical issues. Be practical and actionable.

                     Negotiation Strategy:
                  """
        
        try:
            response = self.llm_manager.complete(prompt      = prompt,
                                                 provider    = provider,
                                                 temperature = 0.3,
                                                 max_tokens  = 400,
                                                )
            
            return response.text.strip() if response.success else "Focus negotiation on the highest risk terms and missing critical protections identified in the analysis."
            
        except Exception as e:
            log_error(e, context = {"operation": "generate_negotiation_strategy"})
            return "Prioritize addressing critical risk terms and essential missing protections during negotiations."
    

    def _generate_market_comparison(self, contract_type: ContractType, overall_risk_score: int, provider: LLMProvider) -> str:
        """
        Generate market comparison context
        """
        prompt = f"""
                     Provide market context for this contract type.

                     CONTRACT TYPE: {contract_type.value}
                     RISK SCORE: {overall_risk_score}/100

                     How does this risk level compare to typical market standards for this type of contract? Provide 1-2 sentences of context.

                     Market Comparison:
                  """
                            
        try:
            response = self.llm_manager.complete(prompt      = prompt,
                                                 provider    = provider,
                                                 temperature = 0.2,
                                                 max_tokens  = 200,
                                                )
            
            return response.text.strip() if response.success else "Compare with industry standards for similar contracts."
            
        except Exception as e:
            log_error(e, context = {"operation": "generate_market_comparison"})
            return "Review against industry benchmarks for this contract type."


    def interpret_specific_clause(self, clause_text: str, clause_reference: str = "Unknown", category: str = "general", provider: Optional[LLMProvider] = None) -> ClauseInterpretation:
        """
        Interpret a specific clause text directly
        """
        temp_clause = ExtractedClause(text              = clause_text,
                                      reference         = clause_reference,
                                      category          = category,
                                      confidence        = 1.0,
                                      start_pos         = 0,
                                      end_pos           = len(clause_text),
                                      extraction_method = "manual",
                                      risk_indicators   = [],
                                      legal_bert_score  = 0.0,
                                     )
        
        return self._interpret_single_clause(temp_clause, provider or self.default_provider)
    
    
    def batch_interpret(self, clauses: List[ExtractedClause], provider: Optional[LLMProvider] = None) -> List[ClauseInterpretation]:
        """
        Batch interpretation with progress tracking
        """
        return self.interpret_clauses(clauses     = clauses,
                                      max_clauses = len(clauses),
                                      provider    = provider,
                                     )
    

    def get_unfavorable_interpretations(self, interpretations: List[ClauseInterpretation]) -> List[ClauseInterpretation]:
        """
        Filter to only unfavorable clause interpretations
        """
        unfavorable = [i for i in interpretations if (i.favorability == "unfavorable")]
        log_info(f"Found {len(unfavorable)} unfavorable interpretations")
        
        return unfavorable

    
    def get_high_risk_interpretations(self, interpretations: List[ClauseInterpretation], min_risk_count: int = 2) -> List[ClauseInterpretation]:
        """
        Filter to interpretations with multiple risks
        """
        high_risk = [i for i in interpretations if (len(i.potential_risks) >= min_risk_count)]
        log_info(f"Found {len(high_risk)} high-risk interpretations")

        return high_risk


###################################
# services/negotiation_enginer.py
###################################
# DEPENDENCIES
import re
import sys
import json
from typing import Any
from typing import List
from typing import Dict
from typing import Tuple
from pathlib import Path
from typing import Optional

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from utils.logger import log_info
from utils.logger import log_error
from config.risk_rules import RiskRules
from config.risk_rules import ContractType
from services.risk_analyzer import RiskScore
from utils.logger import ContractAnalyzerLogger
from model_manager.llm_manager import LLMManager
from model_manager.llm_manager import LLMProvider
from services.data_models import NegotiationPoint
from services.data_models import NegotiationTactic
from services.term_analyzer import UnfavorableTerm
from services.data_models import NegotiationPlaybook
from services.clause_extractor import ExtractedClause
from services.llm_interpreter import RiskInterpretation
from services.llm_interpreter import ClauseInterpretation
from services.protection_checker import MissingProtection


class NegotiationEngine:
    """
    Generate intelligent negotiation strategy with LLM enhancement integrated with full analysis pipeline and RiskRules framework
    """
    def __init__(self, llm_manager: LLMManager, default_provider: LLMProvider = LLMProvider.OLLAMA):
        """
        Initialize negotiation engine
        Arguments:
        ----------
            llm_manager      { LLMManager }  : LLMManager instance
            default_provider { LLMProvider } : Default LLM provider
        """
        self.llm_manager      = llm_manager
        self.default_provider = default_provider
        self.risk_rules       = RiskRules()
        self.logger           = ContractAnalyzerLogger.get_logger()
        log_info("NegotiationEngine initialized", default_provider = default_provider.value)

    # Main entry point with full pipeline integration
    @ContractAnalyzerLogger.log_execution_time("generate_comprehensive_playbook")
    def generate_comprehensive_playbook(self, risk_analysis: RiskScore, risk_interpretation: RiskInterpretation, unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection],
                                        clauses: List[ExtractedClause], contract_type: ContractType, max_points: int = 10, provider: Optional[LLMProvider] = None) -> NegotiationPlaybook:
        """
        Generate comprehensive negotiation playbook using all analysis results
        Arguments:
        ----------
            risk_analysis               : Complete risk analysis
            risk_interpretation         : LLM-enhanced risk explanations
            unfavorable_terms           : Detected unfavorable terms
            missing_protections         : Missing protections
            clauses                     : Extracted clauses with risk scores (should be risk-category based)
            contract_type               : Contract type for strategy
            max_points                  : Maximum negotiation points
            provider                    : LLM provider
        Returns:
        --------
            { NegotiationPlaybook }     : Comprehensive NegotiationPlaybook
        """
        provider = provider or self.default_provider
        log_info("Starting comprehensive negotiation playbook generation", contract_type = contract_type.value, overall_risk = risk_analysis.overall_score, max_points = max_points)
        # Generate prioritized negotiation points
        negotiation_points   = self.generate_negotiation_points(risk_analysis       = risk_analysis,
                                                                unfavorable_terms   = unfavorable_terms,
                                                                missing_protections = missing_protections,
                                                                clauses             = clauses,
                                                                max_points          = max_points,
                                                                provider            = provider,
                                                               )
        # Generate overall strategy using LLM
        overall_strategy     = self._generate_overall_strategy(risk_analysis       = risk_analysis,
                                                               risk_interpretation = risk_interpretation,
                                                               contract_type       = contract_type,
                                                               provider            = provider,
                                                              )
        # Identify walk-away items
        walk_away_items      = self._identify_walk_away_items(negotiation_points = negotiation_points,
                                                              risk_analysis      = risk_analysis,
                                                             )
        # Identify concession items
        concession_items     = self._identify_concession_items(negotiation_points = negotiation_points,
                                                               risk_analysis      = risk_analysis,
                                                              )
        # Generate timing guidance
        timing_guidance      = self._generate_timing_guidance(negotiation_points = negotiation_points,
                                                              contract_type      = contract_type,
                                                              provider           = provider,
                                                             )
        # Generate risk mitigation plan
        risk_mitigation_plan = self._generate_risk_mitigation_plan(risk_analysis      = risk_analysis,
                                                                   negotiation_points = negotiation_points,
                                                                   provider           = provider,
                                                                  )
        playbook             = NegotiationPlaybook(overall_strategy     = overall_strategy,
                                                   critical_points      = negotiation_points,
                                                   walk_away_items      = walk_away_items,
                                                   concession_items     = concession_items,
                                                   timing_guidance      = timing_guidance,
                                                   risk_mitigation_plan = risk_mitigation_plan,
                                                  )
        log_info("Comprehensive negotiation playbook generated", critical_points = len(negotiation_points), walk_away_items = len(walk_away_items))
        return playbook

    @ContractAnalyzerLogger.log_execution_time("generate_negotiation_points")
    def generate_negotiation_points(self, risk_analysis: RiskScore, unfavorable_terms: List[UnfavorableTerm], missing_protections: List[MissingProtection],
                                    clauses: List[ExtractedClause], max_points: int = 10, provider: Optional[LLMProvider] = None) -> List[NegotiationPoint]:
        """
        Generate prioritized negotiation strategy
        Arguments:
        ----------
            risk_analysis       { RiskScore }   : Risk analysis results
            unfavorable_terms     { list }      : Detected unfavorable terms
            missing_protections   { list }      : Missing protections
            clauses               { list }      : Extracted clauses (ideally with risk categories)
            max_points            { int }       : Maximum negotiation points to generate
            provider           { LLMProvider }  : LLM provider
        Returns:
        --------
                        { list }                : List of NegotiationPoint objects sorted by priority
        """
        provider                               = provider or self.default_provider
        # Convert dictionaries to objects if needed
        unfavorable_terms, missing_protections = self._ensure_objects(unfavorable_terms, missing_protections)
        log_info("Starting negotiation points generation", max_points = max_points, unfavorable_terms = len(unfavorable_terms), missing_protections = len(missing_protections))
        negotiation_points                     = list()
        # Critical unfavorable terms (walk-away level)
        critical_terms                         = [t for t in unfavorable_terms if (t.severity == "critical")]
        # Top-10 critical terms
        for term in critical_terms[:10]:  
            point = self._create_enhanced_point_from_term(term, clauses, priority = 1)
            if point:
                negotiation_points.append(point)
        # Critical missing protections
        critical_protections = [p for p in missing_protections if (p.importance == "critical")]
        for protection in critical_protections[:10]:
            point = self._create_enhanced_point_from_protection(protection, priority = 2)
            negotiation_points.append(point)
        # High unfavorable terms
        high_terms = [t for t in unfavorable_terms if (t.severity == "high")]
        for term in high_terms[:10]:
            point = self._create_enhanced_point_from_term(term, clauses, priority = 3)
            if point:
                negotiation_points.append(point)
        # High-risk categories from risk analysis
        high_risk_categories = self._get_high_risk_categories(risk_analysis)
        for category in high_risk_categories[:10]:
            point = self._create_category_strategy_point(category, risk_analysis, clauses, priority = 4)
            if point:
                negotiation_points.append(point)
        # Medium unfavorable terms and missing protections
        medium_terms = [t for t in unfavorable_terms if (t.severity == "medium")]
        for term in medium_terms[:10]:
            point = self._create_enhanced_point_from_term(term, clauses, priority=5)
            if point:
                negotiation_points.append(point)
        medium_protections = [p for p in missing_protections if (p.importance == "medium")]
        for protection in medium_protections[:10]:
            point = self._create_enhanced_point_from_protection(protection, priority = 5)
            negotiation_points.append(point)
        # Enhance with LLM for sophisticated language and strategy
        enhanced_points = self._enhance_with_llm_strategy(negotiation_points[:max_points], 
                                                          risk_analysis,
                                                          provider,
                                                         )
        log_info(f"Negotiation points generation complete", total_points = len(enhanced_points))
        return enhanced_points[:max_points]

    def _create_enhanced_point_from_term(self, term: UnfavorableTerm, clauses: List[ExtractedClause], priority: int) -> Optional[NegotiationPoint]:
        """
        Create enhanced negotiation point from unfavorable term
        Assumes clauses are from RiskClauseExtractor and have risk categories.
        """
        # Find clause by reference
        clause = next((c for c in clauses if (c.reference == term.clause_reference)), None)
        # If not found by reference, try finding by matching risk category (if term.category is a risk category)
        if not clause:
            clause = next((c for c in clauses if (c.category == term.category)), None) # term.category should be a risk category from TermAnalyzer

        if not clause:
            log_info(f"Could not find clause for term: {term.term} in category: {term.category}", clause_reference=term.clause_reference)
            return None

        current               = clause.text
        # Determine negotiation tactic
        tactic                = self._determine_negotiation_tactic(term, clause)
        # Generate sophisticated proposed language
        proposed              = self._generate_enhanced_proposed_language(term, clause, tactic)
        # Calculate difficulty
        difficulty            = self._calculate_negotiation_difficulty(term, tactic)
        # Generate strategic context
        business_impact       = self._generate_business_impact(term, clause)
        counterparty_concerns = self._generate_counterparty_concerns(term, tactic)
        timing                = self._suggest_timing(priority, tactic)
        return NegotiationPoint(priority              = priority,
                                category              = term.category, # Use risk category from term
                                issue                 = term.term,
                                current_language      = current,
                                proposed_language     = proposed,
                                rationale             = term.explanation,
                                tactic                = tactic,
                                fallback_position     = self._generate_strategic_fallback(term, tactic),
                                estimated_difficulty  = difficulty,
                                legal_basis           = term.legal_basis,
                                business_impact       = business_impact,
                                counterparty_concerns = counterparty_concerns,
                                timing_suggestion     = timing,
                                bargaining_chips      = self._suggest_bargaining_chips(term, tactic),
                            )

    def _create_enhanced_point_from_protection(self, protection: MissingProtection, priority: int) -> NegotiationPoint:
        """
        Create enhanced negotiation point from missing protection
        """
        difficulty = "medium" if (protection.importance == "critical") else "easy"
        return NegotiationPoint(priority             = priority,
                                category             = protection.categories[0] if protection.categories else "general", # Use first associated category
                                issue                = f"Add {protection.protection}",
                                current_language     = "[NOT PRESENT IN CONTRACT]",
                                proposed_language    = protection.suggested_language or protection.recommendation,
                                rationale            = protection.explanation,
                                tactic               = NegotiationTactic.ADDITION,
                                fallback_position    = self._generate_protection_fallback(protection),
                                estimated_difficulty = difficulty,
                                legal_basis          = protection.legal_basis,
                                business_impact      = f"Missing this protection creates {protection.risk_score}/100 risk exposure",
                                timing_suggestion    = "Early in negotiations - establishes baseline protections",
                                bargaining_chips     = ["Offer to review their standard protections in return"],
                            )

    def _create_category_strategy_point(self, category: str, risk_analysis: RiskScore, clauses: List[ExtractedClause], priority: int) -> Optional[NegotiationPoint]:
        """
        Create strategic negotiation point for high-risk category.
        Assumes clauses are from RiskClauseExtractor and have risk categories.
        """
        # Find clauses that belong to this *risk* category
        category_clauses = [c for c in clauses if c.category == category] # Direct match on risk category
        if not category_clauses:
            log_info(f"No clauses found for high-risk category: {category}", available_categories=[c.category for c in clauses])
            return None

        score            = risk_analysis.category_scores.get(category, 0)
        description      = self.risk_rules.CATEGORY_DESCRIPTIONS.get(category, {}).get("high", "") # Use high description as default for high-risk
        return NegotiationPoint(priority             = priority,
                                category             = category,
                                issue                = f"Address {category.replace('_', ' ')} risks (score: {score}/100)",
                                current_language     = f"Multiple clauses in {category} category present elevated risk (e.g., {category_clauses[0].reference}).",
                                proposed_language    = f"Request balanced, market-standard terms for {category.replace('_', ' ')} provisions",
                                rationale            = description,
                                tactic               = NegotiationTactic.MODIFICATION,
                                estimated_difficulty = "medium",
                                business_impact      = f"High risk category affecting multiple contract areas",
                                timing_suggestion    = "Mid-negotiations after establishing rapport",
                               )

    def _determine_negotiation_tactic(self, term: UnfavorableTerm, clause: ExtractedClause) -> NegotiationTactic:
        """
        Determine the best negotiation tactic for this term
        """
        text_lower = clause.text.lower()
        if (("unlimited" in text_lower) or ("sole discretion" in text_lower)):
            return NegotiationTactic.LIMITATION
        elif (("indemnify" in text_lower) and ("mutual" not in text_lower)):
            return NegotiationTactic.MUTUALIZATION
        elif (any(word in text_lower for word in ["forfeit", "penalty", "liquidated damages"])):
            return NegotiationTactic.REMOVAL
        elif (("vague" in term.explanation.lower()) or ("ambiguous" in term.explanation.lower())):
            return NegotiationTactic.CLARIFICATION
        else:
            return NegotiationTactic.MODIFICATION

    def _generate_enhanced_proposed_language(self, term: UnfavorableTerm, clause: ExtractedClause, tactic: NegotiationTactic) -> str:
        """
        Generate sophisticated proposed language based on tactic
        """
        language_templates = {NegotiationTactic.REMOVAL       : "Remove the following language: '[EXTRACT PROBLEMATIC PHRASE]'",
                              NegotiationTactic.LIMITATION    : "Add limitation: 'Not to exceed [REASONABLE LIMIT]' or 'Subject to [REASONABLE STANDARD]'",
                              NegotiationTactic.MUTUALIZATION : "Make mutual: 'Each party shall [APPLY TO BOTH PARTIES]'",
                              NegotiationTactic.CLARIFICATION : "Clarify: 'For purposes of this section, [TERM] means [CLEAR DEFINITION]'",
                              NegotiationTactic.MODIFICATION  : "Modify to: '[BALANCED, MARKET-STANDARD LANGUAGE]'",
                             }
        base_template      = language_templates.get(tactic, term.suggested_fix or "[Request balanced language]")
        # Enhance with specific examples based on term type
        if ("non-compete" in term.term.lower()):
            return "Limit to: (a) 6-12 month duration, (b) direct competitors only, (c) reasonable geographic scope"
        elif ("liability" in term.term.lower()):
            return "Add: 'Total liability capped at the greater of $[AMOUNT] or fees paid in preceding 12 months'"
        elif ("termination" in term.term.lower()):
            return "Modify to provide mutual [30-60] day notice period and clear 'for cause' definition"
        return base_template

    def _calculate_negotiation_difficulty(self, term: UnfavorableTerm, tactic: NegotiationTactic) -> str:
        """
        Calculate negotiation difficulty
        """
        if ((term.severity == "critical") and (tactic == NegotiationTactic.REMOVAL)):
            return "hard"
        elif ((term.severity == "high") or (tactic == NegotiationTactic.MUTUALIZATION)):
            return "medium"
        else:
            return "easy"

    def _generate_business_impact(self, term: UnfavorableTerm, clause: ExtractedClause) -> str:
        """
        Generate business impact analysis
        """
        if (term.severity == "critical"):
            return "Could result in significant financial exposure or business restrictions"
        elif (term.severity == "high"):
            return "Creates substantial operational risk or compliance burden"
        else:
            return "Standard business risk that should be managed"

    def _generate_counterparty_concerns(self, term: UnfavorableTerm, tactic: NegotiationTactic) -> str:
        """
        Anticipate counterparty concerns
        """
        concerns = {NegotiationTactic.REMOVAL       : "They may view this as essential protection",
                    NegotiationTactic.LIMITATION    : "They may argue this undermines the provision's purpose", 
                    NegotiationTactic.MUTUALIZATION : "They may prefer one-sided advantage",
                    NegotiationTactic.CLARIFICATION : "They may prefer ambiguity for flexibility",
                   }
        return concerns.get(tactic, "Standard negotiation resistance expected")

    def _suggest_timing(self, priority: int, tactic: NegotiationTactic) -> str:
        """
        Suggest negotiation timing
        """
        if (priority <= 2):
            return "Address early - these are deal-breakers"
        elif (tactic == NegotiationTactic.ADDITION):
            return "Early in negotiations - establishes baseline"
        else:
            return "Mid-negotiations - after establishing key terms"

    def _suggest_bargaining_chips(self, term: UnfavorableTerm, tactic: NegotiationTactic) -> List[str]:
        """
        Suggest bargaining chips
        """
        chips = list()
        if (tactic == NegotiationTactic.REMOVAL):
            chips.append("Offer alternative protection that addresses their underlying concern")
        elif (tactic == NegotiationTactic.LIMITATION):
            chips.append("Accept their position with reasonable cap or standard")
        elif (tactic == NegotiationTactic.MUTUALIZATION):
            chips.append("Frame as fairness principle benefiting both parties")
        chips.append("Trade for lower priority item they care about")
        return chips

    def _generate_strategic_fallback(self, term: UnfavorableTerm, tactic: NegotiationTactic) -> str:
        """
        Generate strategic fallback position
        """
        if (term.severity == "critical"):
            return "If no compromise, seriously consider walking away - this creates unacceptable risk"
        elif (term.severity == "high"):
            return "If they refuse, document objection and consider risk mitigation strategies"
        else:
            return "If they won't budge, assess if other favorable terms compensate for this risk"

    def _ensure_objects(self, unfavorable_terms, missing_protections):
        """
        Convert dictionaries back to proper objects if needed
        """
        if unfavorable_terms and isinstance(unfavorable_terms[0], dict):
            from services.term_analyzer import UnfavorableTerm
            unfavorable_terms = [UnfavorableTerm(**term_dict) for term_dict in unfavorable_terms]
        if missing_protections and isinstance(missing_protections[0], dict):
            from services.protection_checker import MissingProtection
            missing_protections = [MissingProtection(**prot_dict) for prot_dict in missing_protections]
        return unfavorable_terms, missing_protections

    def _generate_protection_fallback(self, protection: MissingProtection) -> str:
        """
        Generate fallback for missing protections
        """
        if (protection.importance == "critical"):
            return "If they refuse, document this material gap and assess deal viability"
        else:
            return "If they refuse, note the gap and consider if other protections compensate"

    def _get_high_risk_categories(self, risk_analysis: RiskScore) -> List[str]:
        """
        Get high-risk categories from risk analysis
        """
        # Use the risk thresholds defined in RiskRules
        high_threshold = self.risk_rules.RISK_THRESHOLDS.get("high", 60)
        return [cat for cat, score in risk_analysis.category_scores.items() if score >= high_threshold]

    # REMOVED: _matches_risk_category method as it's no longer needed for _create_category_strategy_point
    # or assumed to be used elsewhere where clause_category vs risk_category distinction is clear.

    def _enhance_with_llm_strategy(self, points: List[NegotiationPoint], risk_analysis: RiskScore, provider: LLMProvider) -> List[NegotiationPoint]:
        """
        Use LLM to enhance negotiation points with sophisticated strategy
        """
        if not points:
            return points
        log_info(f"Enhancing {len(points)} negotiation points with LLM strategy")
        try:
            prompt   = self._create_strategic_enhancement_prompt(points, risk_analysis)
            response = self.llm_manager.complete(prompt             = prompt,
                                                 provider           = provider,
                                                 temperature        = 0.3,
                                                 max_tokens         = 2000,
                                                 fallback_providers = [LLMProvider.OPENAI],
                                                 retry_on_error     = True,
                                                )
            if response.success:
                enhanced = self._parse_strategic_enhancements(response.text, points)
                log_info("LLM strategic enhancement successful")
                return enhanced
            else:
                log_info("LLM strategic enhancement failed, using original points")
                return points
        except Exception as e:
            log_error(e, context = {"component": "NegotiationEngine", "operation": "enhance_with_llm_strategy"})
            return points

    def _create_strategic_enhancement_prompt(self, points: List[NegotiationPoint],  risk_analysis: RiskScore) -> str:
        """
        Create prompt for strategic LLM enhancement
        """
        context = {"overall_risk" : risk_analysis.overall_score,
                   "risk_level"   : risk_analysis.risk_level,
                   "points"       : [{"priority"   : p.priority,
                                      "issue"      : p.issue,
                                      "category"   : p.category,
                                      "current"    : p.current_language[:150], # Truncate for prompt size
                                      "proposed"   : p.proposed_language,
                                      "tactic"     : p.tactic.value,
                                      "difficulty" : p.estimated_difficulty
                                     }
                                     for p in points
                                    ],
                  }
        prompt = f"""
                     As an expert negotiation strategist, enhance these negotiation points with sophisticated strategy.
                     CONTRACT RISK: {context['overall_risk']}/100 ({context['risk_level']})
                     NEGOTIATION POINTS (format: [{{'priority': int, 'issue': str, 'category': str, 'current': str, 'proposed': str, 'tactic': str, 'difficulty': str}}]):
                     {json.dumps(context['points'], indent=2)}
                     For EACH point (match the order and priority), provide:
                     1. ENHANCED_PROPOSAL: More specific, legally sound alternative language (only return the enhanced text).
                     2. STRATEGIC_RATIONALE: Business-focused reasoning emphasizing mutual benefit (only return the rationale).
                     3. COUNTERPARTY_PERSPECTIVE: Their likely concerns and how to address them (only return the perspective).
                     4. TIMING_STRATEGY: When and how to raise this issue (only return the timing).
                     5. BARGAINING_CHIPS: Specific trade-offs or concessions (only return the chips, as a comma-separated string).
                     Focus on creating win-win solutions and practical negotiation tactics. Respond in the exact format below for each point:
                     Point 1:
                     ENHANCED_PROPOSAL: [text]
                     STRATEGIC_RATIONALE: [text]
                     COUNTERPARTY_PERSPECTIVE: [text]
                     TIMING_STRATEGY: [text]
                     BARGAINING_CHIPS: [chip1, chip2, ...]
                     Point 2:
                     ENHANCED_PROPOSAL: [text]
                     ...
                  """
        return prompt

    def _parse_strategic_enhancements(self, llm_text: str, original_points: List[NegotiationPoint]) -> List[NegotiationPoint]:
        """
        Parse LLM strategic enhancements, assuming a structured format.
        """
        enhanced = []
        for i, point in enumerate(original_points):
            point_identifier = f"Point {i+1}:"
            start_idx = llm_text.find(point_identifier)
            if start_idx == -1:
                log_info(f"LLM response did not contain expected identifier for Point {i+1}. Keeping original.")
                enhanced.append(point)
                continue

            # Find the start of the next point or end of string
            next_point_idx = llm_text.find(f"Point {i+2}:", start_idx)
            if next_point_idx == -1:
                section_text = llm_text[start_idx:]
            else:
                section_text = llm_text[start_idx:next_point_idx]

            # Extract fields using regex within the section_text
            # ENHANCED_PROPOSAL
            proposal_match = re.search(r"ENHANCED_PROPOSAL:\s*(.*?)(?:\n|$)", section_text, re.DOTALL)
            if proposal_match:
                new_proposal = proposal_match.group(1).strip()
                if new_proposal and len(new_proposal) > 10: # Basic sanity check
                    point.proposed_language = new_proposal[:600] # Cap length

            # TIMING_STRATEGY
            timing_match = re.search(r"TIMING_STRATEGY:\s*(.*?)(?:\n|$)", section_text, re.DOTALL)
            if timing_match:
                new_timing = timing_match.group(1).strip()
                if new_timing and len(new_timing) > 5:
                    point.timing_suggestion = new_timing[:200]

            # BARGAINING_CHIPS
            chips_match = re.search(r"BARGAINING_CHIPS:\s*\[(.*?)\]", section_text, re.DOTALL)
            if chips_match:
                chips_str = chips_match.group(1).strip()
                if chips_str:
                    # Split by comma and strip whitespace
                    chips_list = [chip.strip().strip('"\'') for chip in chips_str.split(',') if chip.strip()]
                    point.bargaining_chips = chips_list[:3] # Keep top 3

            # STRATEGIC_RATIONALE and COUNTERPARTY_PERSPECTIVE could be added similarly if needed
            # For now, we just update the fields we parsed.

            enhanced.append(point)

        return enhanced

    def _generate_overall_strategy(self, risk_analysis: RiskScore, risk_interpretation: RiskInterpretation, contract_type: ContractType, provider: LLMProvider) -> str:
        """
        Generate overall negotiation strategy using LLM
        """
        prompt = f"""
                     As a negotiation expert, provide overall strategy for this contract.
                     CONTRACT TYPE: {contract_type.value}
                     RISK LEVEL: {risk_analysis.overall_score}/100 ({risk_analysis.risk_level})
                     KEY CONCERNS: {risk_interpretation.key_concerns}
                     Provide a concise 3-4 sentence negotiation strategy focusing on:
                     - Overall approach (collaborative vs. firm)
                     - Key priorities
                     - Risk management
                     - Success metrics
                     Strategy:
                  """
        try:
            response = self.llm_manager.complete(prompt      = prompt,
                                                 provider    = provider,
                                                 temperature = 0.3,
                                                 max_tokens  = 400,
                                                )
            return response.text.strip() if response.success else "Focus on addressing critical risks while maintaining collaborative negotiation tone."
        except Exception as e:
            log_error(e, context = {"operation": "generate_overall_strategy"})
            return "Prioritize critical risk items while seeking balanced, market-standard terms."

    def _identify_walk_away_items(self, negotiation_points: List[NegotiationPoint], risk_analysis: RiskScore) -> List[str]:
        """
        Identify non-negotiable walk-away items
        """
        walk_away       = list()
        critical_points = [p for p in negotiation_points if (p.priority == 1)]
        for point in critical_points:
            if ((point.estimated_difficulty == "hard") and (risk_analysis.overall_score >= 70)):
                walk_away.append(f"{point.issue} - critical risk that cannot be mitigated")
        # Max 5 walk-away items
        return walk_away[:5]  

    def _identify_concession_items(self, negotiation_points: List[NegotiationPoint],
                                 risk_analysis: RiskScore) -> List[str]:
        """
        Identify items that can be conceded
        """
        concessions  = list()
        low_priority = [p for p in negotiation_points if p.priority >= 4]
        for point in low_priority[:2]:
            if (point.estimated_difficulty == "hard"):
                concessions.append(f"{point.issue} - lower priority, high difficulty")
        return concessions

    def _generate_timing_guidance(self, negotiation_points: List[NegotiationPoint], contract_type: ContractType, provider: LLMProvider) -> str:
        """
        Generate timing guidance for negotiations
        """
        critical_count = len([p for p in negotiation_points if p.priority <= 2])
        if (critical_count >= 3):
            return "Start with critical items early - multiple deal-breakers need immediate attention"
        elif (critical_count >= 1):
            return "Address 1-2 critical items first, then move to high-priority items"
        else:
            return "Progressive approach: start with easier wins to build momentum"

    def _generate_risk_mitigation_plan(self, risk_analysis: RiskScore, negotiation_points: List[NegotiationPoint], provider: LLMProvider) -> str:
        """
        Generate risk mitigation plan for unresolved issues
        """
        if (risk_analysis.overall_score >= 70):
            return "High risk level - focus on critical term resolution. Have fallback positions ready."
        elif (risk_analysis.overall_score >= 50):
            return "Moderate risk - prioritize 2-3 key improvements. Document remaining risks."
        else:
            return "Manageable risk level - focus on most impactful improvements."

    # Keep existing utility methods for backward compatibility
    def generate_negotiation_strategy_document(self, playbook: NegotiationPlaybook) -> str:
        """
        Generate a formatted negotiation strategy document
        Returns:
        -------
            Formatted markdown document
        """
        doc = ["# Comprehensive Negotiation Playbook",
               "",
               f"## Overall Strategy",
               f"{playbook.overall_strategy}",
               "",
               "## Critical Negotiation Points",
               ""
              ]
        # Group by priority with enhanced labels
        by_priority = dict()
        for point in playbook.critical_points:
            if point.priority not in by_priority:
                by_priority[point.priority] = []
            by_priority[point.priority].append(point)
        priority_labels = {1: " CRITICAL PRIORITY - Deal Breakers",
                           2: " HIGH PRIORITY - Essential Items", 
                           3: " MEDIUM PRIORITY - Important Improvements",
                           4: " STANDARD PRIORITY - Recommended Changes",
                           5: " LOW PRIORITY - Optional Improvements"
                          }
        for priority in sorted(by_priority.keys()):
            doc.append(f"### {priority_labels.get(priority, f'Priority {priority}')}")
            doc.append("")
            for point in by_priority[priority]:
                doc.append(f"#### {point.issue}")
                doc.append(f"**Category:** {point.category} | **Tactic:** {point.tactic.value} | **Difficulty:** {point.estimated_difficulty}")
                doc.append("")
                doc.append("**Current Language:**")
                doc.append(f"> {point.current_language}")
                doc.append("")
                doc.append("**Proposed Language:**")
                doc.append(f"{point.proposed_language}")
                doc.append("")
                doc.append("**Rationale:**")
                doc.append(f"{point.rationale}")
                doc.append("")
                if point.business_impact:
                    doc.append("**Business Impact:**")
                    doc.append(f"{point.business_impact}")
                    doc.append("")
                if point.timing_suggestion:
                    doc.append("**Timing:**")
                    doc.append(f"{point.timing_suggestion}")
                    doc.append("")
                if point.bargaining_chips:
                    doc.append("**Bargaining Chips:**")
                    for chip in point.bargaining_chips:
                        doc.append(f"- {chip}")
                    doc.append("")
                if point.fallback_position:
                    doc.append("**Fallback Position:**")
                    doc.append(f"{point.fallback_position}")
                    doc.append("")
                doc.append("---")
                doc.append("")
        # Add strategy sections
        if playbook.walk_away_items:
            doc.append("##  Walk-Away Items")
            doc.append("Do not proceed if these cannot be resolved:")
            for item in playbook.walk_away_items:
                doc.append(f"- {item}")
            doc.append("")
        if playbook.concession_items:
            doc.append("##  Concession Items") 
            doc.append("Consider conceding these if needed:")
            for item in playbook.concession_items:
                doc.append(f"- {item}")
            doc.append("")
        doc.append("##  Timing Guidance")
        doc.append(playbook.timing_guidance)
        doc.append("")
        doc.append("## Risk Mitigation Plan")
        doc.append(playbook.risk_mitigation_plan)
        return "\n".join(doc)

    def get_critical_points(self, points: List[NegotiationPoint]) -> List[NegotiationPoint]:
        """
        Filter to only priority 1-2 points
        """
        critical = [p for p in points if p.priority <= 2]
        log_info(f"Found {len(critical)} critical negotiation points")
        return critical

    def get_points_by_category(self, points: List[NegotiationPoint],
                              category: str) -> List[NegotiationPoint]:
        """
        Filter points by category
        """
        filtered = [p for p in points if (p.category == category)]
        log_info(f"Found {len(filtered)} negotiation points in category '{category}'")
        return filtered


#################################
# services/summary_generator.py
#################################
# DEPENDENCIES
import sys
from typing import Any
from typing import Dict
from typing import List
from pathlib import Path
from typing import Optional

# Add parent directory to path for imports
sys.path.append(str(Path(__file__).parent.parent))

from services.risk_analyzer import RiskScore
from services.data_models import SummaryContext
from utils.logger import ContractAnalyzerLogger 
from model_manager.llm_manager import LLMManager
from model_manager.llm_manager import LLMProvider
from services.data_models import ContractCategory
from services.data_models import RiskInterpretation
from services.data_models import NegotiationPlaybook


class SummaryGenerator:
    """
    LLM-powered executive summary generator for contract analysis : Generates professional, detailed executive summaries using ALL pipeline outputs
    """
    def __init__(self, llm_manager: Optional[LLMManager] = None):
        """
        Initialize the summary generator
        
        Arguments:
        ----------
            llm_manager { LLMManager } : LLM manager instance (if None, creates one with default settings)
        """
        self.llm_manager = llm_manager or LLMManager()
        self.logger      = ContractAnalyzerLogger.get_logger() 
        
        self.logger.info("Summary generator initialized")


    # Main entry point with full pipeline integration
    def generate_executive_summary(self, contract_text: str, classification: ContractCategory, risk_analysis: RiskScore, risk_interpretation: RiskInterpretation,
                                   negotiation_playbook: NegotiationPlaybook, unfavorable_terms: List, missing_protections: List, clauses: List) -> str:
        """
        Generate executive summary using all the pipeline outputs
        
        Arguments:
        ----------
            contract_text               { str }          : Original contract text (for context)
            
            classification       { ContractCategory }    : Contract classification results
            
            risk_analysis            { RiskScore }       : Complete risk analysis
            
            risk_interpretation  { RiskInterpretation }  : LLM-enhanced risk explanations
            
            negotiation_playbook { NegotiationPlaybook } : Comprehensive negotiation strategy
            
            unfavorable_terms            { List }        : Detected unfavorable terms
            
            missing_protections          { List }        : Missing protections
            
            clauses                      { List }        : Extracted clauses
            
        Returns:
        --------
                             { str }                     : Generated executive summary string
        """
        try:
            # Prepare context with all pipeline data
            context = self._prepare_summary_context(contract_text        = contract_text,
                                                    classification       = classification,
                                                    risk_analysis        = risk_analysis,
                                                    risk_interpretation  = risk_interpretation,
                                                    negotiation_playbook = negotiation_playbook,
                                                    unfavorable_terms    = unfavorable_terms,
                                                    missing_protections  = missing_protections,
                                                    clauses              = clauses,
                                                   )
            
            # Generate summary using LLM
            summary = self._generate_summary(context = context)
            
            self.logger.info(f"Executive summary generated - Risk: {context.risk_score}/100 ({context.risk_level})") 
            
            return summary
            
        except Exception as e:
            self.logger.error(f"Failed to generate comprehensive summary: {repr(e)}") 
            
            # Fallback with available data
            return self._generate_fallback_summary(contract_text       = contract_text,
                                                   classification      = classification,
                                                   risk_analysis       = risk_analysis,
                                                   unfavorable_terms   = unfavorable_terms,
                                                   missing_protections = missing_protections,
                                                  )
    

    def _prepare_summary_context(self, contract_text: str, classification: ContractCategory, risk_analysis: RiskScore, risk_interpretation: RiskInterpretation,
                                 negotiation_playbook: NegotiationPlaybook, unfavorable_terms: List[Dict], missing_protections: List[Dict], clauses: List) -> SummaryContext:
        """
        Prepare summary context with all pipeline data
        """
        # Handle null negotiation_playbook
        walk_away_count = 0

        if negotiation_playbook and hasattr(negotiation_playbook, 'walk_away_items'):
            walk_away_count = len(negotiation_playbook.walk_away_items)
            
        # Extract contract text
        contract_preview = contract_text 
        
        # Extract key findings from all sources
        key_findings     = self._extract_findings(risk_analysis        = risk_analysis,
                                                  risk_interpretation  = risk_interpretation,
                                                  negotiation_playbook = negotiation_playbook,
                                                  unfavorable_terms    = unfavorable_terms,
                                                  missing_protections  = missing_protections,
                                                  clauses              = clauses,
                                                 )
                                                            
        # Prepare metadata
        metadata         = {"contract_length"  : len(contract_text),
                            "clauses_analyzed" : len(clauses),
                            "critical_issues"  : len([t for t in unfavorable_terms if (self._get_severity(t) == "critical")]),
                            "walk_away_items"  : walk_away_count,
                           }
         
        return SummaryContext(contract_type         = classification.category,
                              risk_score            = risk_analysis.overall_score,
                              risk_level            = risk_analysis.risk_level,
                              category_scores       = risk_analysis.category_scores,
                              unfavorable_terms     = unfavorable_terms,
                              missing_protections   = missing_protections,
                              clauses               = clauses,
                              key_findings          = key_findings,
                              risk_interpretation   = risk_interpretation,
                              negotiation_playbook  = negotiation_playbook,
                              contract_text_preview = contract_preview,
                              contract_metadata     = metadata,
                             )

    
    def _extract_findings(self, risk_analysis: RiskScore, risk_interpretation: RiskInterpretation, negotiation_playbook: NegotiationPlaybook,
                          unfavorable_terms: List[Dict], missing_protections: List[Dict], clauses: List) -> List[str]:
        """
        Extract findings from all analysis components
        """
        findings = list()
        
        # Overall risk context
        if (risk_analysis.overall_score >= 80):
            findings.append("CRITICAL RISK LEVEL: Contract presents unacceptable risk requiring immediate attention")

        elif (risk_analysis.overall_score >= 60):
            findings.append("HIGH RISK LEVEL: Significant concerns requiring substantial negotiation")
        
        # Critical unfavorable terms
        critical_terms = [t for t in unfavorable_terms if (self._get_severity(t) == "critical")]
        
        if critical_terms:
            findings.append(f"{len(critical_terms)} CRITICAL unfavorable terms identified")
            for term in critical_terms[:2]:
                term_name = self._get_term_name(term = term)
                
                findings.append(f"Critical: {term_name}")
        
        # Critical missing protections
        critical_protections = [p for p in missing_protections if (self._get_importance(p) == "critical")]
        
        if critical_protections:
            findings.append(f"{len(critical_protections)} CRITICAL protections missing")
            for prot in critical_protections[:2]:
                prot_name = self._get_protection_name(protection = prot)
                
                findings.append(f"Missing: {prot_name}")
        
        # High-risk categories
        high_risk_categories = [cat for cat, score in risk_analysis.category_scores.items() if (score >= 70)]
        if high_risk_categories:
            findings.append(f"High-risk categories: {', '.join(high_risk_categories)}")
        
        # Walk-away items from negotiation playbook
        if negotiation_playbook and negotiation_playbook.walk_away_items:
            findings.append(f"{len(negotiation_playbook.walk_away_items)} potential deal-breakers identified")
        
        # Key concerns from risk interpretation
        if risk_interpretation and risk_interpretation.key_concerns:
            top_concerns = risk_interpretation.key_concerns[:2]
            for concern in top_concerns:
                findings.append(f"Key concern: {concern}")
        
        return findings  
    

    def _generate_summary(self, context: SummaryContext) -> str:
        """
        Generate enhanced summary using comprehensive context
        """
        prompt        = self._build_summary_prompt(context)
        system_prompt = self._build_system_prompt()
        
        try:
            response = self.llm_manager.complete(prompt        = prompt,
                                                 system_prompt = system_prompt,
                                                 temperature   = 0.3,
                                                 max_tokens    = 300, 
                                                 json_mode     = False,
                                                )
              
            if response.success and response.text.strip():
                return self._clean_summary_response(text = response.text)
            
            else:
                raise ValueError(f"LLM generation failed: {response.error_message}")
                
        except Exception as e:
            self.logger.error(f"Enhanced LLM summary generation failed: {e}")
            # Fallback to basic summary
            return self._generate_fallback_summary_from_context(context = context)
    

    def _build_system_prompt(self) -> str:
        """
        Build system prompt for executive summary generation
        """
        system_prompt =  """
                            You are a senior contract risk analyst. Generate CONCISE executive summaries.

                            CRITICAL REQUIREMENTS:
                            1. Maximum 120 words (strict limit)
                            2. Must mention SPECIFIC clause numbers (e.g., Clause 8.2, Clause 9.5)
                            3. Direct, urgent tone - no hedging or academic language
                            4. Focus ONLY on top 3 critical risks

                            STRUCTURE (3-4 sentences total):
                            Sentence 1: Overall risk assessment with contract type
                            Sentence 2-3: Top 2-3 critical risks with SPECIFIC clause references
                            Sentence 4: Brief actionable conclusion

                            TONE EXAMPLES:
                             GOOD: "This employment agreement is heavily skewed in favor of the Employer. Clause 8.2 fails to define post-probation salary. Clause 11.2 allows illegal wage forfeiture."
                             BAD: "The comprehensive analysis indicates that there are several concerns that require attention. It is essential to carefully review..."

                            FORBIDDEN PHRASES:
                            - "comprehensive analysis"
                            - "it is essential to"
                            - "requires attention"
                            - "should be reviewed"
                            - "it is recommended"

                            OUTPUT: Pure paragraph text only. No formatting, no bullets, no headers.
                         """
        
        return system_prompt


    def _build_summary_prompt(self, context: SummaryContext) -> str:
        """
        Build prompt for executive summary generation
        """
        # Extract top critical issues only
        critical_terms       = [t for t in context.unfavorable_terms if self._get_severity(t) == "critical"][:10]
        
        critical_protections = [p for p in context.missing_protections if self._get_importance(p) == "critical"][:10]
        
        # Build concise context
        critical_issues_text = ""
        
        if critical_terms:
            critical_issues_text += "CRITICAL UNFAVORABLE TERMS:\n"
            
            for term in critical_terms:
                clause_reference      = self._get_clause_reference(term = term)
                term_name             = self._get_term_name(term = term)
                critical_issues_text += f"- {clause_reference}: {term_name}\n"
        
        if critical_protections:
            critical_issues_text += "\nCRITICAL MISSING PROTECTIONS:\n"
            
            for protection in critical_protections:
                protection_name       = self._get_protection_name(protection = protection)
                critical_issues_text += f"- {protection_name}\n"
        
        # Determine risk tone
        if (context.risk_score >= 80):
            risk_tone = "heavily skewed/very high risk/presents unacceptable risk"
        
        elif (context.risk_score >= 60):
            risk_tone = "significantly unfavorable/high risk/substantial concerns"

        elif (context.risk_score >= 40):
            risk_tone = "moderately concerning/notable risk/requires negotiation"

        else:
            risk_tone = "generally reasonable/manageable risk/standard concerns"
        
        summary_prompt = f"""
                             CONTRACT ANALYSIS DATA:
                            
                             - Type: {context.contract_type.replace('_', ' ').title()}
                             - Risk Score: {context.risk_score}/100
                             - Risk Level: {context.risk_level}
                             - Appropriate Tone: {risk_tone}

                             {critical_issues_text}

                             TASK:
                             Write a 100-120 word executive summary following this EXACT structure:

                             1. First sentence: "This [contract type] [risk assessment with tone matching score]"
                             2. Second sentence: State top critical risk with SPECIFIC clause number
                             3. Third sentence: State second critical risk with SPECIFIC clause number
                             4. Fourth sentence: Brief conclusion about action needed

                             EXAMPLE (for 85/100 risk employment contract):
                             "This employment agreement is heavily skewed in favor of the Employer, presenting a very high risk to the Employee. Key concerns include Clause 9.5's extremely broad 24-month non-compete against the entire industry, and Clause 11.2's punitive penalty allowing forfeiture of earned wages. The termination clauses in Clause 17 are highly asymmetrical, giving the employer unilateral power. Significant negotiation is required before signing."

                             YOUR TURN - Generate summary for THIS contract:
                          """
                        
        return summary_prompt
    

    def _clean_summary_response(self, text: str) -> str:
        """
        Clean and format the LLM response
        """
        # Remove any markdown formatting
        text          = text.replace('**', '').replace('*', '').replace('#', '')
        
        # Remove common LLM artifacts and empty lines
        lines         = text.split('\n')
        cleaned_lines = list()
        
        for line in lines:
            line = line.strip()
            if line and not line.lower().startswith(('executive summary', 'summary:', 'here is', 'based on', 'certainly')):
                cleaned_lines.append(line)
        
        # Join into coherent paragraph
        summary = ' '.join(cleaned_lines)
        
        # Ensure proper sentence structure
        if summary:
            if not summary[0].isupper():
                summary = summary[0].upper() + summary[1:]
            
            if not summary.endswith(('.', '!', '?')):
                summary += '.'
        
        return summary
    

    def _generate_fallback_summary(self, contract_text: str, classification: ContractCategory, risk_analysis: RiskScore, unfavorable_terms: List[Dict], missing_protections: List[Dict]) -> str:
        """
        Generate enhanced fallback summary
        """
        contract_type_display = classification.category.replace('_', ' ').title()
        
        # Count critical items
        critical_terms        = len([t for t in unfavorable_terms if (self._get_severity(t) == "critical")])
        critical_protections  = len([p for p in missing_protections if (self._get_importance(p) == "critical")])
        
        # Risk assessment
        if (risk_analysis.overall_score >= 80):
            risk_assessment = f"This {contract_type_display} presents a CRITICAL level of risk"
            action          = "requires immediate executive attention and significant revision before consideration"
        
        elif (risk_analysis.overall_score >= 60):
            risk_assessment = f"This {contract_type_display} presents a HIGH level of risk" 
            action          = "requires careful legal review and substantial negotiation to mitigate key concerns"
        
        elif (risk_analysis.overall_score >= 40):
            risk_assessment = f"This {contract_type_display} presents a MODERATE level of risk"
            action          = "requires professional review and selective negotiation on specific provisions"
        
        else:
            risk_assessment = f"This {contract_type_display} presents a LOW level of risk"
            action = "appears generally reasonable but should undergo standard legal review"
        
        summary  = f"{risk_assessment} with an overall risk score of {risk_analysis.overall_score}/100. "
        summary += f"The agreement {action}. "
        
        # Add critical items context
        if (critical_terms > 0):
            summary += f"Analysis identified {critical_terms} critical unfavorable terms "

            if critical_protections > 0:
                summary += f"and {critical_protections} critical missing protections. "

            else:
                summary += f"and {len(missing_protections)} missing standard protections. "

        else:
            summary += f"Review identified {len(unfavorable_terms)} areas for improvement. "
        
        # Add high-risk categories context
        high_risk_categories = [cat for cat, score in risk_analysis.category_scores.items() if (score >= 60)]
        
        if high_risk_categories:
            category_names = [cat.replace('_', ' ').title() for cat in high_risk_categories[:2]]
            summary       += f"Particular attention should be given to {', '.join(category_names)} provisions. "
        
        summary += "Proceed with the detailed negotiation strategy and risk mitigation recommendations provided in the full analysis."
        
        return summary

    
    def _generate_fallback_summary_from_context(self, context: SummaryContext) -> str:
        """
        Generate fallback summary from context object
        """
        # Access attributes safely, providing defaults if needed by the fallback logic
        text_preview  = context.contract_text_preview if context.contract_text_preview is not None else ""
        missing_prots = context.missing_protections if context.missing_protections is not None else []
        unfav_terms   = context.unfavorable_terms if context.unfavorable_terms is not None else []

        return self._generate_fallback_summary(contract_text       = text_preview,
                                               classification      = type('MockClassification', (), {'category': context.contract_type})(),
                                               risk_analysis       = type('MockRiskAnalysis', (), {'overall_score': context.risk_score, 'risk_level': context.risk_level, 'category_scores': context.category_scores or {}})(),
                                               unfavorable_terms   = unfav_terms,
                                               missing_protections = missing_prots,
                                              )
    

    def _get_severity(self, term) -> str:
        """
        Safely get severity from term object or dict
        """
        try:
            if (hasattr(term, 'severity')):
                return term.severity
            
            else:
                return term.get('severity', 'unknown')
        
        except (AttributeError, KeyError):
            return 'unknown'
    

    def _get_importance(self, protection) -> str:
        """
        Safely get importance from protection object or dict
        """
        try:
            if hasattr(protection, 'importance'):
                return protection.importance

            else:
                return protection.get('importance', 'unknown')
        
        except (AttributeError, KeyError):
            return 'unknown'
    

    def _get_term_name(self, term) -> str:
        """
        Safely get term name
        """
        try:
            if hasattr(term, 'term'):
                return term.term

            else:
                return term.get('term', 'Unknown Term')

        except (AttributeError, KeyError):
            return 'Unknown Term'
    

    def _get_protection_name(self, protection) -> str:
        """
        Safely get protection name
        """
        try:
            if hasattr(protection, 'protection'):
                return protection.protection
            
            else:
                return protection.get('protection', 'Unknown Protection')
        
        except (AttributeError, KeyError):
            return 'Unknown Protection'
    

    def _get_explanation(self, item) -> str:
        """
        Safely get explanation
        """
        try:
            if hasattr(item, 'explanation'):
                return item.explanation
            
            else:
                return item.get('explanation', 'No explanation available')
        
        except (AttributeError, KeyError):
            return 'No explanation available'


    def _get_clause_reference(self, term) -> str:
        """
        Safely get clause reference from term
        """
        try:
            if hasattr(term, 'clause_reference'):
                ref = term.clause_reference
                return ref if ref and ref != 'None' else 'Multiple clauses'

            else:
                ref = term.get('clause_reference', '')
                return ref if ref and ref != 'None' else 'Multiple clauses'

        except (AttributeError, KeyError):
            return 'Unknown clause'


#####################################
# reporter/pdf_generator.py
#####################################
# DEPENDENCIES
import os
import math
from typing import Any
from io import BytesIO
from typing import Dict
from typing import List
from typing import Optional
from datetime import datetime
from reportlab.lib import colors
from reportlab.pdfgen import canvas
from reportlab.platypus import Image
from reportlab.platypus import Table
from reportlab.lib.units import inch
from reportlab.platypus import Spacer
from reportlab.lib.pagesizes import A4
from reportlab.lib.enums import TA_LEFT
from reportlab.platypus import Paragraph
from reportlab.platypus import PageBreak
from reportlab.graphics import renderPDF
from reportlab.platypus import TableStyle
from reportlab.lib.enums import TA_CENTER
from reportlab.lib.enums import TA_JUSTIFY
from reportlab.lib.pagesizes import letter
from reportlab.lib.utils import simpleSplit
from reportlab.platypus import KeepTogether
from reportlab.graphics.shapes import Circle
from reportlab.graphics.shapes import String
from reportlab.lib.pagesizes import landscape
from reportlab.graphics.shapes import Drawing
from reportlab.lib.styles import ParagraphStyle
from reportlab.platypus import SimpleDocTemplate
from reportlab.platypus.flowables import PageBreak
from reportlab.platypus.flowables import KeepInFrame
from reportlab.lib.styles import getSampleStyleSheet


class PDFReportGenerator:
    """
    Professional-grade PDF report generator matching sample style exactly
    """
    def __init__(self):
        self.styles         = getSampleStyleSheet()
        self._setup_custom_styles()
        self.page_width     = letter[0]
        self.page_height    = letter[1]
        self.margin_left    = 0.75 * inch
        self.margin_right   = 0.75 * inch
        self.margin_top     = 1 * inch
        self.margin_bottom  = 1 * inch
        self.content_width  = self.page_width - self.margin_left - self.margin_right
        self.content_height = self.page_height - self.margin_top - self.margin_bottom


    def _setup_custom_styles(self):
        """
        Setup custom paragraph styles with precise control
        """
        # Title style
        self.styles.add(ParagraphStyle(
            name='ReportTitle',
            parent=self.styles['Heading1'],
            fontSize=24,
            textColor=colors.HexColor('#1a1a1a'),
            spaceAfter=20,
            alignment=TA_LEFT,
            fontName='Helvetica-Bold',
        ))

        # Section heading
        self.styles.add(ParagraphStyle(
            name='SectionHeading',
            parent=self.styles['Heading2'],
            fontSize=16,
            textColor=colors.HexColor('#1a1a1a'),
            spaceAfter=12,
            spaceBefore=20,
            fontName='Helvetica-Bold',
        ))

        # Sub-section heading
        self.styles.add(ParagraphStyle(
            name='SubSectionHeading',
            parent=self.styles['Normal'],
            fontSize=12,
            textColor=colors.HexColor('#333333'),
            spaceAfter=8,
            spaceBefore=12,
            fontName='Helvetica-Bold',
        ))

        # Body text
        self.styles.add(ParagraphStyle(
            name='CustomBodyText',
            parent=self.styles['Normal'],
            fontSize=10,
            leading=14,
            textColor=colors.HexColor('#333333'),
            alignment=TA_JUSTIFY,
            fontName='Helvetica',
            leftIndent=0,
            rightIndent=0,
        ))

        # Small text style
        self.styles.add(ParagraphStyle(
            name='SmallText',
            parent=self.styles['Normal'],
            fontSize=8,
            leading=10,
            textColor=colors.HexColor('#666666'),
            fontName='Helvetica',
        ))

        # Bullet point
        self.styles.add(ParagraphStyle(
            name='BulletPoint',
            parent=self.styles['Normal'],
            fontSize=10,
            leading=14,
            textColor=colors.HexColor('#333333'),
            leftIndent=20,
            bulletIndent=10,
            bulletFontName='Helvetica',
            bulletFontSize=10,
            bulletColor=colors.black,
            spaceAfter=4,
            fontName='Helvetica',
        ))

        # Table header
        self.styles.add(ParagraphStyle(
            name='TableHeader',
            parent=self.styles['Normal'],
            fontSize=10,
            textColor=colors.HexColor('#1a1a1a'),
            fontName='Helvetica-Bold',
            alignment=TA_LEFT,
        ))

        # Table cell
        self.styles.add(ParagraphStyle(
            name='TableCell',
            parent=self.styles['Normal'],
            fontSize=9,
            textColor=colors.HexColor('#333333'),
            fontName='Helvetica',
            alignment=TA_LEFT,
            spaceAfter=2,
        ))

        # Footer
        self.styles.add(ParagraphStyle(
            name='Footer',
            parent=self.styles['Normal'],
            fontSize=8,
            textColor=colors.HexColor('#666666'),
            alignment=TA_CENTER,
            fontName='Helvetica',
        ))


    def _draw_risk_score_circle(self, score: int) -> Drawing:
        """Draw the risk score circle graphic with correct fill percentage"""
        d = Drawing(150, 150)
        # Define circle properties
        center_x, center_y = 75, 75
        outer_radius = 60
        inner_radius = 45
        thickness = 15  # Thickness of the colored ring

        # Determine color based on score
        if score >= 80:
            color = colors.HexColor('#dc2626')  # Red
        elif score >= 60:
            color = colors.HexColor('#f97316')  # Orange
        elif score >= 40:
            color = colors.HexColor('#ca8a04')  # Amber
        else:
            color = colors.HexColor('#16a34a')  # Green

        # Draw background circle (light grey)
        bg_circle = Circle(center_x, center_y, outer_radius)
        bg_circle.fillColor = colors.HexColor('#f0f0f0')
        bg_circle.strokeColor = None
        d.add(bg_circle)

        # Draw colored arc representing the score percentage: The arc is drawn from 0 degrees (3 o'clock) clockwise
        sweep_angle = (score / 100.0) * 360

        # Start angle is 90 degrees counter-clockwise from 3 o'clock (i.e., 12 o'clock)
        start_angle = 90
        end_angle = start_angle - sweep_angle  # Clockwise direction

        # Ensure start angle is greater than end angle for clockwise sweep
        if start_angle < end_angle:
            end_angle = start_angle - sweep_angle
            extent = -sweep_angle
        else:
            extent = -sweep_angle # Clockwise sweep

        # Create a path for the arc (ring segment)
        from reportlab.graphics.shapes import Path
        p = Path()

        # Calculate start and end points using trigonometry
        import math
        start_rad = math.radians(start_angle)
        end_rad = math.radians(start_angle - sweep_angle) # Correct end angle for clockwise

        # Move to the outer perimeter at the start angle
        start_outer_x = center_x + outer_radius * math.cos(start_rad)
        start_outer_y = center_y + outer_radius * math.sin(start_rad)
        p.moveTo(start_outer_x, start_outer_y)

        num_segments = max(10, int(sweep_angle / 5)) # At least 10 segments, or 1 per 5 degrees of sweep
        angle_step = sweep_angle / num_segments

        # Draw outer arc as line segments
        for i in range(1, num_segments + 1):
            current_angle_deg = start_angle - (i * angle_step) # Clockwise
            current_angle_rad = math.radians(current_angle_deg)
            x = center_x + outer_radius * math.cos(current_angle_rad)
            y = center_y + outer_radius * math.sin(current_angle_rad)
            p.lineTo(x, y)

        # Draw inner arc as line segments (reverse direction)
        for i in range(num_segments, -1, -1):
            current_angle_deg = start_angle - (i * angle_step) # Clockwise
            current_angle_rad = math.radians(current_angle_deg)
            x = center_x + inner_radius * math.cos(current_angle_rad)
            y = center_y + inner_radius * math.sin(current_angle_rad)
            p.lineTo(x, y)

        p.closePath()
        p.fillColor = color
        p.strokeColor = None
        d.add(p)

        # Draw inner white circle
        inner_circle = Circle(center_x, center_y, inner_radius - 2) # Slightly smaller to fit inside the ring
        inner_circle.fillColor = colors.white
        inner_circle.strokeColor = None
        d.add(inner_circle)

        # Draw score text in the center
        score_text = String(center_x, center_y - 10, str(score), textAnchor='middle')
        score_text.fontSize = 36
        score_text.fontName = 'Helvetica-Bold'
        score_text.fillColor = color
        d.add(score_text)

        # Draw "/100" text slightly below the score
        subtitle_text = String(center_x, center_y - 28, "/100", textAnchor='middle')
        subtitle_text.fontSize = 12
        subtitle_text.fontName = 'Helvetica'
        subtitle_text.fillColor = colors.HexColor('#666666')
        d.add(subtitle_text)

        return d


    def _get_risk_color(self, score: int) -> colors.Color:
        """Get color based on risk score"""
        if score >= 80:
            return colors.HexColor('#dc2626')
        elif score >= 60:
            return colors.HexColor('#f97316')
        elif score >= 40:
            return colors.HexColor('#ca8a04')
        else:
            return colors.HexColor('#16a34a')

    def _create_header_footer(self, canvas, doc):
        """Add header and footer to each page with consistent positioning"""
        canvas.saveState()

        # Header
        canvas.setFont('Helvetica-Bold', 12)
        canvas.setFillColor(colors.black)
        canvas.drawString(self.margin_left, self.page_height - 0.5 * inch, "AI Contract Risk Analysis Report")

        # Footer
        canvas.setFont('Helvetica', 8)
        canvas.setFillColor(colors.HexColor('#666666'))

        # Page number
        page_num = f"Page {doc.page}"
        canvas.drawString(self.page_width - self.margin_right - 1*inch, 0.5 * inch, page_num)

        # Disclaimer
        disclaimer = "For informational purposes only. Not legal advice."
        canvas.drawCentredString(self.page_width / 2.0, 0.5 * inch, disclaimer)

        canvas.restoreState()

    def generate_report(self, analysis_result: Dict[str, Any],
                       output_path: Optional[str] = None) -> BytesIO:
        """
        Generate PDF report from analysis results

        Args:
            analysis_result: Analysis result dictionary from the API
            output_path: Optional file path to save PDF

        Returns:
            BytesIO buffer containing the PDF
        """
        # Create buffer
        buffer = BytesIO()

        # Create document
        doc = SimpleDocTemplate(
            buffer if not output_path else output_path,
            pagesize=letter,
            rightMargin=self.margin_right,
            leftMargin=self.margin_left,
            topMargin=self.margin_top,
            bottomMargin=self.margin_bottom
        )

        # Build story
        story = []

        # Page 1: Title, Risk Score, Executive Summary
        story.extend(self._build_page_1(analysis_result))
        story.append(PageBreak())

        # Page 2: Unfavorable Terms, Missing Protections
        story.extend(self._build_page_2(analysis_result))
        story.append(PageBreak())

        # Page 3: Negotiation Points, Walk-Away Items, Concession Items
        story.extend(self._build_page_3(analysis_result))
        story.append(PageBreak())

        # Page 4: Risk Category Breakdown Table
        story.extend(self._build_page_4(analysis_result))
        story.append(PageBreak())

        # Page 5+: Clause-by-Clause Analysis (Dynamic pages)
        story.extend(self._build_clause_analysis_pages(analysis_result))

        # Build PDF
        doc.build(story, onFirstPage=self._create_header_footer,
                 onLaterPages=self._create_header_footer)

        # If using buffer, seek to beginning
        if not output_path:
            buffer.seek(0)
            return buffer

        return buffer

    def _build_page_1(self, result: Dict) -> List:
        """Build page 1 content: Title, Risk Score, Executive Summary"""
        elements = []

        # Title
        elements.append(Paragraph("AI Contract Risk Analysis Report", self.styles['ReportTitle']))
        elements.append(Spacer(1, 0.1*inch))

        # Risk Score Circle and Summary Side-by-Side
        score_frame = KeepInFrame(1.5*inch, 1.5*inch, [self._draw_risk_score_circle(result['risk_analysis']['overall_score'])])
        summary_para = Paragraph(
            f"<b>Overall Risk Score: {result['risk_analysis']['overall_score']}/100 ({result['risk_analysis']['risk_level']})</b><br/>" +
            result['executive_summary'],
            self.styles['CustomBodyText']
        )
        from reportlab.platypus import Table as PlatypusTable
        top_row = PlatypusTable([[score_frame, summary_para]], colWidths=[1.6*inch, 4.5*inch])
        top_row.setStyle(TableStyle([
            ('VALIGN', (0, 0), (-1, -1), 'TOP'),
            ('LEFTPADDING', (0, 0), (-1, -1), 0),
            ('RIGHTPADDING', (0, 0), (-1, -1), 0),
            ('TOPPADDING', (0, 0), (-1, -1), 0),
            ('BOTTOMPADDING', (0, 0), (-1, -1), 0),
        ]))
        elements.append(top_row)
        elements.append(Spacer(1, 0.2*inch))

        return elements

    def _build_page_2(self, result: Dict) -> List:
        """Build page 2: Unfavorable Terms and Missing Protections"""
        elements = []
        elements.append(Paragraph("Unfavorable Terms", self.styles['SectionHeading']))

        unfav_terms = result.get('unfavorable_terms', [])
        if unfav_terms:
            for term in unfav_terms[:10]: # Limit to top 10 or adjust as needed
                term_text = f"<b>{term.get('clause_reference', 'N/A')}:</b> {term.get('explanation', 'No explanation provided.')}"
                elements.append(Paragraph(term_text, self.styles['BulletPoint']))
        else:
            elements.append(Paragraph("No unfavorable terms identified.", self.styles['CustomBodyText']))

        elements.append(Spacer(1, 0.15*inch))
        elements.append(Paragraph("Missing Protections", self.styles['SectionHeading']))

        missing_protections = result.get('missing_protections', [])
        if missing_protections:
            for prot in missing_protections[:10]: # Limit to top 10 or adjust as needed
                prot_text = f"<b>{prot.get('protection', 'N/A')}:</b> {prot.get('explanation', 'No explanation provided.')}"
                elements.append(Paragraph(prot_text, self.styles['BulletPoint']))
        else:
            elements.append(Paragraph("No missing protections identified.", self.styles['CustomBodyText']))

        return elements

    def _build_page_3(self, result: Dict) -> List:
        """Build page 3: Negotiation Strategy"""
        elements = []
        elements.append(Paragraph("Negotiation Strategy", self.styles['SectionHeading']))

        playbook = result.get('negotiation_playbook', {})
        points = playbook.get('critical_points', [])

        if points:
            for point in points[:10]: # Limit per page as needed
                issue_para = Paragraph(f"<b>{point.get('issue', 'N/A')}</b>", self.styles['SubSectionHeading'])
                current_para = Paragraph(f"<u>Current Language:</u> {point.get('current_language', '')[:200]}...", self.styles['CustomBodyText'])
                proposed_para = Paragraph(f"<u>Proposed Language:</u> {point.get('proposed_language', '')[:200]}...", self.styles['CustomBodyText'])
                rationale_para = Paragraph(f"<u>Rationale:</u> {point.get('rationale', '')}", self.styles['CustomBodyText'])

                elements.append(issue_para)
                elements.append(current_para)
                elements.append(proposed_para)
                elements.append(rationale_para)
                elements.append(Spacer(1, 0.1*inch))
        else:
            elements.append(Paragraph("No specific negotiation points generated.", self.styles['CustomBodyText']))

        # Walk-Away Items
        walk_away_items = playbook.get('walk_away_items', [])
        if walk_away_items:
            elements.append(Paragraph("Walk-Away Items", self.styles['SubSectionHeading']))
            for item in walk_away_items:
                elements.append(Paragraph(f" {item}", self.styles['BulletPoint']))

        # Concession Items
        concession_items = playbook.get('concession_items', [])
        if concession_items:
            elements.append(Paragraph("Concession Items", self.styles['SubSectionHeading']))
            for item in concession_items:
                elements.append(Paragraph(f" {item}", self.styles['BulletPoint']))

        return elements

    
    def _build_page_4(self, result: Dict) -> List:
        """Build page 4: Risk Category Breakdown Table"""
        elements = []
        elements.append(Paragraph("Risk Category Breakdown", self.styles['SectionHeading']))
        risk_breakdown = result['risk_analysis'].get('risk_breakdown', [])
        if risk_breakdown:
            # Prepare table data
            table_data = [
                [Paragraph('<b>Category</b>', self.styles['TableHeader']),
                 Paragraph('<b>Score</b>', self.styles['TableHeader']),
                 Paragraph('<b>Summary</b>', self.styles['TableHeader'])]
            ]
            for item in risk_breakdown:
                category = item.get('category', 'N/A')
                score = item.get('score', 0)
                summary = item.get('summary', 'No summary available.')
                score_color = self._get_risk_color(score)
                score_para = Paragraph(f'<font color="{score_color.hexval()}">{score}/100</font>', self.styles['TableHeader'])
                summary_para = Paragraph(summary, self.styles['TableCell'])
                table_data.append([
                    Paragraph(category, self.styles['TableCell']),
                    score_para,
                    summary_para
                ])

            # Create table
            col_widths = [2*inch, 1*inch, 3.5*inch] # Adjusted for better fit
            table = Table(table_data, colWidths=col_widths)

            # CORRECTED Table Style - Ensure all commands have (operation, (start), (end), [values])
            table.setStyle(TableStyle([
                # Header row styling
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#f5f5f5')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.HexColor('#1a1a1a')),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('ALIGN', (1, 0), (1, -1), 'CENTER'), # Score column center
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('FONTSIZE', (0, 0), (-1, 0), 10),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                # Data row styling
                ('TOPPADDING', (0, 1), (-1, -1), 8),
                ('BOTTOMPADDING', (0, 1), (-1, -1), 8),
                # Grid
                ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#d1d5db')),
                # Vertical alignment
                ('VALIGN', (0, 0), (-1, -1), 'TOP'),
                # Row heights (example: set specific height for header and data rows)
                # ('ROWHEIGHTS', [0.3*inch] + [0.25*inch] * (len(table_data)-1)), # Optional: Uncomment to enforce row heights
            ]))
            elements.append(table)
        else:
            elements.append(Paragraph("No risk breakdown data available.", self.styles['CustomBodyText']))
        return elements

    def _build_clause_analysis_pages(self, analysis_result):
        """Build dynamic pages for clause-by-clause analysis"""
        story = []
        clauses = analysis_result.get('clauses', [])
        if not clauses:
            story.append(Paragraph("No clauses analyzed.", self.styles['CustomBodyText']))
            return story

        story.append(Paragraph("Clause-by-Clause Analysis", self.styles['SectionHeading']))

        for clause in clauses:
            # Use KeepTogether to ensure a clause block stays on one page if possible
            clause_elements = []

            # Clause Reference and Category as Header
            ref_cat_text = f"{clause.get('reference', 'N/A')}  {clause.get('category', 'N/A').replace('_', ' ').title()}"
            clause_header = Paragraph(ref_cat_text, self.styles['SubSectionHeading'])
            clause_elements.append(clause_header)

            # Clause Text
            clause_text = clause.get('text', 'No text available.')
            # Split long text into manageable chunks if necessary, though Paragraph usually handles this.
            clause_para = Paragraph(clause_text, self.styles['CustomBodyText'])
            clause_elements.append(clause_para)

            # Risk Indicators (if any)
            risk_inds = clause.get('risk_indicators', [])
            if risk_inds:
                ri_text = f"<b>Risk Indicators:</b> {', '.join(risk_inds)}"
                ri_para = Paragraph(ri_text, self.styles['SmallText'])
                clause_elements.append(ri_para)

            # Add Spacer between clauses
            clause_elements.append(Spacer(1, 0.15 * inch))

            # Wrap in KeepTogether
            kt_flowable = KeepTogether(clause_elements)
            story.append(kt_flowable)

        return story


def generate_pdf_report(analysis_result: Dict[str, Any], output_path: Optional[str] = None) -> BytesIO:
    """
    Convenience function to generate PDF report

    Args:
        analysis_result: Complete analysis result from the API
        output_path: Optional file path to save PDF

    Returns:
        BytesIO buffer containing the PDF
    """
    generator = PDFReportGenerator()
    return generator.generate_report(analysis_result, output_path)